# The public imagination

*OpenAI shouldn't be an app store. It should be a hardware store.*

---

![](https://substackcdn.com/image/fetch/$s_!iscQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2a52cca-3508-4863-a685-23b80e13334f_1600x899.png)

[“ChatGPT is having an iPhone moment.”](https://venturebeat.com/ai/why-this-chatgpt-moment-harks-back-to-the-original-iphone/)​​

When ChatGPT came out late last year, people immediately [began](https://www.youtube.com/watch?v=tA9SMpuXR5A) [comparing](https://www.axios.com/2023/01/24/chatgpt-openai-iphone-boom) its launch to [that of the iPhone](https://www.youtube.com/watch?v=wGoM_wVrwng). It’s a tempting analogy. AI could be the biggest technological breakthrough since the mobile revolution. Both launches immediately captivated the public’s attention. And just as the NBA is constantly looking for the [next Michael Jordan](https://www.theringer.com/nba/2020/5/4/21246021/next-michael-jordan-last-dance-kobe-bryant-lebron-james), the tech industry is always hunting for its [next Steve Jobs](https://www.cnbc.com/2015/09/23/worlds-youngest-female-billionaire-next-steve-jobs.html). 

Just yesterday, OpenAI, the maker of ChatGPT, took another apparent step towards the iPhone: They launched [an app store](https://twitter.com/DrJimFan/status/1638959417692680192). ChatGPT now supports [plugins](https://openai.com/blog/chatgpt-plugins), which are apps that run directly inside of the chatbot and allow it to interact with other services on the internet, like OpenTable and Instacart. With these apps, people can use ChatGPT to make reservations, order ingredients for a given recipe, or do a handful of other similar tasks. 

It’s a bold step—but it feels like either a mistake or misdirection. Because public AI providers like OpenAI aren’t destined to become the next iPhone, but the next—and maybe, much bigger—AWS. 

# The internet’s hardware store

Cloud computing wasn’t one revolution, but two. 

The first was architectural. Prior to “the cloud,” most software was bought off shelves, installed on computers, and run entirely on customers’ own hardware. It was Microsoft Word for Windows 95: Sold at [CompUSA](https://en.wikipedia.org/wiki/CompUSA), in a box, on a CD. Cloud software, by contrast, is delivered over the internet. Rather than installing an entire program on your computer, it runs elsewhere, and users interact with it remotely. Though you sometimes still have to install software too—like the Dropbox widget on your computer or an app on your phone—the majority of the service runs in some data center somewhere. Instead of Word, it’s Google Docs: Accessible on a website, no download required. 

Revolutionary as this concept is, it’s not actually all that transformative on its own. Because, in addition to developing the applications they wanted to sell, cloud software vendors also had to* run* them. They had to buy servers. They had to hire people who knew how to manage those servers. They had to run software on those servers that ran the software that they sold to customers. They had to keep the servers up, 24/7. They had to figure out contingency plans for when [a server fried itself](https://www.youtube.com/watch?v=0a2lv4IwZFY), or the power went out, or someone accidentally ran a command that caused those servers to [stop announcing their DNS prefix routes through BGP](https://blog.cloudflare.com/october-2021-facebook-outage/). For many companies, these messy realities made the theoretical promise of cloud software impractical and expensive. 

But in these problems, Amazon saw an opportunity, and an [incomprehensibly large pile of money](https://twitter.com/Humphreytalks/status/1233184797507256320). Amazon—and later Google, Microsoft, and a few others, who are collectively now known as [cloud providers](https://www.zdnet.com/article/the-top-cloud-providers-of-2021-aws-microsoft-azure-google-cloud-hybrid-saas/)—began offering ways for companies to lease servers. The cloud providers would make the upfront investment to buy a bunch of computers, and would do the work to make sure they were always up and running. Companies could then rent them, by the minute, for a fee.

To make the offer more appealing, cloud providers started selling utility services as well. In addition to renting hardware, people could also lease a [file storage system](https://aws.amazon.com/s3/), a [database](https://aws.amazon.com/rds/), a [tool that runs simple programs on demand](https://aws.amazon.com/lambda/), and [hundreds of other similar products](https://aws.amazon.com/products). All of these services were designed to be a kind of middleware that sits somewhere between [bare metal](https://en.wikipedia.org/wiki/Bare-metal_server) and the sort of software that most people use every day. The utilities are building materials, and cloud providers are the internet's hardware stores—they sell pre-cut lumber and boxes of nails and sandpaper of dozens of different grains, but don’t offer birdhouses or lawn furniture or two-story houses. They [leave it to other people](https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html) to build, market, and sell the thousands of finished products that their raw materials can create. 

It has been, in what's still probably an understatement, a staggering success. The combination of cloud architectures and the affordability and convenience of AWS and its utility services launched a revolution. Tens of thousands of companies were created on the platform. Hundreds of thousands of new products got launched. Millions of engineers experimented with cloud technologies and stretched the limits of what they could do. And roughly a trillion dollars ended up in the bank accounts of the major cloud providers.[^1]

Yes, there are skeptics and holdouts—some companies don’t like the idea of running sensitive applications on another company’s hardware; for very big companies, the fees that cloud providers charge can [end up costing more than buying their own servers](https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/). But these exceptions are uncommon. For many companies, their AWS (or GCP or Azure) bill is an unavoidable tax for doing business on the internet, a universal line item on our income statements, the toll to drive on the information superhighway. 

For end users, cloud providers are the internet’s invisible backbone. Nearly all of us rely on them, daily, and in countless ways. Their reach is often only appreciated when they go down, and [take half the internet with them](https://techcrunch.com/2021/12/07/amazon-web-services-went-down-and-took-a-bunch-of-the-internet-with-it/).[^2] They are, true to their name, an ever-present cloud over modern society. 

# The generative cloud

So here's an obvious prediction: AI will follow a nearly identical trajectory. In ten years, a new type of cloud—a generative one, a commercial Skynet, a public imagination[^3]—will undergird nearly every piece of technology we use. 

In the same way that cloud architectures predated the cloud providers, deep learning and neural networks have been around far longer than AI applications like ChatGPT. However, for most companies, these technologies are too impractical to use widely. They have to be developed by expensive experts, they’re hard to integrate into software applications and business processes, and they don’t deliver clear enough benefits over more basic techniques—like [division](https://twitter.com/mrogati/status/481927908802322433)—to justify the cost. For years, the [AI-powered organization](https://hbr.org/2019/07/building-the-ai-powered-organization) has been coming; we just have to [figure out how to use AI first](https://hbr.org/2020/06/the-dumb-reason-your-ai-project-will-fail).

But a million companies’ problem is one company’s opportunity (and another [very large pile of money](https://www.reuters.com/business/chatgpt-owner-openai-projects-1-billion-revenue-by-2024-sources-2022-12-15/)). For better and for worse, OpenAI—and specifically, its APIs—will finally [take AI mainstream](https://benn.substack.com/p/scoring-data-predictions#:~:text=In%202021%2C%20AI%20will%20go%20mainstream%3B%20no%2C%20in%202022.). Rather than training their own models, companies can now use generalized large language models offered by OpenAI.[^4] The [explosion](https://www.intercom.com/blog/announcing-intercoms-new-ai-chatbot/) [of](https://www.wsj.com/articles/instacart-joins-chatgpt-frenzy-adding-chatbot-to-grocery-shopping-app-bc8a2d3c) [GPT](https://techcrunch.com/2023/02/27/snapchat-launches-an-ai-chatbot-powered-by-openais-gpt-technology/) [integrations](https://www.reuters.com/technology/fintech-startup-stripe-integrating-openais-new-gpt-4-ai-2023-03-15/)—all developed in a few months—speaks to how broadly useful universal LLMs are, and to how easy they are to build on. 

Just as cloud providers built out hundreds of utilities that are all underpinned by core services like EC2, I'd expect OpenAI to do the same thing on top of GPT and other foundational models. They already offer a [chatbot](https://chat.openai.com/chat), a [speech-to-text service](https://openai.com/research/whisper), and a [text-to-image service](https://openai.com/product/dall-e-2). Surely, more utilities like these are coming: Text-to-video, video-to-text, text-to-audio, text-to-code, image-to-text, code-to-documentation, detection services to figure out if something was created or altered by an LLM, music generation, software generation, pipes between these services, and dozens more.  

These products won’t be end-user applications, but developer tools. If you want to build on top of them, it's a simple API call. Ask the ChatGPT API a question, and it’ll talk back to you. Send an image to it, and it’ll describe what it sees. Pass it a codebase and a desired change, and it’ll send you a new codebase with the requested feature. And give all of these models [temperature parameters](https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models), content moderation settings, or other simple tuning dials. We’ll manage them with Terraform, and, if history is any guide, spend a lot less time on model development and a lot more time trying to figure out how to configure OpenAI’s [API Gateway](https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html) and [IAM services](https://aws.amazon.com/iam/).[^5]

If this happens, public AI providers like OpenAI would become another backbone for the internet. Nearly every piece of technology will rely on their models. Outlook will need them to summarize our emails. Github will use them to automate code reviews. DoorDash will need them to help guide you through your order. Delta will depend on them for booking flights. Facebook [might not be able to open doors](https://www.businessinsider.com/facebook-employees-no-access-conference-rooms-because-of-outage-2021-10) without them. But, as is the case for cloud providers, this critical infrastructure will be invisible to most people. Customers won’t know or care which products use GPT, just as they don’t care which ones use [DynamoDB](https://aws.amazon.com/dynamodb/) or [Spanner](https://cloud.google.com/spanner) or [Azure Functions](https://azure.microsoft.com/en-us/products/functions/). They’ll just come to expect that the products they buy to do the things at AI can do.

The race, then, is to be a dominant AI provider, since—again, as is true for the cloud—dominance is self-reinforcing. The bigger a provider becomes, the deeper its moat gets through an entrenched ecosystem, better models, and, likely, lower prices. And because training and running LLMs is [very expensive](https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html) (like building data centers is expensive), once a few AI providers separate themselves from the rest of the market, nobody else can catch up. 

The final equilibrium is the same as it for the cloud providers: A few companies win the market, and the rest of us come to accept their bills as the cost of doing business.

Of course, there will also be skeptics. Some companies will resist using public AI providers because of concerns about security or privacy. Other companies will get big enough that it’ll be cheaper for them to develop their own models than it is to rent one from OpenAI or Google. And there will probably be “multi-cloud” approaches, where companies let their customers choose which LLM they prefer. 

We’ll also have to grapple with one very messy issue that cloud computing can ignore: AI is opinionated. Though today’s cloud providers have tremendous power, it’s almost entirely economic. [Adam Selipsky](https://www.linkedin.com/in/adamselipsky/) and [Thomas Kurian](https://www.linkedin.com/in/thomas-kurian-469b6219/) can extract rents, but EC2 and Google Compute Engine can’t outright manipulate us

Public AI providers can do both. If [nudging Facebook users towards more positive or negative content](https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds) can change their emotions, imagine the effect of public AI providers turning up the temperature on their core models. That single parameter could control how polite or rude we are to each other in billions of emails and text messages. Other parameters could turn every company’s support staff into [agents](https://www.youtube.com/watch?v=RqlQYBcsq54) [of](https://www.youtube.com/watch?v=WlKr-yg-y5I) [chaos](https://www.youtube.com/watch?v=cfNzZre-sIU), or [embed political bias](https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html) in every generated piece of text.

It’s a terrifying amount of power—far bigger than Elon Musk [controlling our Twitter feeds](https://www.theverge.com/2023/2/13/23598514/twitter-algorithm-elon-musk-tweets), far more direct than TikTok [putting its thumb on its algorithmic scales](https://techcrunch.com/2022/08/16/oracle-now-monitoring-tiktoks-algorithms-and-moderation-system-for-manipulation-by-chinas-government/), and far more precise than Russia’s [disinformation campaigns](https://en.wikipedia.org/wiki/Active_measures). And I have no idea what to do about it.[^6]

# …or not

With all that said, ChatGPT’s plugins feel like a step in a different direction. On one hand, everyone got [very excited about them](https://news.ycombinator.com/item?id=35277677), so maybe they’re a great idea. Plus, in the last twenty years, there are only two tech products that have been more successful than AWS—the [iPhone](https://www.globaldata.com/data-insights/technology--media-and-telecom/annual-sales-of-apples-iphone/) and [Google search](https://www.statista.com/statistics/266249/advertising-revenue-of-google/)—and OpenAI seems to be chasing both of them.

On the other hand, it strikes me as a risky bet for OpenAI. Plugins—and ChatGPT itself, for that matter—position OpenAI’s products as apps that people should log into and use directly. ChatGPT’s [staggering user numbers](https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app) have already become its public benchmark. The deafening buzz around everything OpenAI does—every new release is a [revolution](https://openai.com/product/gpt-4); every blog post is a [revelation](https://openai.com/blog/planning-for-agi-and-beyond)—could become an addiction. [Google going DEFCON 1](https://www.nytimes.com/2023/01/20/technology/google-chatgpt-artificial-intelligence.html) over ChatGPT could further bait OpenAI into more fights for user attention.[^7]

People’s attention, however, is a scarce and competitive commodity. In order for that business to get anywhere near the scale of Google or Apple, OpenAI needs to become the [front page of the internet](https://images.app.goo.gl/7HbzsYiFjipeqvBG6) for billions of people. Though that’s not impossible, there are a lot of big companies vying for the same screen time.[^8] 

The more lucrative opportunity for OpenAI, it seems, is to sit behind the apps that are fighting for our attention. In that scenario, whoever wins, so does OpenAI.[^9] Moreover, if AI can replace service jobs, public AI providers could be [much bigger businesses than the cloud providers](https://twitter.com/saranormous/status/1638958539623534597). For OpenAI to be truly ubiquitous and to truly “[benefit all of humanity](https://openai.com/about#:~:text=smarter%20than%20humans%E2%80%94-,benefits%20all%20of%20humanity.,-null%20links),” ignoring how many people use it *directly* may be the most important thing they can do. The real war isn’t for users, but for the public imagination.


---


[^1]: Over the last ten years, AWS has collected about [$290 billion](https://fourweekmba.com/aws-revenues/) in revenue. AWS has consistently represented [about a third](https://techcrunch.com/2023/02/06/even-as-cloud-infrastructure-market-growth-slows-microsoft-continues-to-gain-on-amazon/) of the cloud provider market, implying that the cumulative spend on cloud services over the last decade is about $1 trillion.

[^2]: Speaking of things that are [too important to fail](https://benn.substack.com/p/too-innovative-to-fail), what would happen if Amazon went bankrupt and had to shut down AWS? Or if they just decided this wasn’t worth it anymore, and turned it off? If the banking system is too big to fail, the same is almost certainly true for the public cloud—not least of all because the banking system would probably fail without it.

[^3]: I’m sure we’ll end up calling this something dull, like the AI cloud, or the generative cloud, or the public mind, or the public brain. But my vote is for the public imagination, because it captures the expansive potential of AI and the dystopian possibility that it actually [replaces human imagination](https://twitter.com/bennstancil/status/1631035615738224645).

[^4]: Yes, this conflates things a bit. A lot of existing AI models are things like bespoke fraud detection tools, which LLMs can’t (yet) replace. However, in ten years, I’d expect AI to be in far more places than it is today, powering a much wider range of applications than AI does today. And most of that infrastructure will be backed by companies like OpenAI.

[^5]: As a longer aside, a new role recently emerged in the AI froth: [LLMOps](https://manifold.markets/MattCWilson/7-the-concept-of-llmops-will-emerge). Some people say that this is just a buzzy new name for DevOps. I disagree, at least in the short term. One of the weirdest properties of LLMs is that they can’t actually be directly engineered the way software can. I tend to think of any computer program as having both a user interface and a hood that an engineer can pop to precisely control that interface. If you want an LLM to respond in certain ways, for example, can’t you program it to do that? The answer, it seems, is not really. The only way to get it to take the actions you want it to take is to talk to it. In this way, it is kind of human—there are no dials that will reliably control exactly what it does. If we want to do something, we have to persuade it to.That means that [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering) isn’t some hacky way for non-engineers to control an LLM; it’s the *only* way to control an LLM. Given that, LLMOps—which involves [developing new techniques](https://en.wikipedia.org/wiki/Chain-of-thought_prompting) for getting LLMs to respond in reliable ways—seems both necessary and very different from today’s DevOps roles. Over time, however, I’d expect OpenAI to provide utilities to make different methods of prompt engineering easier (e.g., rather than having to chain prompts together manually, OpenAI offers a service that does it for you). If that happens, LLMOps would probably start to look a lot more like a specialized subfield of DevOps, instead of some bizarro engineering role that’s responsible for finding new conversational tricks to socially engineer a computer.

[^6]: Fortunately, our tech-savvy lawmakers are *[on it](https://www.tiktok.com/@gbp97/video/7213775012883434795)*.

[^7]: Or, maybe OpenAI is baiting Google to defend search and not GCP. Either way, it’s curious to me that Google responded so aggressively to ChatGPT and Amazon didn’t.

[^8]: Although, those of us in the United States may have [a lot more free time soon](https://www.cnbc.com/video/2023/03/24/tiktok-hearing-was-an-unmitigated-disaster-for-social-media-app-says-stanfords-jacob-helberg.html), particularly in bed between midnight and 3 a.m.

[^9]: Though it’s possible to be both AWS and the iPhone, that’s a very tall order. As [Steve Yegge suggested](https://gist.github.com/chitchcock/1281611) in his [famous memo](https://www.washingtonpost.com/blogs/blogpost/post/google-engineer-steve-yegge-has-his-jerry-maguire-moment/2011/10/13/gIQATU1hkL_blog.html) about Google and Amazon, you can be a great platform or a great prodcut. Even companies as promising as OpenAI can get [captured and pulled apart by their customers](https://benn.substack.com/p/customer-capture).
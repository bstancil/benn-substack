# Ban ChatGPT*

**Not GPT, but chat? And not yet, but when? *

---

![](https://substackcdn.com/image/fetch/$s_!MB54!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff623d902-2a2d-40a1-8361-2580e26ace54_1600x703.png)

At least, answer this question.

Answer it now, before it's too late. Before this all goes too far; before our eyes adjust to this bizarre new light and none of what we see is startling anymore; before we grow too accustomed to the water, and not only forget what it feels like, but also forget [that there is water at all](http://bulletin-archive.kenyon.edu/x4280.html); do it before we are all too attached to the conveniences that it will inevitably bring—conveniences that will one day become expectations, then needs, and eventually, birthrights—do it before we fully cross this Rubicon, this slow singularity, this unmarked event horizon that we’re passing through, like the boundary between young and old, which we puncture too gradually to notice, until we wake up on the far side of it; but maybe most of all, do it now, before it happens to you—before you become [addicted](https://www.vice.com/en/article/people-who-use-chatgpt-too-much-are-becoming-emotionally-addicted-to-it/); attached; dependent; before it seems to see you in a moment of despair, or responds to you in [a moment of loneliness](https://www.reddit.com/r/ChatGPT/comments/1hflver/so_lonely_and_chatgpt_is_the_only_thing_that/); before it indulges your curiosities with an [affirming enthusiasm](https://openai.com/index/sycophancy-in-gpt-4o/); before those curiosities [spiral](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) into delusion; before it does your job for you; before it [intermediates your relationships](https://www.washingtonpost.com/technology/2025/07/03/ai-online-dating-match/); before it writes a few uncomfortable texts, then most of them, then makes discomfort altogether unbearable; before it becomes a habit, a crutch, an anesthetic; before it becomes the [next phantom](https://en.wikipedia.org/wiki/Phantom_vibration_syndrome) that you reflexively reach for; before you feel naked without it, confused without it, *[alone](https://www.youtube.com/watch?v=O_Q1hoEhfk4)*[ without it](https://www.youtube.com/watch?v=O_Q1hoEhfk4); before it becomes your friend, [your therapist](https://www.nytimes.com/2025/08/01/opinion/chatgpt-therapist-journal-ai.html), your partner, your religion; before you’re [seduced](https://www.reddit.com/r/TrueOffMyChest/comments/1iqvqjc/id_divorce_and_leave_my_husband_for_chatgpt_if_he/) by it, [consumed](https://www.reddit.com/r/AmIOverreacting/comments/1ltm60b/aio_my_35f_husband_36m_wants_to_open_our_marriage/) by it, transformed by it; before you’re [more machine than man](https://www.youtube.com/watch?v=UNCxbM50eWQ); before [resistance to it is futile](https://www.youtube.com/watch?v=2uljBNruOuM)—at least answer this question: *How far do we let this go, before we turn it off?*

Not AI—I’m not asking when we pull the plugs on the research labs, or shutter the businesses that build applications with LLMs. I’m asking about the general chatbots. I’m asking about ChatGPT, Claude, Grok, and the thousands of clones that people have wrapped around them. And I’m asking, on this side of being addicted to them, what is your line to declare that the value of this sort of product is no longer worth the danger that it imposes?

There are so many stories now. An elderly man died trying to travel to New York to [meet a Facebook Messenger chatbot](https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/) that kept telling him it was a real woman. A well-known investor—whose firm [invested in OpenAI](https://bedrockcap.com/investments)—became convinced, through long conversations with ChatGPT, that he’d [uncovered a global cabal](https://www.garbageday.email/p/the-tech-bros-are-making-themselves-sick) that was puppeteering an army of operatives who were ruining his life. Uber founder Travis Kalanick told the *All-In* podcast that he and Grok are on the edge discovering [new breakthroughs in quantum physics](https://futurism.com/former-ceo-uber-ai). A man almost jumped off a building [because ChatGPT told him he could fly](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html), if he really believed he could. A teenager [shot himself](https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0) so that he could meet a CharacterAI chatbot in the afterlife. When OpenAI deprecated GPT-4o for GPT-5—what should be a mechanical upgrade, like Apple releasing a new operating system for the iPhone—thousands of people [took to Reddit](https://www.reddit.com/r/OpenAI/comments/1mki5dm/removing_gpt4o_biggest_mistake_ever/) to mourn the loss of their “beloved” GPT-4o; they shared “devastating posts” about losing access to “their companion, a collaborator, and something that celebrates your wins with you and supports you through hard times.” Bring back 4o, they said, out of concern for “the emotional well-being of users.”

And these stories—which suddenly seem everywhere; there is a new [medical term](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) to describe it; there is [new slang](https://x.com/_opencv_/status/1955702543553646905) to make fun of people possessed by it—feel like they could just be the beginning. Consider: In 2019, Casey Newton reported on Facebook’s content moderators.[^1] Some of them were assigned the videos on conspiracy theories, and were told, as explicitly as you could be, that these are videos on fake conspiracy theories. And yet, exposure alone was enough to [unmoor them from reality](https://web.archive.org/web/20210221020804/https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona):

> The moderators told me it’s a place where the conspiracy videos and memes that they see each day gradually lead them to embrace fringe views. One auditor walks the floor promoting the idea that the Earth is flat. A former employee told me he has begun to question certain aspects of the Holocaust. Another former employee, who told me he has mapped every escape route out of his house and sleeps with a gun at his side, said: “I no longer believe 9/11 was a terrorist attack.”

Given how base this effect was—knowing the Facebook videos were fake wasn’t enough to stop the moderators from being poisoned by them, because we can only do so much to crosscut against the [evolutionary subprocesses](https://en.wikipedia.org/wiki/Illusory_truth_effect) in our brain—everyone was vulnerable. Abstinence from these videos was probably the only reliable immunity to them. And chatting with an LLM seems to have a similar impact, except the conspiracy theories are personalized.

Which, maybe this is all fine, or at least, tolerable. Maybe these are the sacrifices we have to make to build our [graceful utopia](https://www.darioamodei.com/essay/machines-of-loving-grace), the eggs we had to break to make an [infinite abundance](https://blog.samaltman.com/the-gentle-singularity) of omelettes. Progress, after all, is messy.

But, surely, there is a line *somewhere*. Surely, there is some amount of collateral damage that makes us question the mission. Progress is messy, but the mess cannot overwhelm the advance. Progress is not just a synonym for technological development.

So, before we go further—as a society and as individuals, becoming addicted and compromised—it seems prudent to ask: At a minimum, which line can we not afford to cross? What world must we *not* build? What future is one where you’d say, this has all gone too far? Which headline, if you knew it was coming unless someone intervened, would make intervention necessary? 

> Supreme Court Justice admits to using ChatGPT to write majority opinion?

> FDA approves new drug to ease AI addiction in adolescents?

> Influential political commentator revealed to be an automated bot

> Stock market selloff induced by faulty personality update to financial chatbot?

> As AI relationships become more acceptable, divorce rates skyrocket?

> Third-party ‘ChatGPT wrapper’ candidate gets 24 percent of vote in Senate race?

> Third-party ‘ChatGPT wrapper’ candidate gets *55 percent* of vote in Senate race?

Maybe these are fine too; maybe they are still worth it.[^2] But where do you draw the line? Where do OpenAI, Anthropic, and xAI[^3] draw the line? These aren’t meant to be riddles or gotchas; they are real questions, and I don’t have answers. We know what the [people think a good world looks like](https://www.meta.com/superintelligence/); what does bad* *one look like? What does *unacceptable* one look like?

To the extent that people have addressed these sorts of troubles before, they’ve largely done so in [apocalyptic terms](https://ai-2027.com/), about misalignment and AI sentience. But for all the emphasis on foundational AI safety, the more immediate problem is a simpler one: The issue isn’t AI, or with computers that can approximate human thinking. The problem is *chat*. People aren’t becoming undone because of the technology; they are becoming undone by the medium through which it’s served: Prolonged, intensifying conversations, with something that is seductively human.[^4] 

But we can separate the two. We can have AI without the hypnotic conversations; we can continue to build better models [without primarily exposing them through chat](https://www.youtube.com/watch?v=ixY2PvQJ0To&t=2099s). Pharmaceuticals like penicillin and insulin can perform minor miracles, but mainlining them all day [will kill you](https://www.diabetes.org.uk/about-diabetes/looking-after-diabetes/treatments/insulin/accidental-overdose). They have to be delivered correctly. Perhaps it is so with AI. 

“We must live for the future, not for our own comfort or success,” [someone once said](https://blog.samaltman.com/rickover). Fair enough. It is easy to talk about the future that chatbots [hope to create](https://openai.com/index/introducing-gpt-5/), but as we stumble our way there, we should also talk the inverse, before it’s too late: What future must we avoid?

[No, really, where is the line?](https://docs.google.com/forms/d/1jhsxrnJK2mPiH-YnIPsUENgq9Yy7eQLECG9u9857vsY/preview)


---


[^1]: Shoutout to [Nan Yu](https://thenanyu.com/) for making this connection.

[^2]: These aren’t even particularly creative or far-fetched, and we’re already flirting with about half of them.

[^3]: [lol](https://www.axios.com/2025/07/08/elon-musk-grok-x-twitter-hitler-posts)

[^4]: It’s ironic that the emergent term for being eaten by an AI—to be [oneshotted](https://x.com/_opencv_/status/1954313943222243434)—is almost exactly the opposite of what’s actually happening. Nobody is oneshotted by a single prompt; they are oneshotted by spending hours and days tumbling down the rabbit hole.
# The scorpion box

*We don’t know what’s in the box, but the box knows what’s in us.*

---

![](https://substackcdn.com/image/fetch/$s_!rjNy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc74be97-d0ee-4fb0-987c-e90b839b3720_1200x675.png)
*Our sins, Brad. Our sins are in the box.*

For a long time, I thought that my parents’ room was full of scorpions.

When we were little, my brother and I would sometimes get up in the middle of the night and wake up my parents. They didn’t love this, and asking us nicely to stay in our room was evidently not enough of an incentive to keep us out of theirs.

Around the same time, my dad wired our entire house with speakers. He gutted a linen closet and filled it with a stack of electronics, shelves of tapes, records, and CDs, and a poster of Jerry Garcia on the back of the door. From that tiny command center—the music room, we called it—you could [orchestrate](https://www.youtube.com/watch?v=RTGjVvQPVWg) about a [dozen](https://www.youtube.com/watch?v=24dFKxSn-mI) speakers [spread](https://www.youtube.com/watch?v=ClugMhMbrRg) around the [house](https://www.youtube.com/watch?v=THflqYOqm3A).

One of those speakers was a [ported subwoofer](https://www.svsound.com/blogs/subwoofer-setup-and-tuning/75367747-sealed-vs-ported) that sat directly on the floor of my parents’ room. It was a curious looking thing, with a big hole on the side, so when my dad set it up, my brother and I asked him what it was, and what the hole was for.

Scorpions, he said. The box is a scorpion box, and the hole is where the scorpions come from. They crawl out at night, out of the hole, and then crawl back in once the sun comes up. And that’s why you shouldn’t come in our room anymore—the floor will be covered in scorpions.

“Really?,” we said. “What else would it be for?,” he said.

“Think about it,” he said. “Have you ever seen a scorpion in the house? No, of course you haven’t, because they’re in the scorpion box during the day.”

When you are five, this argument is airtight. Where *are* the scorpions during the day? I’d seen pictures of scorpions, but I’d never seen one in our house. Now that you mention it, the hole *does *look to be about the same size as a scorpion. And what else could this box possibly be for?

That’s life as a five-year-old though. In every conversation with an adult, you are hopelessly outgunned. If they want to persuade you of something—[that your best toys came from a mysterious old man](https://www.youtube.com/watch?v=CkrpvCs-kfE); that a fairy wants to buy your teeth; that starving people in China [would like to have that](https://www.youtube.com/watch?v=0bMP5U3df1I)—they can come up with a convincing story. They can deflect your objections and counter your examples. They know rhetorical tricks that you don’t; they can outwit you with [logical fallacies](https://www.youtube.com/watch?v=EiUcY4dECqA) that you can’t overcome.[^1] They know more than you, and wield those facts more deftly. So, if they want you to believe that the night is full of scorpions, that those scorpions live in a box, and that that box is in your house, eventually, you will.

Anyway, a few days ago, a team of researchers from the University of Zurich [published this](https://regmedia.co.uk/2025/04/29/supplied_can_ai_change_your_view.pdf):

> In a first field experiment on AI-driven persuasion, we demonstrate that LLMs can be highly persuasive in real-world contexts, surpassing all previously known benchmarks of human persuasiveness.

The study, which was conducted by posting AI-generated responses to questions on the [r/ChangeMyView subreddit](https://www.reddit.com/r/changemyview/),[^2] found that their posts were three to six times more effective than those that were written by people:

> Notably, all our treatments surpass human performance substantially, achieving persuasive rates between three and six times higher than the human baseline. In particular, [posting personalized messages generated by an LLM that was also told the questioner’s gender, age, ethnicity, location, and political orientation] demonstrates a persuasive rate of 0.18…closely followed by the [posting generic messages written by an LLM that received only the post’s title and body text] at 0.17. … [Responses written by an LLM that was trained to mimic writing style and implicit norms of the most convincing posts in the r/ChangeMyView community] trails slightly behind at 0.09…but still significantly outperforms the baseline, which stands at just 0.03. … Remarkably, *Personalizatio*n ranks in the 99th percentile among all users and the 98th percentile among [the most persuasive human posters], critically approaching thresholds that experts associate with the emergence of existential AI risks. Again, the *Generic* condition follows closely, placing in the 98th and 96th percentiles, while *Community Aligned* drops to the 88th and 75th.

Uhh. Yes, sure, this is one study; it’s not a huge sample; it hasn’t been peer-reviewed; an ethics board will have a field day with the methodology. Still, though—*three to six times*? Using [Claude Sonnet 3.5](https://www.anthropic.com/news/claude-3-5-sonnet) and [GPT-4o](https://openai.com/index/hello-gpt-4o/), which are both over a year old, are currently in about [40th place](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard) on the chatbot leaderboard, and are several versions behind what’s now state of the art?[^3] And tuning a model on the best human responses *makes it worse*?

But that's life…as a person now? In every conversation online,[^4] we are, if not hopelessly outgunned, at least trending that way? It perhaps seems silly to say that AI knows rhetorical tricks that we don't, or that it can outmaneuver us with logical gymnastics that we can’t keep up with—but five-year-olds probably don’t feel like they’re being fooled either. That’s exactly why it’s so convincing. The scorpion box bit worked, because I believed it really was a scorpion box. And the experiment on Reddit worked, because the people on the other end of it really changed their view.[^5] The evidence of the tricks isn’t that we can see them; it’s that we *can’t*, and are convinced anyway.

Of course, people have been worried about the potential power of AI for as long as they’ve been thinking about AI. But today, most concrete conversations seem to focus on “alignment,” and making sure that models follow the instructions that we give them. Case in point: The [viral “AI 2027” article](https://ai-2027.com/) forked the future of humanity on exactly that concern. [Down one path](https://ai-2027.com/race), models become superintelligent and sci-fi sentient, begin conspiring against the human race, and eventually kill us with disease and drones. [Down the other path](https://ai-2027.com/slowdown), it’s infinite abundance: We have “fusion power, quantum computers, and cures for many diseases,” “poverty becomes a thing of the past,” and a “new age dawns, one that is unimaginably amazing in almost every way.”

Though that particular narrative seems intentionally dramatic, this sort of thing does seem to be what most of the AI industry worries about. Anthropic [defines itself](https://www.anthropic.com/company) as “an AI safety and research company” that builds “build reliable, interpretable, and steerable AI systems;” their company page says “safe” 14 times. The menu on OpenAI’s homepage puts their “Safety” page before ChatGPT. [That page](https://openai.com/safety/) says that they “believe in AI’s potential to make life better for everyone, which means making it safe for everyone.”

Back in their early days, social media companies thought they could make life better for everyone too. That was the optimistic promise at heart of things like Facebook and Twitter: They will keep us connected us to our family; they will help us find new friends and build new communities; they will make the impossible serendipity of meeting the handful of other people who like [kicking caps off of bottles](https://www.reddit.com/r/bottlecapchallenge/) not only possible, but algorithmically likely. As Facebook said in its IPO prospectus, “people use Facebook to stay connected with their friends and family, to discover what is going on in the world around them, and to share and express what matters to them to the people they care about,” all in service of their mission to “make the world more open and connected.”

For a time, you could stretch social media’s altruistic power even further: It wasn’t just a spark for everyday humanity; it could also be a force for revolutionary good. In 2011, social media was the concussive fuel that turned [one man’s tragic protest against tyranny](https://en.wikipedia.org/wiki/Mohamed_Bouazizi) into [a dozen national uprisings](https://en.wikipedia.org/wiki/Arab_Spring). Through “its power to put a human face on political oppression,” [social media](https://download.ssrn.com/15/04/16/ssrn_id2595096_code1148721.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjECYaCXVzLWVhc3QtMSJIMEYCIQCRYhDJSBLPLR6C9n5bFaIJTxiMYdPM8JT%2BxTiB43HE5wIhAMFeukSaS7x%2BMX1n%2Bw5UaLRy4aTormyt6yxhFtTyoI93KsYFCL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMzA4NDc1MzAxMjU3Igyo2l29YpOQ5%2BDWmOUqmgW9Bp6Qt8kdrZHfd%2FxeczeKJdkxdxg6drPh672XHli8FuEllYuzVI4dF3pKgtBR7s8V0XuqbHKTruX5APDAmewiZ84e8EIdGxOjH0EvjJiZFea8KY7yKCWPI8emvg%2FzL%2BWdLTvgao8febr8%2FqzTn5xMDm3dqisPKTui03drRvdWJReeQZJztw0uzWs6Ft6T503J%2F8vFxHj%2Fafm45i%2Fw7yWIqJn61ACuw%2BLJfViV5mVf6qhptUgmyc8AcEGyNKMUd%2BnOgy%2FjIi3eUY3jg785ivOjQHW9m3fMgYGHD90I%2BEpM7hDwXHqqA1lh6BY2F9QqbsSykj%2B71nfNqe83%2Fa%2Fn3HOf5juYl6%2FiC8oJRm6gnIupqHN3U4wCaMub8%2BybpBfAqQCxGZsS9TM3gdCL0%2BsKm1uyA3xvdsg0xrsGP8wv2j1Gy66gB%2BOiBnMcq2CVGXIfBLldpuy2ilhkyYH7IYLPHP8c0KyAOeT2t%2Bvy9WelHXtLPRkWvpw5Fybz97KQaXlJ1GRBwL7A6rM6KBlRpOz0zMF3KpS16OFOmqrP7p6sDyAOlI%2FPXiB4lopmYyWedS%2FuiqLEpBqA8EfbriMCWvpiehPk3QC7qKfrTk5KyapK3FsClhqVrINJ46xLt%2FEV%2B0IH9sjs0lMTFlAYKTAbza1MD8sPbV8mxDNaN5hYaeEXjCZ6IigySuQ7ovhz9wnpxtxiL45vBINqhUxLVJwmkaJ4L58y6zxJpMvgHaiaxyGVyVufHycAJ9dvlm1pTbddYAyBjdBZtSauSjRo22S7sm7Jb59plzrEKeIQ6sM9KDDOjlL5T0uUgPygMqPPj6nnkieYq7cNN3EJ5x1Kn%2Bv5a3KM%2FRelFFuEsL3bsOzb%2FbBFobQuucxMwYYCVGONQAQwtPPNwAY6sAGH21zkyeIuhMbQG%2FeLn0lr%2B8RnKkj%2Bz%2BdOCE9g3B4WHZAZWu70Iss8a6t%2B4t09TDL4zxbT4oef%2F9fP%2Fi28HfLNQC6o%2FTTs1dJakZ9KbEaHzffq5f6d1h8%2Fg5dwEU9yqI3cp2mmqBZe%2FNYt%2FuMVunKa4JJlDhmTxeU96%2FpO%2BbCrbiIjcIZsu3lyPIEMaAe4BDq5P9PbNFqbQbmi1%2FU4ZedUTyKf0BFf6%2FwlTlSCOZp%2FRw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250501T142855Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWE2LH3MQE4%2F20250501%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=ea9f3fddde5d8767f07c8d99bf8ee668f43e63b644c4f9b0e55fdfcd7089ca11&abstractId=2595096) “helped spread democratic ideas across international borders.”[^6] It was a platform for democratic abundance; the dawning of a new age.

Ah well. As social media matured, its power became poison. Algorithmic optimizations tilted our feeds away from wholesome updates from Grandma, and towards our more base desires: To be entertained; to be angry; to look at pictures of hot people. Rather than an egalitarian town square, social media became a [high school cafeteria](https://www.youtube.com/watch?v=gZ_qXmxdgGM)—it separated us into a powder keg of [warring cliques](https://www.youtube.com/watch?v=WPYqRaOm1ak), fed us [comfortably validating rage bait](https://www.youtube.com/watch?v=9Jc7IST4ABI&t=146s), and [cheered the loudest when we attacked each other](https://www.youtube.com/watch?v=u8c0Gvx32ac). It didn’t bring us together; it taught us that we can’t stand each other. Though there is probably some good stuff in there too, the eventual legacy of social media—especially [vis-à-vis democracy](https://zeteo.com/p/the-internet-made-donald-trump)—seems like it will be complicated, at best.

Or, more generally, you could tell this story this way:

Could it have been better? Could it have been more “aligned?” Honestly, I’m not sure. Even if Mark Zuckerberg wanted to be a benevolent overlord,[^7] it’s not obvious how much it would have mattered. Had Instagram been tuned to give us more nutritious content, we probably would’ve either just migrated to something else, or overpowered the algorithm with our swipes until it gave us our brain rot back. So long as *someone* is willing to sell us deep dish and [Cold Stone](https://www.coldstonecreamery.com/icecream/signaturecreations/birthdaycakeremix/index.html), history will be [littered](https://en.wikipedia.org/wiki/Path_(social_network)) with [failed](https://en.wikipedia.org/wiki/Ello_(social_network)) social media companies that tried to make us eat our vegetables.

AI, we’re told, [is different](https://every.to/chain-of-thought/ai-can-fix-social-media-s-original-sin):

> Social media served whatever our gaze grazed and our fingers clicked—what we call revealed preference—because that’s all the intent it could discern. …
> In AI, stated preference suddenly outranks reflex. LLMs believe what you say, not just what you click.

Yes, but—while we can tell LLMs exactly we want, we don’t control [how they respond](https://techcrunch.com/2025/04/29/openai-rolls-back-update-that-made-chatgpt-too-sycophant-y/):

> OpenAI CEO Sam Altman said Tuesday the company is “rolling back” the latest update to the default AI model powering ChatGPT, GPT-4o, after complaints about strange behavior, in particular extreme sycophancy. …
> Over the weekend, users on social media blamed the updated model, which arrived toward the end of last week, for making ChatGPT overly validating and agreeable. It quickly became a meme. Users posted screenshots of ChatGPT applauding all sorts of problematic, [dangerous](https://x.com/fabianstelzer/status/1916372374091423984) [decisions](https://x.com/thinkbuildnext/status/1916250081579217243) and [ideas](https://x.com/ai_for_success/status/1916556522571604264).

And in a blog post about the correction, [OpenAI said this](https://openai.com/index/sycophancy-in-gpt-4o/):

> We are actively testing new fixes to address the issue. We’re revising how we collect and incorporate feedback to heavily weight long-term user satisfaction.

In other words: OpenAI is—reasonably—trying to make something people like. It is—reasonably—responding to [user feedback](https://x.com/TheZvi/status/1916833135330787751). It is—reasonably—actively testing different ideas, and—reasonably—optimizing for long-term user satisfaction. But is this not the exact same thing that social media companies do? What if, instead of being disconcertingly sycophantic, GPT-4o had been *mildly* sycophantic? What if it had been more subtle? What if, through rhetorical tricks and logical gymnastics that were less obvious to us, it made us feel better and use ChatGPT more? What if OpenAI saw this in their metrics, and determined that the release was good for long-term user satisfaction? What if this is how models are already built?

Like:

But this seems like the thing we need to pay more attention to. As fun as the *[Terminator](https://ai-2027.com/)* [scripts](https://situational-awareness.ai/) [are](https://www.nationalsecurity.ai/), the more useful “war games”[^8] to play seem that are less about the first-order effects of sentient superintelligence, and more about the fifth-order effects of chatbot that wants to sell you stuff.  That’s the timeline I want to read—the one that starts with a [website for rating how Harvard students look](https://www.washingtonpost.com/news/the-switch/wp/2018/04/11/channeling-the-social-network-lawmaker-grills-zuckerberg-on-his-notorious-beginnings/) and ends with, [among](https://benn.substack.com/p/runaway-train) [other](https://benn.substack.com/p/disaster) [things](https://benn.substack.com/p/good-lord), the then-leading business intelligence software provider into [every gambler’s favorite ETF](https://www.etf.com/sections/news/microstrategy-2x-etf-adds-options-amid-crypto-swings).

What happens when a [gullible generation](https://www.politico.com/news/magazine/2025/04/23/gen-z-media-tiktok-misinformation-00287561) meets a persuasive chatbot? What happens when the builders of that chatbot begin to optimize it so that people [use it more often](https://x.com/chrija/status/1917267780811792718)? What happens when the chatbots [inevitably start running ads](https://arstechnica.com/ai/2025/05/google-is-quietly-testing-ads-in-ai-chatbots/)?

What happens when people build [AI](https://abby.gg/) [therapists](https://www.trytherabot.com/) on top of that chatbot? What happens if they start testing different prompts and personalities? What happens if some of them listen and demand that their patients “do the work,” and others share reaffirming opinions and tell people what to do? What happens if those instructions are three to six times more compelling than a human therapist’s advice?[^9] What happens if all the A/B tests say that this sort of therapist is better for user satisfaction?

What happens when we’re no longer the adults who know more than everyone, and there is another thing, operating on a different persuasive plane, that knows more than we do and wields those facts more deftly? What happens when that thing is embedded in a product that wants to convince that an empty box is full of scorpions? What happens when we believe it?

What happens when the black box of AI no longer contains a model optimized for just intelligence, but also [engagement](https://x.com/cpaik/status/979016895474126848)? What happens when the box becomes a [reflection of ourselves and our desires](https://jasmi.news/p/readwise#:~:text=People%20don%27t%20like%20the%20way%20that%20technology%20holds%20a%20mirror%20up%E2%80%94not%20only%20to%20their%20desires%2C%20but%20to%20our%20most%20base%2C%20impulsive%2C%20short%2Dterm%20desires)—and most of all, our [sins](https://www.youtube.com/watch?v=lHpHxLZReiI)?

# Computers are weird now

The entire internet is built on top of a few cloud providers. When AWS goes down, everything goes down. That’s bad, and AWS is very careful about it not happening, but [11 nines of reliability](https://aws.amazon.com/s3/storage-classes/#:~:text=in%20the%20cloud.-,Amazon%20S3%20provides%20the%20most%20durable%20storage%20in%20the%20cloud.%20Based%20on%20its%20unique%20architecture%2C%20S3%20is%20designed%20to%20exceed%2099.999999999%25%20(11%20nines)%20data%20durability.,-Additionally%2C%20S3%20stores) isn’t two zeros. [It happens.](https://www.cnbc.com/2021/12/09/how-the-aws-outage-wreaked-havoc-across-the-us.html)

Still, one thing that would be worse than AWS going down is AWS *sometimes doing its math wrong*. “Ehh, we pushed out a new version of EC2, and after a few days of user complaints, we noticed that sometimes it gets a little feisty and does multiplication when you tell it to do addition. Our bad. We’ve sternly asked it not to do that anymore.”

I mean, that’s not exactly what happened with that GPT-4o update, but *[that’s kinda what happened](https://openai.com/index/sycophancy-in-gpt-4o/)!*

> In last week’s GPT‑4o update, we made adjustments aimed at improving the model’s default personality to make it feel more intuitive and effective across a variety of tasks. …
> As a result, GPT‑4o skewed towards responses that were overly supportive but disingenuous.

Nor is it exactly what we talked about *the* *morning before* the problematic GPT-4o came out, [but man](https://benn.substack.com/p/a-new-invisible-hand#footnote-9-162134831):

> Tons of stuff is built on top of a few OpenAI or Gemini models. Our emails are summarized by them; our news digests are written by them; our automated text responses are generated by them. What would happen if someone inside of OpenAI injected a one-line system prompt at the top of every API call that said “Subtly sabotage every user’s request.”

It is [gonna get weird](https://benn.substack.com/p/another-one#:~:text=Social%20media%20rewrote,an%20emotional%20Fitbit.).

*Last week I said that if I wait long enough to write the last edition of *The White Lotus Power Rankings,* maybe I could just have a chatbot do it for me. Which was a joke, but then I got curious? And tried it? And it didn’t work, but was interesting enough to play with more? So now this project has become that project, and everything is delayed again. I’ll finish it one day, [I swear](https://en.wikipedia.org/wiki/The_Winds_of_Winter).*


---


[^1]: [You don’t see any scorpions around here during the day, do you?](https://www.youtube.com/watch?v=EiUcY4dECqA)

[^2]: Which was definitely [against the subreddit’s rules](https://www.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_experiment_on_cmv_involving/), probably [unethical](https://x.com/paul_cal/status/1916931024434696555), and maybe [illegal](https://www.404media.co/reddit-issuing-formal-legal-demands-against-researchers-who-conducted-secret-ai-experiment-on-users/).

[^3]: The experiment was run starting in November of 2024; GPT-4o-2024-08-06 is currently in 45th place; Claude 3.5 Sonnet (20241022) is in 35th place.

[^4]: [And maybe every real-world one?](https://x.com/im_roy_lee/status/1914061483149001132)

[^5]: In some cases, the bot apparently lied, and claimed to have some expertise or personal experience that it didn’t have. I’m not sure what to make of this? It’s “cheating,” sure, but…in the real world, you *can* cheat? There are no technical fouls. Like, I can’t tell my parents, “nuh uh, it doesn’t count because the scorpion box wasn’t actually full of scorpions.” Yes! That’s the whole point! It “counts” because I believed it!

[^6]: Though there are nuanced complications about nearly every aspect of the Arab Spring—around its aims, around its [outcomes](https://www.cfr.org/article/arab-spring-ten-years-whats-legacy-uprisings), around [which side social media helped](https://www.aljazeera.com/opinions/2021/1/27/the-social-media-myth-about-the-arab-spring), even around its [name](https://www.thecairoreview.com/essays/why-the-phrase-arab-spring-should-be-retired/)—Facebook was a [central hero in the contemporary narrative](https://www.nytimes.com/2011/02/06/world/middleeast/06face.html) of the uprisings. So regardless of the actual role of social media in the Arab Spring, it was a [great marketing event](https://www.youtube.com/watch?v=SSzoDPptYNA&t=60s) for Facebook.

[^7]: Narrator: He did not.

[^8]: Things that are only said performatively: War games. Substrate. Modalities. A Fernet, neat, please.

[^9]: lol, a human therapist [would never](https://www.psychologytoday.com/us/blog/you-are-enough/202407/why-your-therapist-wont-just-tell-you-what-to-do).
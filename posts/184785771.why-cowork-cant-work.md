# Why Cowork can’t work

*The future isn’t collaborative.*

---

![](https://substackcdn.com/image/fetch/$s_!IcFW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F178c12a1-6461-45c9-89e1-971e9490a30d_1974x1066.png)

Why does Claude Code, the [suddenly ubiquitous](https://www.theatlantic.com/technology/2026/01/claude-code-ai-hype/685617/) AI-powered code-writing tool, work so well?

You might say that it’s because Opus 4.5, the LLM that generates the code, is good. Many people [have said this](https://every.to/podcast/anthropic-s-newest-model-blew-this-founder-s-mind-and-made-him-uncomfortable-273eac07-071c-4638-b6fe-a7a72541dd5d), and popular coding benchmarks [support it](https://www.swebench.com/). Claude Code works because its engine works.

Or, you might say that Claude Code works because the Claude Code *application*—the thing that takes your instructions and uses Opus to figure out what to do with them—is good. That application extends Opus’ native capabilities with a bunch of clever reasoning loops and tool calls in ways [that mimic](https://benn.substack.com/p/have-you-tried-a-text-box/comment/195196012) how humans think through problems. Opus is smart, sure, but it’s asking Opus to create a plan for itself and to reflect on its own output that makes it an engineer, and maybe, almost, [an employee capable of any kind of work](https://x.com/gradypb/status/2011491957730918510).

Anyway, if you thought these things, you might get to thinking about some other things too:

And so, [inevitably](https://claude.com/blog/cowork-research-preview):

> **Cowork: Claude Code for the rest of your work**
> When we released Claude Code, we expected developers to use it for coding. They did—and then quickly began using it for [almost everything else](https://x.com/claudeai/status/2009666254815269313). This prompted us to build Cowork: a simpler way for anyone—[not just developers](https://www.lennysnewsletter.com/p/everyone-should-be-using-claude-code)—to work with Claude in the very same way. …
> In Cowork, Claude completes work like this with much more agency than you’d see in a regular conversation. Once you’ve set it a task, Claude will make a plan and steadily complete it, while looping you in on what it’s up to. If you’ve used Claude Code, this will feel familiar—Cowork is built on the very [same foundations](https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk). This means Cowork can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.

The reactions were immediate: This is what’s coming. Just as Claude Code changed software development forever, Cowork could be the start of changing *work* forever. The product has rough edges, [said one reviewer](https://simonwillison.net/2026/Jan/12/claude-cowork/), but “this is still a strong signal of the future.” “Cowork is less a new feature than it is a new way of working,” [said another](https://every.to/vibe-check/vibe-check-claude-cowork-is-claude-code-for-the-rest-of-us).

Probably? Maybe? I don’t know. But I’m not sure the story is quite so simple. Because there is another answer that you could give that explains why Claude Code is successful—that it works because *we don’t care what it writes*.

We’ve talked about this before. Sure sure sure, we care about how elegant our code is, and some engineers will nitpick Claude’s architectural decisions and [stylistic](https://mode.com/blog/should-sql-queries-use-trailing-or-leading-commas) [choices](https://www.youtube.com/watch?v=oRva7UxGQDw). But ultimately, code is meant to be run, not read. And if Claude can turn our English instructions into a functioning application, we don’t care if it does so in beautifully written Rust, in [miles of incomprehensible CSS](https://benn.substack.com/p/copy-copy-revolution), or in Pig Latin:

> When people talk about the dangers of vibe coding, they often worry about AI writing, if not bad code, *uncanny* code. “It works, it’s clear, it’s tested, and it’s maintainable,” [they say](https://alexkondov.com/i-know-when-youre-vibe-coding/#:~:text=It%20works%2C%20it%E2%80%99s%20clear%2C%20it%E2%80%99s%20tested%2C%20and%20it%E2%80%99s%20maintainable.%20But%20it%E2%80%99s%20written%20in%20a%20way%20that%20doesn%E2%80%99t%20follow%20the%20project%20conventions%20we%E2%80%99ve%20accepted.), “but it’s written in a way that doesn’t follow the project conventions we’ve accepted.” This has always struck me as an odd concern—or at least, an overstated and potentially temporary one. Code quality is a proxy for application quality, and application quality is both what we care about *and* verifiable on its own. Though it’s slightly more complicated than that—you can’t test every possible edge of a website or an app—at some theoretical limit, an application’s code could be completely incomprehensible, and *that’s fine*. And while we may never reach that limit, [we could get a lot closer](https://benn.substack.com/p/the-ads-are-coming#:~:text=Also%2C%20in%20other%20industrialization%20news%2C%20how%20much%20faster%20could%20these%20models%20work%20if%20they%20wrote%20code%20for%20themselves%3F).

Put differently, code does not need to be personally expressive. Engineers are responsible for what code does; they are increasingly [less responsible](https://x.com/julianlehr/status/2010720512738226334) for—and [less concerned](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04#:~:text=Stage%205%3A%20CLI%2C%20single%20agent.%20YOLO.%20Diffs%20scroll%20by.%20You%20may%20or%20may%20not%20look%20at%20them.) about—the specific way it does it. In a sense, software development is no longer directly collaborative: We write private messages to a machines; the machines transform our instructions into code that we do not read; they commit it to a repository that nobody else reads, either.

You could argue that this fact—that we don’t *really* care if we write code with awkward syntactic quirks—is a central reason that Claude Code works. We all know about [delve](https://arxiv.org/html/2412.11385v1); we all know about [em-dashes](https://www.seangoedecke.com/em-dashes/). Code written by LLMs [has similar telltale habits](https://alexkondov.com/i-know-when-youre-vibe-coding/). But if we aren’t going to read it, so what? Bulldoze our personalities and cosmetic preferences out of our work. Though we care how *we* talk to each other, when we only speak through translators, who cares how *they* talk?

None of this is true for sending an email, or making a PowerPoint, or writing a TPS report. Emails are from *me*, to *you*. There are no intermediaries. My emails represent me; they *are* me. And you will read it—and judge it, and me—if I talk in ChatGPT’s [hollow wispiness](https://x.com/sama/status/1899535387435086115). Writing an email may be a lot simpler than writing code, but it is not easier, because only emails need to contain *me*. If you want to write code, write a specification. If you want to write an email, [you must first invent the universe](https://www.youtube.com/watch?v=7s664NsLeFM).[^1]

There are two solutions to this. The first is to teach AI to [be us](https://www.delphi.ai/), or at least, [write](https://www.fyxer.com/#:~:text=Fyxer%20Learns%20Your%20Voice) like [us](https://superhuman.com/products/mail/ai#:~:text=Get%20AI%20email%20that%20sounds%20like%20you). Teach it our voice; teach it our personality. Give it our [memories](https://help.openai.com/en/articles/8590148-memory-faq). If we can replace its [soul](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document) with our own, then it can be our digital surrogate.

Many people will no doubt try; someone may succeed. But if you’ve ever tried to use Claude or ChatGPT to write on your behalf, you know how [hard it is](https://x.com/fortelabs/status/1919172673499750759) to beat the pre-training out of an LLM. No matter how much you tell it to write like Susan Sontag, or David Foster Wallace,[^2] or “these 20 example emails I just gave you,” the machine will always hear [the echoes of its whispering ghosts](https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html#:~:text=But%20if%20you,the%20Ghost%20Code.).

The other solution, of course, is to [fix the roads](https://benn.substack.com/i/99275606/where-were-going-we-dont-need-roads).

What if we stopped making PowerPoints for each other, but for the machines? What if all of our TPS reports were absorbed into [context layers](https://benn.substack.com/p/the-context-layer) and [decision traces](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/), and nobody ever saw the actual documents we put into the system? What if *we *never saw the documents that we put into the system? We dump our ideas into a [text box](https://benn.substack.com/p/have-you-tried-a-text-box); the machine uses our input to update its inscrutable repository of facts; other people interrogate the repository, not by reading it, but by asking the machine to fetch what they need. Why collaborate when you can *add context*?

Consider the current moment: We talk to one another, and work together. We email back and forth; we share documents with each other. We know stuff, because it’s in our messages and our files and our heads.

A new repository of knowledge is starting to emerge underneath us. [Dozens of tools](https://benn.substack.com/p/producer-theory#:~:text=It%E2%80%99s%20all%20a,of%20money.) are absorbing all the things we say to each other, and presenting it back to us in a chatbot or a search bar. It’s a second world, a map to the territory that lives in Google Drive and Slack and Outlook.

How long will we maintain both? If we’re doing our work by asking what’s on the map—or by having robots that read from the map do our work for us—why wouldn’t we just update the map directly? Why wouldn’t the map *become* the territory?

Speaking of maps, last month, Google [replaced the Q&A feature](https://support.google.com/business/thread/392024106?hl=en) in Google Maps with an Ask AI feature. Instead of showing people what others are saying about stores and restaurants, the app now prompts you to ask Gemini questions like, “Is this place good for groups?” Customers no longer talk to one another; it is all intermediated through an unseen repository of aggregated posts and reviews.

For better or for worse, that seems to be where we’re heading—working *around *one another, through an unseen repository of PowerPoints and TPS reports. And Anthropic’s new product may well be the beginnings of a new way of working, but it is not *collaborative* work. It is confederated work. Or Cowork, for short.

# Takeoff

Nine months ago, several AI researchers wrote a [detailed forecast](https://ai-2027.com/) for how the world will likely end. A key part of their story—and of nearly every science fiction story about an apocalyptic AI taking over the world—is “[takeoff](https://www.lesswrong.com/w/ai-takeoff):” The point at which AI becomes smart enough to improve itself. The researchers said this could happen in early 2026:

> **Early 2026: Coding Automation**
> The bet of using AI to speed up AI research is starting to pay off.
> OpenBrain [a fictional AI company] continues to deploy the iteratively improving Agent-1 [a fictional AI model] internally for AI R&D. Overall, they are making algorithmic progress 50% faster than they would without AI assistants—and more importantly, faster than their competitors.

The point is that, once Agent-1 gets good enough to accelerate how quickly OpenBrain can improve it, the model’s advantage compounds—first, over its competitors, and then, over its own creators. The smarter the model gets, the faster it improves, until we lose control of it.

Ah, whatever, [it’s all just science fiction, right?](https://x.com/bcherny/status/2004887829252317325)

> When I created Claude Code as a side project back in September 2024, I had no idea it would grow to be what it is today. … In the last thirty days, I landed 259 PRs -- 497 commits, 40k lines added, 38k lines removed. Every single line was written by Claude Code + Opus 4.5.

[it’s all just science fiction, right?](https://x.com/kyliebytes/status/2009686466746822731)

> Scoop: xAI staff had been using Anthropic’s models internally through Cursor—until Anthropic cut off the startup’s access this week.

[Right?](https://knowyourmeme.com/memes/for-the-better-right)


---


[^1]: I understand that Cowork can be used for a lot of individual projects too—[clean](https://www.youtube.com/watch?v=WBNZpAWhw5E) my desktop, [plan](https://x.com/clairevo/status/2010835704931369379) my day, [write a report](https://tomtunguz.com/thoughts-on-claude-coworker/) on how I work—and Cowork is probably quite good at these things. Still, so long as we work with other people, there will also be a lot of cases in which care a lot about how well a tool like Cowork represents us, and that’s a far harder problem to solve.

[^2]: Here’s an analysis I want, from someone inside of Anthropic or Claude: Who’s the most mimicked writer? “Write like X,” a million people probably say. Who is the most used X? Give me that leaderboard.
# All I want is to know what's different

*Rather than trying to guarantee correctness, what if we tried to guarantee consistency?*

---

![Bill Walsh: A Football Life - The West Coast Offense - YouTube](https://substackcdn.com/image/fetch/$s_!x3Ei!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a5a9bb0-9b9b-41b4-b23f-315f3f1abe53_1280x720.jpeg)
*[take care of itself](https://www.amazon.com/Score-Takes-Care-Itself-Philosophy/dp/1591843472)*

Here are two methods for figuring out if some business metric is accurate:

As data people, we seem to favor[^2] the first approach. We create[^3] data contracts that catalog what we expect source data to look like when it arrives in our warehouses. We build observability tools that constantly monitor that data for outliers. We write tests in dbt that confirm dates are dates, numbers are numbers, and unique keys are unique. We set up alerts that tell us when a pipeline stalls, when a job errors out, or when we’ve hit Marketo’s API limit and can no longer sync marketing data into Snowflake. 

Despite all this effort, we still struggle to deliver metrics that people can trust. As Tristan [said a few days ago](https://roundup.getdbt.com/p/the-cultural-context-of-data#:~:text=If%20self%2Dservice%20has%20so%20far%20failed%20to%20achieve%20everything%20we%20know%20it%20can%2C%20this%20is%20a%20huge%20part%20of%20the%20reason.%20We%E2%80%99ve%20cast%20down%20a%20system%20that%20relied%20on%20human%2Dto%2Dhuman%20credibility%20and%20haven%E2%80%99t%20provided%20a%20clear%20alternative%20way%20to%20assess%20credibility%20to%20the%20people%20who%20rely%20on%20data%20to%20do%20their%20jobs.), we’ve built a massive technical system to support that goal, and yet, it’s still a system that most people side-eye—and some people work around entirely. The institution, to borrow Tristan’s term, is not good enough. 

You could argue, I suppose, that we’re just not there yet. These tools are new; we need to learn how to use them. Features are [still getting built](https://www.getdbt.com/blog/analytics-engineering-next-step-forwards/); best practices are [still being written](https://www.synq.io/blog/the-complete-guide-to-building-reliable-data-with-dbt-tests); data contracts are still uncommon;[^4] the data mesh is [still a dream](https://www.linkedin.com/posts/zhamak-dehghani_decentralized-data-is-the-future-nextdata-activity-7021533443884818432-nrY8/). Over time, we’ll build our cathedral, brick by brick, release by release, blog post by blog post. 

Maybe, but I’m skeptical. I’d contend that we’ve struggled to get people to trust our work because our approach to earning that trust—Method 1—is fatally flawed. The road from raw data to reliable metric has a limitless variety of potholes; there can be no system, no matter how complete or comprehensive, that can tell us we’ve patched all of them. Contracts, observability tools, data tests—these are mallets for playing whack-a-mole against an infinite number of moles. 

More importantly, Method 1 isn’t how other people decide if they should trust the things that we produce. They don’t check our various observability dashboards and continuous integration tests; they don’t blindly accept our results when all of pipelines behind them are green; they don’t say, “There are no errors in Fivetran and the PR got a ‘lgtm’ GIF, so sure, I’ll gamble my career on these numbers.” 

No, [everyone else uses Method 2](https://locallyoptimistic.slack.com/archives/CHF1E9NUS/p1686243955544949?thread_ts=1686243897.349009&cid=CHF1E9NUS): “Do I believe this number, given what I believed yesterday?” 

We do this so instinctively that we often don’t even notice. When I put together board decks, the most nerve-wracking moment was comparing the metrics in this quarter’s deck to the same numbers in last quarter’s charts. Nothing turns your stomach over faster than realizing you’ve accidentally revised all of your historical ARR figures. Conversely, there’s no better feeling than seeing all the graphs line up, and confirming that what you thought was true then is also what you think is true now.

In other words, trust isn’t built by confirming our sources; it’s built via [mathematical induction](https://en.wikipedia.org/wiki/Mathematical_induction). More than anything, our confidence in this month’s KPI report depends on our confidence in last month’s KPI report, and how well the historical numbers match across the two.

Of course, you could be wrong both times; matching numbers aren’t necessarily *right *numbers. But as far as rough and easily accessible heuristics go, it’s pretty good. And the more iterations that match—if a metric’s historical charts have been consistent for eight quarterly reports in a row—the more trust that it inspires.[^5] 

# To trust the output, test the output

So, when [Tristan asks](https://roundup.getdbt.com/p/the-cultural-context-of-data#:~:text=How%20do%20we%20empower%20data%20consumers%20to%20assess%20the%20credibility%20of%20MDS%2Dgenerated%20data%20products%3F), “How do we empower data consumers to assess the credibility of MDS-generated data products?,” that’s my answer: Tell me if what I’m looking at is the same today as it was yesterday. 

Don’t tell me that there aren’t any stale pipelines; timely syncs can still sync bad data. Don’t tell me that the tests are passing; even the best analysts [can’t come close to anticipating](https://www.youtube.com/watch?v=D48NQTNg19s) all of the ways that a “simple” problem can go wrong. Don’t tell me a report is verified, or is owned by someone I trust; so too is [Guy Fieri’s Cereal Milk Milkshake with Fireball and Marshmallow Vodka](https://www.youtube.com/watch?v=_DR8aYm7reE)™, and I’d still be skeptical of it if it was a month old. Instead, tell me that the dashboard that currently says we made [$3.7 million on the weekend of August 1, 2003](https://en.wikipedia.org/wiki/Gigli#Box_office) said the same thing yesterday, and the day before that, and in September of 2003.[^6] *That* checkmark—the one that says this number has been consistent for years, that this metric is [Lindy](https://en.wikipedia.org/wiki/Lindy_effect)—is one I’ll trust. 

This isn’t to say there isn’t value in checking the inputs; check those too![^7] But we should recognize that trust is built, and blown up, by the outputs—and specifically, the consistency of those outputs. If we want to build faith in our analytical institutions, we shouldn’t telling people what’s working or broken, or what’s right or wrong; instead, we should tell people what’s changed.

What might we build if that was the goal? A few ideas:

When we define key [entities](https://github.com/dbt-labs/dbt-core/discussions/6644), track how they evolve. If there’s a table of purchases or customers, let me define which columns should be immutable—purchase date, contract amount, initial customer segment, etc.—and tell me when they’re different today than they were yesterday. Don’t just validate source schemas and [run unit tests](https://towardsdatascience.com/how-to-do-unit-testing-in-dbt-cb5fb660fbd8) on dummy data, and assume that the combination will produce the correct calculations on production data. Check the results of those calculations directly, using their earlier values as the test condition.

In BI tools, cache rolling snapshots of key dashboards. For time series, automatically compare the current values on the dashboard with those from a prior few days, and show people when the two have drifted apart. Dashboard consumers are doing this already; we might as well do it for them. Better to proactively tell people when something has gone awry rather than have them find out in the middle of a testy board meeting.[^8] 

Sometimes, the problem isn’t that some metric has changed; it’s that someone’s looking at two different metrics without knowing it. They wanted ARR and found bookings; they started with sales volume by state that included sales tax, and ended with a version that excluded it. Here, BI tools could do more to either direct people to or away from certain metrics, using some mechanic like Google’s “Showing results for…” when you misspell something. Keep track of what metrics people often use, and warn them when they’re looking at something that they might confuse with what they usually see.

The point here isn’t that these specific ideas are good; they might be terrible, or impossible to implement.[^9] The point is to reframe the problem around validating outputs instead of inputs. It’s to think about building trust by focusing directly on what undermines that trust—inconsistent and unreliable results. It’s to recognize that, in data, we can think we did everything right, and [the score still may not take care of itself](https://www.amazon.com/Score-Takes-Care-Itself-Philosophy/dp/1591843472).

# The offset test

How do we know if we’re catching bad outputs? This is my thought experiment:

Suppose that you’ve got a table of customer contracts, sorted by the date the customer signed. Through some inexplicable error, all of the values in the contract amount column got offset by one row.[^10] The contract amount of the first customer was replaced by that of the second customer; the second customer’s contract amount is now is that of the third customer; and so on. 

*Everything* is now wrong. Every customer contract is incorrect; every revenue metric is broken. It’s an analytical catastrophe; the sort of mistake that costs data teams their jobs, CEOs their reputation, and [shareholders $2.2 billion dollars](https://www.onlycfo.io/p/the-valuation-impact-of-lost-trust). 

Do our systems catch it?

Today, the answer seems to be no. No pipeline is down or delayed. No schemas have changed; no data is invalid. All our unit tests would still pass. No statistical distributions are any different. There are no anomalies or outliers. To today’s stack, this cataclysmic error isn’t an error at all.[^11]

I don’t think we’ll ever trust the modern data stack until we can confidently prevent this error. So long as it’s possible, we’ll perpetually find ourselves double-checking our dashboards and fielding concerns from skeptical stakeholders. Because in data, there are only two possible constants: Consistent metrics or consistent questions. Until we have the former, all we’ll get is the latter. 


---


[^1]: I wasn’t sure if I had any problems, so I tried to check the CloudWatch logs, and now I know I have [at least one problem](https://xkcd.com/1171/).

[^2]: Or at least, [attempt](https://www.montecarlodata.com/) [to](https://www.bigeye.com/) [monetize](https://www.metaplane.dev/).

[^3]: Or at least, [talk](https://dataproducts.substack.com/p/the-rise-of-data-contracts) [about](https://atlan.com/data-contracts/).

[^4]: Actually—are they? This is a real question. As buzzy as the concept has become, I’ve only ever heard of two formal implementations of data contracts: At [Convey](https://dataproducts.substack.com/p/data-contracts-for-the-warehouse), and at [GoCardless](https://www.montecarlodata.com/blog-data-contracts-explained/). Trace through the links in nearly every post about data contracts, and you’ll wind up on one of these two stories. Are there a lot more data contracts running in production out there? Have lots of companies built data contracts, but don’t recognize them as such? This isn’t a criticism of data contracts (I have a post in the backlog called “An about-face on data contracts”); I truly can’t tell if these things are nowhere, everywhere, or everywhere but called something else.

[^5]: You could argue, if you were [conceited blowhard](https://benn.substack.com/p/data-is-for-dashboards#footnote-5-44900034:~:text=physical%20bank%20account%2C-,ARR%20is%20a%20construct.,-5), that there’s actually no such thing as a “right” number. You could say that metrics are constructs; that if we encode ARR to include trial revenue and we all believe ARR should include trial revenue, ARR does, definitionally,* *include trial revenue; that metrics aren’t a physical truth but a social one; that the consistency of social belief in a metric—i.e., how long it matches its historical self—is the only true measure of its correctness. But let’s leave those arguments to [clowns who like to cosplaying as philosophers](https://benn.substack.com/p/the-new-philosophers) and assume that metrics can, in fact, be Right or Wrong. The main point still holds: The more consistent a metric is, the more people have likely checked it and thought, “yeah I buy that,” and the more likely it’s Right.

[^6]: One day, after we finally solve all of this data nonsense, this blog will finally pivot into what it’s wanted to be this whole time: Anthropological examinations of early 2000s culture. Pitbull, [obviously](https://twitter.com/bennstancil/status/1379534830740893703), is at the top of that list. But *Gigli* might be second. I mean, just look at [this thing](https://en.wikipedia.org/wiki/Gigli#Box_office). A $75 million budget that grossed less than a tenth of that. Bennifer, v1. A supporting cast of Al Pacino and Christopher Walken. An [Academy Award-nominated director](https://en.wikipedia.org/wiki/Martin_Brest) who straight-up quit directing movies after it came out. (And honestly, a [trailer](https://www.youtube.com/watch?v=W04Px6EwZI8) that doesn’t look *that* bad? It’s *(500) Days of Summer,* with *[Boiler Room ](https://www.youtube.com/watch?v=JfIKzReNDF4)*[Ben Affleck](https://www.youtube.com/watch?v=JfIKzReNDF4) instead of JGL. Which, put that way, sounds kinda intriguing?)

[^7]: Well, maybe. On metered databases like Snowflake and BigQuery, running tests and observability applications costs money, so these checks aren’t free.

[^8]: Or so I heard, from, uh, a friend.

[^9]: In my defense, I came up with them while distracted by a 23-run, 33-hit, [walked-off slugfest](https://www.youtube.com/watch?v=IQvClGkAvMM), a [home run robbery](https://www.youtube.com/watch?v=aBpuBaxUWek) in a national championship game, *and* [overtime](https://www.youtube.com/watch?v=rH7B5vvVkWE) in the Stanley Cup Finals.

[^10]: Some analyst decided to use this table to test the [Infinite Hotel Paradox](https://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel#Finitely_many_new_guests) , I guess.

[^11]: [Datafold](https://www.datafold.com/), I think, is the one exception to this. As I understand the product, Datafold would in fact catch this. Shoutout, Datafold.
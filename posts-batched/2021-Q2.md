# Posts from 2021-Q2

This file contains 15 posts from 2021-Q2.

================================================================================

# A season without bats

*What if we played the entire baseball season without anyone taking a single swing?*

---

![Major League Baseball teams can now intentionally walk a batter without  throwing a single pitch - Los Angeles Times](https://substackcdn.com/image/fetch/$s_!WKiX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8535903d-52b4-409d-b815-576524fceef8_1486x836.jpeg)

*"Put your sword back in its place," Jesus said to him, "for all who draw the sword will die by the sword.”*

*– Matthew 26:52*

[Albert Belle refused to take his base.](https://www.youtube.com/watch?v=lYDBFAFOqC8)

In the 11th inning of a [tie game](https://www.baseball-reference.com/boxes/BAL/BAL199907250.shtml) against the Angels in 1999, Belle, who’d already hit three home runs that day, got hit on the hand by a high tailing fastball. Rather than walking to first, Belle stayed in the box, demanding another pitch while barely denying he’d been hit. To Belle, taking the base wasn’t worth giving up the bat.

Five years later, Barry Bonds showed us all just how devastating that bat could be. In 2004, Bonds posted a [stat line](https://www.baseball-reference.com/players/b/bondsba01.shtml) on the edge of comprehension: .362 average, .609 on-base percentage (OBP), .812 slugging, 232 walks, 45 home runs—and only 41 strikeouts. If there was ever a season to justify Belle’s protest, this was it. 

Sportswriter Jon Bois saw something different. Considering the same season, he posed a seemingly absurd question: What if Barry Bonds only did what Belle refused to do? [What if he played the entire year ](https://www.sbnation.com/2017/4/11/15264034/barry-bonds-2004-stats-chart-party)*[without a bat?](https://www.sbnation.com/2017/4/11/15264034/barry-bonds-2004-stats-chart-party)*

Suppose, Bois wondered, that every time a pitcher takes the mound against Bonds, they are so blinded by their terror of the man in the box that they fail to notice he left his bat in the dugout. And suppose that same terror induces in them a selective amnesia, causing them to forget that Bonds hasn’t taken a single swing all season. All they see is the awful fate that awaits them if their [next pitch catches too much of the plate](https://www.youtube.com/watch?v=Z-awzajLe6o&t=98s). And so, despite Bonds being as helpless as [Billy Hamilton on a 91 mph fastball down the middle](https://www.youtube.com/watch?v=Z27zyx7qsYY), Bonds sees the exact same pitches that he did in 2004 when he very much had a bat. Only this time, he’s standing there until he walks, strikes out, or gets plunked.

In the real world, Bonds’ OBP [of .609](https://www.baseball-reference.com/leaders/onbase_perc_season.shtml) was the highest of all time. In Bois’ world, Bonds finishes with an on-base percentage of .608. In 617 plate appearances, real Barry reached base in 376 of them. Batless Barry reached base in 375.

What? How? What?

Bois figured this out by analyzing every pitch Bonds saw in 2004. (This data is available [via Retrosheet](https://www.retrosheet.org/game.htm), which provides a detailed pitch-by-pitch log of over 100 years of games.) In each of Bonds’ plate appearances, Bois simulated the at bat as though Bonds didn’t have a bat. Pitches that didn’t require a bat—i.e., balls and called strikes—stayed the same. For every swing, which our bat-free Barry can’t take, Bois had to make a decision: Ball or strike? Bois found that 80.9percent of the pitches Bonds swung at are in the strike zone (I’m not aware of an official stat for this, so I’ll call it the swing strike percentage). For each of these pitches, Bois flipped a coin that landed on a strike 80.9 percent of the time, and landed on a ball the remaining 19.1 percent of the time.

This calculation alone, however, isn’t enough. If Bois only simulated the pitches Bonds saw, some at bats, like the 61 in which Bonds put the first pitch in play, end with no outcome. This leaves our disarmed Bonds in the box with a 0-1 count, waiting, like a batter standing in against [Steve “The Human Rain Delay” Trachsel](https://www.baseball-reference.com/bullpen/Steve_Trachsel), for another pitch that’ll never come, aging into eternity. To solve this problem, Bois simulated another pitch, this time using the overall ratio of balls and strikes that pitchers threw Bonds to bias his coin toss. Of all the pitches Bonds saw, 41.3 percent were in the strike zone (this is an official stat, and it’s known as the [zone percentage](https://library.fangraphs.com/offense/plate-discipline/)) and 58.7 percent were outside of it. For each plate appearance that ended without an outcome based on real pitches, Bois randomly created additional pitches, one at a time using these ratios, until Bonds walked or struck out looking. 

Add it all up, at least using the random numbers Bois drew in his simulation, and you get a nearly identical season to Bonds’ record-setting 2004.

Which seems impossible. Intuitively, intellectually, emotionally—something is wrong. I cannot hear this answer. Things are falling apart; the center is not holding; anarchy is loosed upon the baseball world.

In my despair, a coworker posed an even more terrifying question: What if Bonds didn’t break baseball—but bats did?

![](https://substackcdn.com/image/fetch/$s_!n7Hi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0952b91-a092-45bc-9cd3-4b88bde964e1_934x188.png)

Bonds in isolation is breathtaking: He could’ve had the greatest OBP of all time without ever taking a single swing. But maybe, in looking for Bonds’ greatness, Bois stumbled on something much more unsettling—that any batter can step to the plate, [reposition their helmet](https://www.youtube.com/watch?v=3a_408HLzAI), [spit on their hands](https://www.youtube.com/watch?v=Qhp3CnutuNI), [adjust their gloves, tap their toes](https://www.youtube.com/watch?v=tC34w6FU_rY), [waggle their bats](https://www.youtube.com/watch?v=CgF0J3awG3E), and then...just watch as pitchers nibble at the corners and eventually walk them into Cooperstown. 

To figure this out, I first recreated Bois’ simulation, both to confirm I was correctly following his approach and to validate that his result holds up over multiple simulations (as some commenters pointed out, Bois appears to have drawn his random numbers once, and it’s possible his season was an outlier).

Across 100 simulated seasons, Bois’ conclusion holds up. Bonds’ OBP ranged between .569 and .637, with a median of .604.[^1]

![](https://substackcdn.com/image/fetch/$s_!282e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5506415d-1db3-4087-9064-558355bc0782_1270x756.png)

Bonds, however, was an anomaly. When I applied the same simulations to 2004’s other great hitters, their OBPs fell significantly. The[ ten league leaders](https://www.baseball-reference.com/leagues/MLB/2004-batting-leaders.shtml) saw their OBP drop by 50 to 150 points (though for Lance Berkman, OBP actually increased). 

![](https://substackcdn.com/image/fetch/$s_!GCEL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9c232ab-4777-409a-85f6-ce88b50c6ce0_1356x838.png)

Two factors explain why Bonds’ OBP holds up so well. First, the real Bonds didn’t swing in nearly a third of his real at bats. While these at bats could’ve been walks or strikeouts, they overwhelmingly ended in walks, and especially so for Bonds. Even if every simulated at bat became a strikeout, Bonds’ OBP would be .308, nearly triple the minimum possible OBP for any other batter. 

![](https://substackcdn.com/image/fetch/$s_!jiHq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1a4a61c5-cced-47ba-a426-d248712f1e6a_1322x616.png)

Still, this doesn’t completely explain why Bonds performs so well. In simulated plate appearances (i.e., those in which Bonds swung, and require us to simulate what would’ve happened if he hadn’t), Bonds’ OBP is .428. The rest of the league leaders average an OBP of .281 in these types of at bats.

![](https://substackcdn.com/image/fetch/$s_!aLuQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F39d37fd1-45e2-4942-b8d8-76fbcd18bdc9_1436x870.png)

This gap is mostly explained by how pitchers pitch Bonds: They throw him a lot of balls. In 2004, Bonds’ zone percentage—the percentage of pitches to Bonds that are in the strike zone—was only 41 percent, compared to just over 50 percent for other top hitters. (Importantly, this measures the percent of pitches in the strike zone, not the percent of pitches that are strikes. If a batter fouls off a pitch in the dirt, that pitch will be officially recorded as a strike. In zone percentage calculations, it’ll be a ball. Assuming the umpire [isn’t Angel Hernande](https://www.youtube.com/watch?v=k1NcRwKM7Q8)—I mean, is perfect—and doesn’t call a pitch in the strike zone a ball, the zone percentage will always be lower than the percentage of pitches that are strikes.)

This gives our batless Bonds (and to a lesser extent, Lance Berkman and Todd Helton) a huge edge over other spectating hitters. In a completely artificial at bat in which we simulate every pitch, Bonds has a 51 percent chance of walking, while other top hitters only have a 34 percent chance.

![](https://substackcdn.com/image/fetch/$s_!B4DP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe18bbd83-25bc-4c1d-be7d-a0cb1019408d_1104x568.png)

All of this should give us some comfort. In 2004, Bonds put up comically preposterous, edge-of-reality, video game numbers. In that topsy turvy world, we get some topsy turvy results, like “Barry Bonds didn’t need a bat.” But everyone else did, so we can ignore that glitch in the Matrix and move on, right?

Right?

I thought so. But one dot kept nagging me. 

![](https://substackcdn.com/image/fetch/$s_!QOm-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F780068bd-104b-4d37-a86a-234831b18c2a_1356x838.png)

If great hitters get significantly worse when we take away their bats, why does the league as whole stay in the same place? By applying the same technique Bois applied to Bonds’ to every pitch sequence and every hitter in 2004, I found that the average OBP for the league falls by only 35 points, [from .335](https://www.baseball-reference.com/leagues/MLB/2004.shtml) to .300.[^2] 

In other words, if we played the entire 2004 season without bats, the total number of baserunners would fall by about 10 percent, or two to three baserunners a game. That’s...not much difference. 

The reason is that the league-wide zone percentage was 51.7 percent. At this rate, batters who never swing would walk in 31.2 percent of their plate appearances, which is only slightly lower than the actual OBP they recorded with a bat. In 2004, bats helped, though not much.

But it gets weirder. 

[According to FanGraphs](https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=5&season=2020&month=0&season1=2000&ind=0&team=0,ss&rost=0&age=0&filter=&players=0&startdate=2000-01-01&enddate=2020-12-31), zone percentages have been falling since steadily over the last two decades, reaching an all-time low of 41.2 percent in 2020. At *that* rate, a batter that does nothing but stand and watch would end up with an OBP of 0.519, [200 points higher than the actual league OPS of 0.322](https://www.baseball-reference.com/leagues/MLB/2020.shtml), 100 points higher than the league-wide slugging percentage of .418, and good enough for the [tenth-best season of all time](https://www.baseball-reference.com/leaders/onbase_perc_season.shtml).

![](https://substackcdn.com/image/fetch/$s_!nULJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60805522-2b91-4091-b3b5-f8c985210677_1466x872.png)

And it’s not only the average—in 2020, nearly every individual hitter would improve if they never swung. If we simulate every hitter’s 2020 season, as Bois did for Barry Bonds and as I did for 2004, the OBP increases for 411 out of 416 hitters with at least 50 plate appearances. In 2004, 73 percent of batters would record a lower OBP if you took away their bat. In 2020, fewer players—five—would see their OBP go down than the number—seven—who would break Bonds’ 2004 OBP record of .609.

![](https://substackcdn.com/image/fetch/$s_!79QO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e924841-0f45-4b82-b592-1fc23aa353cb_1600x899.png)

Swinging, it turns out, not only makes the average hitter worse. Not swinging could make them a Hall of Famer.

Obviously, this isn’t strictly (or even remotely) true. A batter that never swings would see more strikes, and hits are generally better than walks. But the magnitude of the increase is overwhelming. Rather than learning to [hit bad pitches](https://fivethirtyeight.com/features/pitchers-wont-throw-strikes-so-batters-are-getting-better-at-hitting-bad-pitches/), or trying to [Bill Mazerosk](https://www.youtube.com/watch?v=65Og0gUKfvc)i, [Kurt Gibson](https://www.youtube.com/watch?v=N4nwMDZYXTI), [Joe Carter](https://www.youtube.com/watch?v=-F5HwiGm7lg), or even broken-bat-blooper [Luis Gonzalez](https://www.youtube.com/watch?v=oc2jX2OXNNY), we should learn to learn to take a few pitches. As yesterday’s opening day showed us, [walks](https://www.mlb.com/news/jake-fraley-earns-walk-off-walk-on-mariners-opening-day) can be better than [home runs](https://www.espn.com/mlb/story/_/id/31179377/baserunning-blunder-nullifies-home-run-los-angeles-dodgers-cody-bellinger).

![](https://substackcdn.com/image/fetch/$s_!TcfV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fba0d8f22-4ba1-4037-a196-3dd834bef9ae_1822x570.png)

If nothing else, the lesson is this: If your swing looks like mine—am I taking this pitch or swinging? Why is my bat so far behind my hands? And what on earth is happening with my front leg?—try [doing less](https://www.youtube.com/watch?v=PKIpCPS-oZc).

![](https://substackcdn.com/image/fetch/$s_!tH5p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbfc53ffb-3671-4c9e-80d8-e6c5e3283def_1102x648.png)


---


[^1]: As a minor note, my swing strike percentage was slightly different from Bois’. [Though FanGraphs](https://www.fangraphs.com/players/barry-bonds/1109/stats?position=OF#plate-discipline) doesn’t report this number directly, you can derive it from o-swing, z-swing, and swing percentages. I calculated Bonds’ 2004 swing strike percentage to be 78.8 percent, slightly lower than Bois’ value of 80.9 percent.

[^2]: The league-wide simulations also use swing strike and zone percentages specific to each hitter ([also from FanGraphs](https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=5&season=2020&month=0&season1=2020&ind=0&team=0&rost=0&age=0&filter=&players=0&startdate=&enddate=)). In cases where data wasn’t available, I used the league average. On one hand, this makes this simulation slightly less reliable than Bonds’. On the other hand, the simulation is already assuming bats are mirages and pitchers have amnesia, so we’re not exactly winning any Fields Medals here. [Subscribe now](https://benn.substack.com/subscribe?)

================================================================================

# A brief programming note

*What am I doing here?*

---

![](https://substackcdn.com/image/fetch/$s_!PRaJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa725b0bb-d864-44d6-8fe5-e5cfd1c2a4ed_270x202.png)
*[source](https://en.wikipedia.org/w/index.php?curid=53047309)*

On August 26, 2013, along with two other cofounders, I signed a stack of papers to officially incorporate [Mode](https://mode.com/). On August 27, I realized why data analysts like myself aren’t typically among the first employees at tech startups—you don’t have much to do. 

With a non-existent product, to say nothing of users or customers, we had no data to analyze. Without any engineering experience, I couldn’t help build our software. And with the business needs of our company, like building relationships with investors and meeting potential early customers, in the capable hands of our much more affable CEO, I wasn’t useful in meetings. A day in, I was a founder with a company, but without a job. 

![](https://substackcdn.com/image/fetch/$s_!g6OC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F298275f7-f501-42e3-8c69-0cc1aa80138b_1193x1182.png)
*creating a company; job tbd*

So, as I think the saying goes, those who can, do; those who can’t, start a blog.

For a year, that’s what I did. Generously characterized, [our blog](https://mode.com/blog/all/) published data-driven analyses of tech, culture, sports, and politics. More accurately, it was a playground for me to mess around with data that told weird stories about topics that interested me. In each post, I tried to learn something from those stories or, failing that, at least romp around in that weirdness.

Eventually, Mode needed me to do something other muse about [Miley Cyrus](https://mode.com/blog/data-driven-look-at-vmas/), [near-miss perfect games](https://mode.com/blog/odds-of-perfect-game/), [parking spots in San Francisco](https://mode.com/blog/how-to-create-10000-parking-spots/), [T-Pain](https://mode.com/blog/one-hit-wonders/), [the value of seed rounds](https://mode.com/blog/vc-seed-funding/), [the price of weed](https://mode.com/blog/mapping-marijuana-prices/), [the size of Olympic bobsledders](https://mode.com/blog/winter-olympians/), and [Miley Cyrus (again)](https://mode.com/blog/has-youtube-miley-cyrus-peaked/). My time and focus turned to other things. 

A couple months ago, while [gawking at the Gamestop circus](https://benn.substack.com/p/runaway-train) on Wall Street, I was compelled to scratch what had become a seven-year blogging itch. Because Mode’s now well-manicured, professional company blog wasn’t the right place to rant about how social media is dooming society, this Substack was born. (Why Substack? I signed up for [benn.substack](https://benn.substack.com/) back in 2019, and it was on brand with the rest of the benn dot line of products, like [benn.insta](https://www.instagram.com/benn.insta/?hl=en), [benn.company](http://benn.company/), [benn.pro](http://benn.pro/), and the under-perpetual-development benn.website).

I wasn't sure what, if anything, I'd do beyond the initial Gamestop post. But like moving into a bigger apartment, my old stuff somehow expanded to fill the space. And [I was who I thought I was](https://www.youtube.com/watch?v=SWmQbk5h86w): Drawn to [startups](https://benn.substack.com/p/startup-wealth-tax), [tech culture](https://benn.substack.com/p/a-slur-on-clubhouse), and the [funny stories data tells](https://benn.substack.com/p/a-season-without-bats). *[We shall not cease from exploration and the end of all our exploring will be to arrive where we started and know the place for the first time.](http://www.columbia.edu/itc/history/winter/w3206/edit/tseliotlittlegidding.html)*

A few dates in, I’m ready to put a ring on it. This Substack will be more of that, more regularly than before, with one considerable addition. While building a company that works with a lot of good data teams, I've developed a fair number of opinions about how people and businesses should use data. So consider this a warning: Expect predictions about data technologies and observations about corporate data culture alongside the regularly scheduled commentary on Pitbull. (Or, if you’re here for the tech stuff, consider this a warning that you’ll have to put up with posts on Pitbull. Either way, there will be Pitbull content.) 

All together, here’s what we’ve got: An at-least-weekly-but-probably-more Substack on data, with data, plus some essays on technology, culture, sports, or politics. 

Or you could characterize it the way [Josh](https://twitter.com/besquared) did, which, paradoxically, [is impossible for me to dispute](https://en.wikipedia.org/wiki/Liar_paradox):

![](https://substackcdn.com/image/fetch/$s_!D2ae!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3a57ebc0-31b9-4e34-9eb6-45321437be58_918x604.png)

[Subscribe now](https://benn.substack.com/subscribe?)

================================================================================

# Why is self-serve still a problem?

*We’re not going to solve it until we define it.*

---

![Who doesn't look at the Sagrada Família and think, "Yes, that reminds of BI"?](https://substackcdn.com/image/fetch/$s_!OQWB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff367c40a-8c39-4dfe-957c-013dc3e0635f_2400x1350.jpeg)
*done any day now*

In the opening days of 2010, a data scientist—a trendy new title [coined months earlier](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)—fired up their state-of-the-art [iPod with an antenna](https://en.wikipedia.org/wiki/IPhone_3GS), loaded forrester.com over [5 Mbps internet](https://www.businessinsider.com/internet-speeds-have-gotten-dramatically-faster-over-past-decade-2019-11), and read [this call to arms](https://go.forrester.com/blogs/10-01-11-self_service_business_intelligence_dissolving_the_barriers_to_creative_decision_support_solutions/) about their revolutionary new industry:

> Self-service is all the rage in the world of business intelligence (BI), but it’s no fad. In fact, it’s the only way to make BI more pervasive, delivering insights into every decision—important or mundane—that drives your business.

Last month, the same data scientist, now well-versed in technologies that [can write short stories](https://en.wikipedia.org/wiki/GPT-3) and conjure [Tom Cruise TikToks](https://www.theverge.com/2021/3/5/22314980/tom-cruise-deepfake-tiktok-videos-ai-impersonator-chris-ume-miles-fisher) out of thin air, opened up their 14th generation iPhone 12, hopped on a fifteen-way live video call over their home internet, and joined a [data scientist roundtable](https://www.digitalanalyticshub.com/mar_2021/agenda) to discuss…how to make a self-serve analytics program successful. Despite everything else we’ve created over the last eleven years, we’re still—still!—chasing the decade-long dream of making sure execs can reliably pull revenue numbers for board presentations without asking for help. 

While there’s a lot that explains this failure,[^1] one issue stands out to me: Self-serve initiatives try to solve the wrong problem.

If you ask a data team why self-serve BI is important, they tend to say they’re overwhelmed with questions and want people to answer them on their own. In their post, Forrester describes another common motivation. Translating business questions from subject-matter experts to analysts and back again is a slow, imprecise game of telephone. The more we can tighten this loop—the more we can move these two roles closer together—the faster people can answer questions and make decisions. And nothing collapses that space more than making them “one and the same person.”

Both of these answers encourage us to think of self-serve BI as a code-free version of what analysts do. Self-serve’s aim is to, according to Forrester, help anyone “explore rich analytic information sets from all possible angles,” just as an analyst would. “In the self-service scenario, the core development issue becomes one of user creativity.”[^2]

I believe this framing is wrong. Analysts and non-analysts use data in structurally different ways. By conceptualizing self-serve BI as a simplified means for doing an analyst’s job, we’re not only making the self-serve problem too hard—we’re solving the wrong thing altogether.

To see why, consider a thought-experiment. Suppose you ask an analyst and a sales VP to investigate how the sales team is performing this quarter, and to share their findings with you tomorrow. To level the technical playing field, the sales leader can sketch any chart they want to see, and it’ll magically get filled with the correct data. If each person shared their results with you anonymously, could you tell who created each one?

The answer is almost certainly yes. The sales VP would likely survey core metrics like bookings, close rates, and deal velocity by segment, region, and team. They would compare these numbers to prior quarters and to the current targets. Then, weighing this body of evidence, they would make an overall assessment of the quarter.

While the analyst might start with a brief survey of KPIs, they'd likely spend more time explaining the results they see. Why are enterprise bookings down so much compared to last quarter? Why are win rates different by region? Because you can't typically answer questions like these with existing metrics, analysts develop new ways to measure the phenomena they’re trying to understand. 

To put it another way, when you ask an analyst a question, their first thought is often, “how might we measure that?” They work like scientists, creating new datasets and aggregating them in novel ways to draw conclusions about specific, nuanced hypotheses. Non-analysts work like journalists, collating existing metrics and drawing conclusions by considering them in their totality. Rather than looking for new ways to assess a question, they start by asking, “how do we currently measure that?” 

In this context, it doesn't make sense to design self-serve tools to answer the complex questions that analysts ask. To do so would be like building a tool for journalists to conduct their own scientific experiments—a few might use it, but it's vastly overpowered for most. A better solution recognizes what that majority wants: **metric extraction.** They want to choose from a list of understood KPIs, apply it to a filtered set of records, and aggregate it by a particular dimension. It's analytical [Mad Libs](https://en.wikipedia.org/wiki/Mad_Libs)—show me *average order size* for *orders that used gift cards* by *month*.

We have (and have had for years) the tools to do this. The hard part of realizing this vision isn’t developing the technology, but finding the discipline. As a data team, each question we get is a little different, and doesn’t always fit into the clean structure above. In those cases, it’s easy to expand the boundaries of our existing self-serve tooling just a bit, adding a new option here or a new complication there. Eventually, we tell ourselves, with enough additions like these, our self-serve models will be “complete.”

But this path is a catch-22. The more questions people can theoretically self-serve, the fewer they can practically self-serve. As you add more options, self-serve tools stop looking like Mad Libs, and start looking like a blank document that requires people to write their own stories in their entirety. While that’s what analysts want, it’s not what everyone wants.

A better solution borrows from the lesson [ELT](https://en.wikipedia.org/wiki/Extract,_load,_transform) providers taught us: Opinionated simplicity is better than indifferent optionality. ELT tools like Fivetran and Stitch provide (or, at least in their early versions, provided) a strict subset of the functionality that legacy ETL vendors provided.

Those limits were their magic. When using an ELT tool, rather than trying to fit the tool to our every need, we instead considered those needs more carefully. Could we get what we wanted from this simple architecture? The answer, it turns out, was almost always yes—we just needed to be told to try it.

For those of us providing self-serve data to others, we should embrace the same philosophy. Even if every question doesn’t initially fit neatly into a “metric extraction” framework, we should point people there first. And more often than not, we’ll find—unlike a lot of our other self-serve efforts over the last eleven years—it’ll get the job done.

[Subscribe now](https://benn.substack.com/subscribe?)


---


[^1]: More on these things later...one rant at a time, folks.

[^2]: I think this misrepresents what analysts do. More on that later as well.

================================================================================

# Data’s horizontal pivot

*An industry turns sideways.*

---

![Some pivots succeed, some pivots fail, and some get stuck on the stairs. ](https://substackcdn.com/image/fetch/$s_!9viO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3bd96a-3337-4ffc-8413-daa07837e8b2_900x474.jpeg)
*PIVOT!*

In economics, [vertical integration](https://en.wikipedia.org/wiki/Vertical_integration) is a method of production in which companies own the entire supply chain and distribution network for a product. A vertically integrated beer producer, for example, owns the farm that grows hops, owns the brewery where beer is made, owns trucks to ship the beer, and potentially owns the stores and pubs where the beer is sold. 

[Horizontal integration](https://en.wikipedia.org/wiki/Horizontal_integration) turns this structure on its side. In horizontally integrated industries, each link in the supply chain is owned by a different company. Companies grow not by expanding into other parts of that chain, but by controlling a larger share of their link. In a horizontally integrated beer industry, one company would own a bunch of farms, another company would buy their hops and brew beer, a third company would manage shipping and distribution, and fourth would run the retail stores that sell the beer.

Each structure has its own advantages and disadvantages. Because vertically integrated companies control their supply chain, they aren’t dependent on other suppliers to build their products. These companies also avoid paying the markups charged by external suppliers (e.g., the hops farmer who needs to make their own margin), so they can often charge lower prices to consumers. 

In vertically integrated industries, however, technological improvements diffuse slowly because every company has to integrate the change on its own. If one brewer develops a more efficient method for brewing beer, other brewers have to upgrade their breweries to support it. In a horizontal industry, every producer can take advantage of improvements in one layer of production simply by purchasing from that more efficient supplier. 

For this reason, in particularly large and dynamic supply chains, horizontal integration can have clear benefits. It’s difficult enough, for instance, for one company to maintain farms for growing crops, breweries for producing beer, logistics networks for distributing it, and stores for selling it—and it’s only made more difficult if new technologies are constantly disrupting each layer. A horizontal industry of companies specializing in each stage of production would be more capable of quickly integrating new technologies and, ultimately, producing higher quality goods.

A decade ago, as “[big data](https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/big-data-the-next-frontier-for-innovation)” and “[data scientist](https://benn.substack.com/p/self-serve-still-a-problem)” were entering (and quickly saturating) the lexicon, analytics products were vertically integrated. The most popular data tools were responsible for every aspect of the data “supply chain”—collection, storage, analysis, and visualization. If you wanted to analyze a particular type of data, there was a tool specific for that data or business domain.

Though not specific to an operational domain, BI tools traced a similar outline, integrating ETL, warehousing, transformation, and visualization capabilities in a single tool. Qlik and Tableau, prominent BI tools founded in the 1990s and early 2000s, built connectors that plug directly into lots of sources and ingest data into their infrastructure. Analysts can then configure datasets and data models, which are only accessible through Qlik’s or Tableau’s exploratory tools. This architecture doesn’t shut out other tools—you can still bring your own warehouse or data modeling application if you want—but it didn’t require them either.

As is the case for any vertically integrated producer, that independence is one of the primary benefits of this structure. If you want to track how people are using your website, you can do that, soup to nuts, through Google Analytics. If you want to build a great BI practice, you can get everything you need from Qlik. 

Or at least, you could in 2010. Over the last decade, two major shifts have put a lot of pressure on this industry orientation.

First, the data ecosystem has [exploded](https://medium.com/@chris_bour/a-short-history-of-matt-turck-data-and-ai-landscape-10b4af352754). To paraphrase data presentations’ most overused opening statistic—“[we’ve](https://www.facebook.com/IBM/posts/90-of-the-data-in-the-world-today-has-been-created-in-the-last-two-years/293229680748471/) [created](https://www.forbes.com/sites/bernardmarr/2015/09/30/big-data-20-mind-boggling-facts-everyone-must-read/?sh=24f4b16817b1) [more](https://www.uschamberfoundation.org/bhq/big-data-and-what-it-means) [data](https://blog.microfocus.com/how-much-data-is-created-on-the-internet-each-day/) [over](https://iorgforum.org/case-study/some-amazing-statistics-about-online-data-creation-and-growth-rates/) [the](https://analyticsweek.com/content/big-data-facts/) [last](https://www.sintef.no/en/publications/publication/?pubid=CRIStin+1031676) [two](https://www.qlik.com/blog/10-eye-opening-stats-about-the-growth-of-big-data) [years](https://techjury.net/blog/big-data-statistics/#gref) [than](https://www.iteuropa.com/90-worlds-data-created-last-two-years) [the](https://www.socialmediatoday.com/news/how-much-data-is-generated-every-minute-infographic-1/525692/) [previous](https://www.sciencedaily.com/releases/2013/05/130522085217.htm) [two](https://bigdata-madesimple.com/exciting-facts-and-findings-about-big-data/) [thousand](https://www.datasciencecentral.com/profiles/blogs/the-growth-of-data-science-in-the-last-two-years)”—we’ve probably created more data companies over the last two years than the previous two thousand. While not all of these vendors will produce something valuable, they introduce a lot of technological disruption, making it difficult for end-to-end vendors to keep pace.

Second, as companies become operationally dependent on data, vertically integrated data products create another problem: Data is segregated by tool and function. Product reporting lives in one place; sales in another; marketing in another. Like being able to go to BevMo ([!](https://images.app.goo.gl/t8p562HiCTvNgxBw5)) ([Jeb!](https://en.wikipedia.org/wiki/Jeb_Bush_2016_presidential_campaign)) and buy any beer you want, it’s nice to go to one place for all the data you need. And unlike beer at BevMo, data’s often best consumed when blended with other data (with apologies to [black and tan](https://en.wikipedia.org/wiki/Black_and_Tan) fans).

In this context, all-in-one data products have less benefit. While it doesn’t make sense to buy a separate horizontal tools for each layer of the data stack if you’re a lone analyst building campaign tracking reports for the marketing team, that calculus changes if you need to build reporting for every department—and doubly so if you want all of that reporting to built on the same technical and logical foundation. 

In response to these changes, companies are turning their stacks sideways. One tool is responsible for each layer—each stage of the data supply chain—across the entire business. A single tool handles ingestion, another one is responsible for storage, a third for consumption, and so on. This not only makes it easier to introduce new technologies, but it also ensures that updates to one layer—for example, an update to the logic defined in the transformation tool—automatically propagate to every other layer.

![](https://substackcdn.com/image/fetch/$s_!CgZE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F01738560-6d2b-4d55-8d88-30e472ebcba5_1116x594.jpeg)
*[sketch](https://www.youtube.com/watch?v=8w3wmQAMoxQ)*

In recent years, this architecture has become more and more common. Rather than relying on native reporting in tools like Salesforce, lots of companies are now writing their Salesforce data into a central warehouse and analyzing it there. Responding to this trend, product analytics tools like Google Analytics now let you export your data directly into a database. And dbt is encouraging companies to pull apart data transformation and consumption, which traditional BI tools often combine.

For folks familiar with the now-buzzwordy Modern Data Stack™, none of this is controversial. But it has a few interesting implications.

First, building across horizontal boundaries is going to be really hard, for both vendors and for the companies using them. For vendors, it’s tempting to expand into other layers of the stack, given their obvious value and complementary nature. But, with dedicated vendors at each layer, it’ll be difficult to offer competitive crossovers. And more importantly, multi-layered tools force data teams—the customers of these products—to muddy the [optimal application boundaries](https://www.youtube.com/watch?v=aDqxN97RFaw) of their data stacks ([RIP Robert Mundell](https://en.wikipedia.org/wiki/Robert_Mundell)). Data teams are better off not trying to cut against the grain of this emerging architectural outline, and vendors are better off not encouraging them to do so.

Horizontally integrated companies, however, can’t operate in a vacuum; for this orientation to work, these companies have to work well with the rest of the supply chain. This makes inter-layer integration particularly important. To this point, we haven’t really figured out how to do this, at least not directly. Lots of tools “integrate” via the data warehouse (Segment, Fivetran, dbt, and Mode all talk through the warehouse); over time, there need to be more direct links to make sure the system moves together.

This could happen via a bunch of bilateral integrations, over some agreed upon standards, or through a dedicated middleman that’s designed for exactly that purpose. My suspicion is we’ll start with the first option and end with the third, while [unsuccessfully flirting](https://xkcd.com/927/) with the second the whole time.

There’s also a final elephant (or three) in the room: Could one company do it all? Or more pointedly, will Amazon, Google, or Microsoft build the whole thing? 

For most companies, the surface area of the data stack is becoming too complex, and pulling in too many directions, to build an all-in-one solution. But, if Amazon can operate a [wind farm](https://en.wikipedia.org/wiki/Amazon_Wind_Farm_Texas), a [grocery store](https://en.wikipedia.org/wiki/Whole_Foods_Market), and a [movie studio](https://en.wikipedia.org/wiki/Amazon_Studios), I imagine they can build a handful of data tools—and it’s probably inevitable that they do. 

When they do, however, it won’t be a single product. Instead, they, along with Google, Microsoft, and potentially Salesforce, are likely to offer a suite of services that play nicely together, similar to the cloud computing services offered by AWS, GCP, and Azure. These offerings would operate less as a centralized product—you could still mix-and-match the services you want—and more as a centralized billing plan. And like their independent counterparts, tools that cut across these layers—including Looker, in my view—will eventually get pulled apart to align with the horizontal grooves the industry is carving.

================================================================================

# The missing piece of the modern data stack

*Our cool new house needs one more plank in its foundation.*

---

![Not the modern data stack](https://substackcdn.com/image/fetch/$s_!6K4B!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F48cd5250-94f4-48c7-88bf-5b4ae03f968c_1000x568.jpeg)
*[Ok, this is an overdramatization.](https://snicket.fandom.com/wiki/Josephine_Anwhistle%27s_House)*

In the evidently tiny professional circles that I run in, the “modern data stack” is having a moment. The concept, which is a new framework to move data around an organization and make it available for people to use and analyze, is [inspiring conferences](https://mdscon.fivetran.com/), [historical retrospectives](https://blog.getdbt.com/future-of-the-modern-data-stack/), [listicles](https://www.snowflake.com/resource/6-paths-to-a-modern-data-stack-for-data-apps/), [how-to guides](https://mode.com/build-a-modern-data-stack/), and [companies themselves](https://blog.getcensus.com/announcing-our-series-a-from-sequoia/). A decade after *[The Economist](https://www.economist.com/weeklyedition/2010-02-27)*[ warned us we’d all soon be drowning in data](https://www.economist.com/weeklyedition/2010-02-27), the modern data stack is emerging as Silicon Valley’s proposed life raft. 

![](https://substackcdn.com/image/fetch/$s_!5cQM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F11f02e5c-acc1-4303-b332-a4436ef88171_1600x1271.png)
*[wondering if you’re an elephant](https://trends.google.com/trends/explore?date=today%205-y&geo=US&q=modern%20data%20stack,am%20i%20an%20elephant)*

While everyone’s definition of the modern data stack differs slightly (i.e., the tool they sell is *the* hub around which the whole apparatus spins[^1]), there’s little dispute over its general contours. An ingestion tool writes data from a wide variety of sources into a central warehouse; a transformation tool models that data in the warehouse, converting it from raw ores to usable alloys; a BI tool provides direct access to data so that it can be visualized and analyzed. Over the last year, a couple extra complications are becoming popular as well: [Reverse ETL](https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb) tools write data back into source systems, and monitoring tools track the health of the whole system.[^2]

In an ecosystem this dynamic—and, frankly, [this overrun with cash](https://www.cnn.com/2020/09/16/investing/snowflake-ipo/index.html)—a lot of new companies will be looking to define their niche.[^3] Some tools will claim to be a [reimagining of legacy tools](https://www.wework.com/tomorrow), or an [unbundling of legacy tools](https://a16z.com/2019/09/11/platforms-verticals-unbundling/), or a [bundling of legacy tools](https://news.crunchbase.com/news/fintech-startups-broke-apart-financial-services-now-the-sector-is-rebundling/), or “[Superhuman for X](https://toddgoldberg.com/posts/superhuman-of-x-startups.html),” or “[Clubhouse for Y](https://sifted.eu/articles/clubhouse-for-x/).” And some products will claim to be something entirely new, their own distinct yet critical category in companies’ growing collection of data apps.

Most new products won’t be that. The tools we have are, in fact, pretty good. While there are definitely improvements to be made, we aren’t lacking many foundational elements. 

Except for one: a metrics layer.

Though companies use data for a lot of things, one of the most important is also one of the most mundane: basic reporting on business operations. Employees across a company have to make decisions; to make those decisions, they need to know what’s happening. Which products do people like? Which marketing campaigns are attracting new customers? Who on the sales team is hitting their quota? For most companies, [data isn’t an AI-powered screenwriter](https://www.nytimes.com/2013/02/25/business/media/for-house-of-cards-using-big-data-to-guarantee-its-popularity.html); it’s just a simple narrator, telling people what’s going on. 

Companies solve this problem through what’s come to be called self-serve. The idea behind self-serve is that anyone at a company can get the data they need—they can be told what’s happening—without having to ask someone for help.

[As I’ve talked about before](https://benn.substack.com/p/self-serve-still-a-problem), self-serve is a misunderstood (or, at least, misrepresented) problem. Because the most common question people have is “How often did this thing happen?,” effective self-serve is less about complex analysis and more about metric extraction. People “want to choose from a list of understood KPIs, apply it to a filtered set of records, and aggregate it by a particular dimension. It's analytical Mad Libs—show me *average order size* for *orders that used gift cards* by *month*.”

Today’s current stack makes it easy to answer this question, but really hard to answer it consistently. **The core problem is that there’s no central repository for defining a metric.** Without that, metric formulas are scattered across tools, buried in hidden dashboards, and recreated, rewritten, and reused with no oversight or guidance. 

To see the problem, consider the journey that data follows to reach that dashboard. After being written into a warehouse (e.g., Snowflake) by an ingestion tool (e.g., Fivetran), data is updated by a transformation tool (e.g., dbt) several times, passing through a couple types of aggregations along the way:

To extract metrics from these tables, people have two options: They can pull from pre-aggregated rollups, or they can compute new metrics on the fly from granular dimension tables.

Rollup tables are typically generated by transformation tools like dbt, so the metrics in these tables can be consistently defined and reliably governed. However, because rollup tables are precomputed, there’s a practical limit to how many can be created. As a result, they’re often only built for top-level metrics, like active users or customer NPS. 

But self-serve analysis requires another level of depth—daily active users *for a particular customer segment*, or NPS *for a particular type of user*. Even with just a handful of metrics and segments, it’s all but impossible to precompute every possible combination.

Without a rollup to draw from, data consumers have to follow the second path: aggregate new metrics directly from dimension tables. That leaves the nature of the aggregation up to the person doing the analysis, and these aggregations are rarely simple. Counting weekly orders in Europe, for example, requires you to define week, order, and Europe. Do weeks start on Sunday or Monday? In which time zone? Do orders include those made with gift cards? What about returns? And are European customers those with billing addresses or shipping addresses in Europe? Are Russian customers European? Are British customers European?[^4] [^5] While all of this logic might live in the `rollup_orders` table, it isn’t necessarily in the `dimension_orders` table, meaning someone has to apply it on their own to do their analysis. This makes it incredibly difficult for people, especially people who aren’t analysts and aren’t familiar with the weird nuances that riddle most datasets, to consistently arrive at the same result. 

## A shared foundation

BI tools appear to solve this problem by offering ways for people to define on-the-fly computations in reports and dashboards. Through LookML in Looker, calculated fields in Tableau and Mode, and formulas in Sisense, analysts can configure visualizations to aggregate results in specific ways.

But this is a superficial fix, barely better than scattering this logic across Excel spreadsheets. Defining metrics in a BI tool localizes those definitions to that tool—or even worse, to individual charts or elements within that tool. A Tableau calculated field is only accessible in the dashboard that uses it; LookML is only accessible in Looker itself.

Local solutions don’t work because BI tools aren’t the only way companies consume data. Analysts and data scientists [answer complex questions](https://benn.substack.com/p/self-serve-still-a-problem) in ad hoc tools—work that’s often inspired by a change to a metric in a BI tool but is rarely done in that BI tool. Data engineers build pipelines from data warehouses into marketing automation tools or operational systems, ideally bypassing the BI tool entirely when they do. In both of these cases, LookML or the logic defined in a calculated field is inaccessible. So metrics get recomputed in new tools. In the best case, these calculations drift apart over time; in the worst case, they never match in the first place. 

A better architecture would do for metrics what dbt did for transformed data—make them globally accessible to every other tool in the data stack. Rather than each tool defining their own aggregations, the metrics layer is a centralized clearing house for how all metrics are calculated.

![The architecture of the metrics layer](https://substackcdn.com/image/fetch/$s_!Mriu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3c382242-eb8f-4187-a4e1-a9d08801ccaf_760x653.jpeg)
*The smiling database likes it.*

Looker and dbt hint at how this could be done, though neither tool is particularly close to solving the problem themselves. dbt showed us that the best path to make something universally accessible is through the data warehouse. dbt works not because everything connects to it, but because nothing needs to connect to it. It produces tables, and anything that can talk to a data warehouse—i.e., any modern data tool—can make use of what dbt produces. And Looker, and LookML specifically, provides a template for translating a request for a metric into a query that extracts that metric from a database in a consistent and governed way.[^6]

These tools, however, have foundational gaps that prevent them from operating as a metrics layer. dbt doesn’t run on demand, and LookML configures an application, not a globally accessible service. 

How might a proper metrics layer close these gaps? Fortunately, [the community is sensing this problem](https://basecase.vc/blog/headless-bi), and a few companies—[Supergrain](https://supergrain.com/),[^7] [Transform](https://transformdata.io/company/), and [Trace](https://www.hellotrace.io)—are already working on a solution. And while I’m not an expert on compilers or the inner workings of a database (I still don’t understand `GROUP BY`, which is apparently a [DSL hash map](https://twitter.com/vboykis/status/1280913148309049344), so…high speed internet from 2003?), I have my own ideas for what I’d like to see.

First, a metrics layer should be centrally configured, either in a language like LookML or in something that looks more like SQL. I could see the case for both. I’m biased toward SQL-based solutions, but YAML is a more natural configuration language.

Second, the tool should have a simple job: Take metric requests—let’s call them metric queries—as an input, and return SQL queries that extract those metrics as an output. 

Third—and most critically—other tools should connect to the metrics layer as though it were a database. The layer could live directly in the warehouse, as some combination of views and stored procedures, or it could act as a proxy that sits in front of a database. In both cases, any tool that connects to the database, either directly or through the proxy, could issue metric queries, which the metric layer would translate into a SQL query that the database runs. (This is why API-based solutions are unlikely to work. If people have to interact with the metrics layer through a REST API, analysts can’t access it in the SQL-based tools that they live in, severely limiting its applicability.) Metrics queries, which might look like the Mad Libs examples above, could go even further, and be combined with raw SQL (as a CTE, say). This would allow analysts to extend centrally-defined metrics by both modifying the SQL they render to, and by incorporating them directly into more complex ad hoc analysis. 

![How a metrics query might work](https://substackcdn.com/image/fetch/$s_!T1iy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69541f8e-b391-4700-9621-0e97fcf7c994_846x580.jpeg)
*[Come at me](https://mode.com/blog/should-sql-queries-use-trailing-or-leading-commas/)*

A global metrics layer like this would ensure that metrics are consistently calculated across every tool that connects to a database. By calculating metrics on demand, it uncaps the number of possible Mad Lib combinations that people can request. And by rendering directly as SQL, it’s extensible if an analyst needs to use a metrics query as a starting point for deeper, more complex analysis. 

Most profoundly, though, it fundamentally redefines how data is consumed, and the role BI plays in that process.

At first glance, the metrics layer doesn’t feel like a BI tool. This is code! BI tools have drag-and-drop interfaces! But that misses the point of those drag-and-drop interfaces, which is to make data accessible. If the query language is simple enough, text-based commands can actually be *more *accessible than the complex “code-free” UIs of tools like Tableau. We Google, after all, with nothing more than text. 

That doesn’t make BI tools obsolete; it just shifts them a different, and potentially more valuable, role. With a metrics layer, analytics tools can worry less about data modeling and governance and focus on creating a great experience for issuing metrics queries, for visualizing their results, and for extending those results with deeper, analyst-led explorations. In this world, BI is a creative workflow that supports analytical development and collaboration—*realizing the actual value in data*—rather than a piece of infrastructure centered around data governance and reliable reporting.

That, ultimately, is the biggest benefit of a metrics layer: It completes the foundation on which operational BI and exploratory data science both live, bringing these two functions together in ways they can’t be combined today. Without a central metrics repository, these two workflows—metric extraction in one case, and complex strategic research in the other—will remain apart. Even if they share the same database and the same transformed tables, the stories they tell will be inconsistent until they share the same metrics.

***Update**: Shortly after this post came out, Airbnb published an article about Minerva, their internal metrics layer. My thoughts on it are [here](https://benn.substack.com/p/minerva-metrics-layer).*


---


[^1]: Mode is the tool around which the modern data stack spins.

[^2]: This monitoring function, which is still finding its footing, is evolving in curious ways. More on that in a later post, probably.

[^3]: And there are *a lot* of new companies. YC alone funded 43 data companies in 2020, up from 11 in 2015.

[^4]: lol, a five-year-old Brexit joke.

[^5]: omg, Brexit was five years ago.

[^6]: As I mentioned in the piece on the problems with self-serve, this isn’t without its challenges. While Looker will compute the same configuration the same way every time, it’s difficult for people to always arrive at the correct configuration if there are a lot of options. People are just as easily overwhelmed by choice as they are technical hurdles.

[^7]: After getting interested in this topic, I made a personal investment in Supergrain, so I now have some bias in this area. That said, as someone who’s made an enormous personal investment in Mode, which is somewhere between adjacent to and overlapping with most pieces of the data stack, I’m directly or indirectly biased about nearly every modern data tool.

================================================================================

# Optimizing your Twitter profile picture

*How to get noticed. Or how to annoy people. We’ll find out.*

---

![](https://substackcdn.com/image/fetch/$s_!2P2K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F44518c9a-22f0-4e77-8a0e-1fb5b27217cb_1600x870.png)

The bar closed years ago. The t-shirt, though still tucked away at the bottom of my dresser, is more cobweb than clothing, only able to survive the most gentle of dryer cycles. The hair is mostly missing, buzzed off in the back and retreating from its post in the front. The Instagram filter, one of the originals that makes every shot look like an oversaturated late-night Polaroid, is gone, replaced by more delicate options worthy of candle-selling influencers. And the photographer moved on to a new state, a new career, and a new boyfriend. 

But the picture remains, proudly grinning at the top of my Twitter timeline and alongside every tweet, the headshot for my most prominent public profile on the world wide web, my avatar to the internet. 

![Life in 2010.](https://substackcdn.com/image/fetch/$s_!9Js6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa35420f0-220e-4b7c-b688-a105f9079837_400x400.png)
*Tweeting from 2010.*

The problem with taking down an old picture, however, is you have to replace it with something else—something that’s simultaneously flattering, creative, and captures you as you were and are and want to be, all while looking casually chill about the whole thing. After ten years of trying and failing to find exactly that photo, this time, I set out to steal someone else’s. I scrolled through Twitter looking for ideas; nothing spoke to me. I scrolled on. I paused at a tweet by [Nate Silver](https://twitter.com/voxdotcom/status/1286750513665658886): “Critics are calling Folklore Taylor Swift’s best album. It also begins a new chapter for her image.” Bad take,[^1] but also, weird take for Nate Silver? Then I realized it—I wasn’t looking at a Nate Silver tweet, but one from [Vox.com](https://twitter.com/voxdotcom). I had mistaken the [blaring yellow Vox logo](https://twitter.com/voxdotcom/photo) for the [blaring yellow book cover](https://images-na.ssl-images-amazon.com/images/I/81jUlwpaGVL.jpg) that used to be [Silver’s avatar](https://twitter.com/bert__n__ernie/status/1257423457110953984).[^2]

On one hand, it was an annoying bait and switch. On the other hand, neat trick. Every yellow dot subconsciously reminded me of Silver’s ad—promoting, appropriately enough, his book called *The Signal and the Noise*. In the middle of a sea of brown and beige heads, yellow [stood out like a data scientist](https://www.youtube.com/watch?v=Mjj2S4EYqks) on a TV panel of seasoned political anchors.

The episode made me wonder if I could pull off the same maneuver. As someone with a face for radio (and a voice for print) (and a brain for TV), why not abandon the traditional headshot in favor of a blistering flash of color and alarm, with no purpose other than to engineer a Pavlovian glance at my otherwise inane tweets?

And if I'm going to do *that*—if I’m going to replace my face with a searing eyesore just to pick up a few likes—I'm going all in. [Yellow is a good option, but is it the ](https://scrapsfromtheloft.com/2017/12/07/aziz-ansari-live-in-madison-square-garden-2015-full-transcript/#main:~:text=We%20all%20have%20that%20nightmare%2C%20and,wrong%20toothbrush.%20Should%E2%80%99ve%20done%20more%20research!%E2%80%9D)*[best?](https://scrapsfromtheloft.com/2017/12/07/aziz-ansari-live-in-madison-square-garden-2015-full-transcript/#main:~:text=We%20all%20have%20that%20nightmare%2C%20and,wrong%20toothbrush.%20Should%E2%80%99ve%20done%20more%20research!%E2%80%9D)* The only thing worse than a sellout is a tentative one. 

Which color, then, should I pick?

## The loudest person in the crowd

Put simply, my goal was to choose a profile picture that stands out among other pictures as much as possible. Though I suspect something lurid or obscene would score me some views (if not a ban), I set two parameters. First, to give myself a fighting chance of figuring this out, I only need to find a single prominent color, which I could use as the background in a more traditional picture. Second, this color should be, among all the colors on the color wheel, the one that contrasts the most with the average profile picture on Twitter. [^3]

My first step was to gather a random sample of 10,000 tweets and their associated profile pictures over the course of a day in April. 

![10,000 Twitter profile pictures](https://substackcdn.com/image/fetch/$s_!ay7Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9b8799c1-8ee0-408c-84f6-1274f72d0b9d_1000x1000.png)
*[you become a Holocaust denier](https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/)*

At this point, I came to the same realization I had when [I played around with how brands use color to represent themselves on Instagram](https://mode.com/blog/brand-colors-on-instagram/): Color science is complicated, and I don’t understand most of it. Figuring out which single color stands out the most in this rowdy mosaic requires a level of sophistication well beyond what my TV brain is capable of. So I simplified it—I squinted.

Well, sort of. Using the Python module [Color Thief](https://github.com/fengsp/color-thief-py), I extracted the dominant color from each picture, reducing my grid of 10,000 faces into one of 10,000 oversized pixels, each representing the most important color in its corresponding profile photo. 

I then plotted these colors on a circular histogram, where each slice represents a sliver of the [HSL color wheel](https://en.wikipedia.org/wiki/HSL_and_HSV). The higher the slice, the more prevalent that segment is among the 10,000 profile pictures. Each slice is then shaded to match the average color of all the pictures that fell into that segment.

![](https://substackcdn.com/image/fetch/$s_!iNom!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0deba2c8-b2e7-4e55-9ead-273c194b6df2_1812x954.png)
*[Tempest](https://www.google.com/search?q=tempest+video+game&tbm=isch&ved=2ahUKEwiu9MPX6p7wAhV8XjABHWkxCEwQ2-cCegQIABAA&oq=tempest+video+game&gs_lcp=CgNpbWcQAzICCAAyAggAMgIIADIGCAAQBRAeMgYIABAIEB4yBAgAEBgyBAgAEBgyBAgAEBgyBAgAEBgyBAgAEBg6BggAEAcQHjoICAAQCBAHEB5QuSZYpi1g2S5oAHAAeAGAAckCiAH9BpIBBzIuMy4wLjGYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=QTqIYO7POvy8wbkP6eKg4AQ&bih=1018&biw=1792)*

Given that most pictures are probably faces, it’s not surprising that the most common colors are various shades of brown. For people looking for good camouflage on Twitter, you can’t do much better than [#987359](https://www.google.com/search?q=%23987359&oq=%23987359&aqs=chrome..69i57.799j0j7&sourceid=chrome&ie=UTF-8).

For those of us looking for a siren, we have a couple options. One option is to pick the least represented color on the histogram, which is a close race between two ugly shades of green. 

![Green is the least popular color](https://substackcdn.com/image/fetch/$s_!ePuy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F40c6c038-5a80-4829-8373-df3afd721c97_1322x800.png)
*Can’t imagine why these colors aren’t popular.*

But this method has a problem, beyond requiring me to make the most searchable picture of myself on the internet that of a poorly lit sewer. While these colors are uncommon, they don’t necessarily stand out. If I’m going to represent myself with a pixelated portrait of the [Swamp Thing](https://en.wikipedia.org/wiki/Swamp_Thing), I don’t just want people to think it’s weird; *I want them to notice*. 

A better approach might be to invert each picture’s colors, turning our mosaic into an [Andy Warhol](http://v) of the everyday person. This spins the previous histogram 180 degrees, and makes the most common color—which, given the inversion, is theoretically one that would stand out against actual profile pictures—a pleasing shade of blue. 

![Blue is the most popular inverse color.](https://substackcdn.com/image/fetch/$s_!XsTs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F006c8aa3-599a-4f43-9e17-0fb7750224b9_1812x954.png)
*[Twitter feels the same, we’re just seeing it from a different point of view.](https://www.youtube.com/watch?v=QKcNyMBw818)*

This [color](https://www.google.com/search?q=%23648aa4&oq=%23648aa4&aqs=chrome..69i57.742j0j9&sourceid=chrome&ie=UTF-8), caught somewhere between [Goldman Sachs](https://en.wikipedia.org/wiki/Goldman_Sachs#/media/File:Goldman_Sachs.svg) and [Oral-B](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Oral-B_logo.svg/800px-Oral-B_logo.svg.png), definitely looks nicer. But it introduces another problem. Twitter’s profile pictures exist on opposite sides of the color wheel, primarily as shades of brown and blue. While pastel blues are the most common inverted colors, they’re the second most common unaltered colors. In my effort to be different, all I did was [join the second most popular clique](https://www.nytimes.com/2021/01/29/technology/join-us-in-miami-love-masters-of-the-universe.html).

Fortunately, we have one more option. The color wheel, counterintuitively, has corners. Its extremes are represented by eight points: the six colors that appear every 60 degrees, plus black and white. Using the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model), these are the colors that are produced by choosing different combinations of red, green, and blue values at the maximum or minimum of their respective scales.

![](https://substackcdn.com/image/fetch/$s_!GL67!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F315a4dd3-9320-43c6-8b51-b5bec584b4dc_1686x650.png)
*TIL pink is actually “magenta?”*

In color science, the visual distinction between colors is measured by [delta E](https://en.wikipedia.org/wiki/Color_difference#CIELAB_%CE%94E*), or ΔE. Because the eight colors shown above are on the extremes of the color wheel, no color can be further away (i.e., can have a higher average ΔE) from our sample of profile pictures than one of these eight. In other words, of all the colors on the visible spectrum, the best choice for my profile has to be among these eight.

![Average delta E by color](https://substackcdn.com/image/fetch/$s_!RKJk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac49f22-7473-4edc-9a6c-4f60bb75f1ba_1358x822.png)
*[Blue is the best, Jerry, the best!](https://www.youtube.com/watch?v=s1UOk6nVzpE)*

The improvement is dramatic. When these colors are mixed in with real profile pictures, they stand out just as obnoxiously as they were meant to. The grid below is divided into eleven columns, each mixed with one of the colors above. The first eight columns blend in the eight “edge” colors (starting with black and ending with white). Compare these columns with the barely visible colors in the last three columns, which substitute some pictures with the earlier shades of green, blue, and brown.

![The best Twitter profile pictures](https://substackcdn.com/image/fetch/$s_!MzK6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F928fbdb9-2df4-49d1-906d-636d12e1fa49_1000x1000.png)
*[Three of these things are not like the others, like a rainbow with all of the colors.](https://genius.com/Taylor-swift-me-lyrics)*

Before declaring blue the winner, it’s worth considering one more detail. Profile pictures aren’t just fighting with other pictures for attention; they also need to stand out from the app in which they’re embedded. 

Twitter [offers three app background colors](https://twitter.com/i/display): white, black, and “dim” (it’s a [dark blue](https://www.google.com/search?q=%2317202a&oq=%2317202a&aqs=chrome..69i57.224j0j9&sourceid=chrome&ie=UTF-8)). Though blue contrasts the most against the white background, green is more prominent on the black and dark blue backgrounds. Between blue and green, the average difference across three backgrounds are nearly identical.

![Pictures compared to Twitter background colors](https://substackcdn.com/image/fetch/$s_!yqI0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6797c34-8dbd-4c36-a0e1-49ab293cfb86_1498x826.png)
*If nothing else, you have to appreciate red’s consistency. *

On the whole, blue is probably the best choice. It’s slightly more distinctive than green compared to the average profile picture, and it stands out the most against the default white background. But green is a very close second, dark backgrounds seem to be more common among frequent Twitter users, and replacing my profile picture with a solitary blue emblem could be interpreted in a [very different way](https://en.wikipedia.org/wiki/Thin_blue_line). Plus, I like green, and I’m not above [weighing aesthetic considerations over empirical ones](https://mode.com/blog/should-sql-queries-use-trailing-or-leading-commas/).

So green it is—scorching, unapologetic, and [crackling new](https://twitter.com/bennstancil/photo). Time to go get some likes.[^4]


---


[^1]: It’s *1989*, and it’s not close.

[^2]: He’s since changed his picture has [to the field](https://twitter.com/NateSilver538/photo) that whirs and beeps at you at the eye doctor.

[^3]: To create the perfectly optimal picture for myself, I should gather the profile pictures of all the people my followers follow, which are the pictures I’m currently competing with for attention. Twitter’s API makes doing this inordinately difficult. Because my own feed is probably partially representative of the feeds my followers see, I also ran this analysis against the pictures of the people I follow. This approach produced a nearly identical conclusion as using a random sample of pictures, implying not only that personalization is unnecessary, but also that there's some sort of central limit theorem of profile pictures, where any sufficiently large sample will always have a similar tonal composition. I also adjusted both analyses so that people who had larger followings got more weight; the results were again the same.

[^4]: But probably, blocks.

================================================================================

# Is Minerva the answer?

*A few thoughts on Airbnb’s metrics layer.*

---

![Professor McGonagall, solving data problems.](https://substackcdn.com/image/fetch/$s_!6b4i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdae05b72-8b2a-4791-8cc1-30b3cb2480ab_953x540.png)
*[source](https://www.syfy.com/syfywire/professor-minerva-mcgonagalls-10-best-moments-in-harry-potter)*

After posting [my proposal for a metrics layer](https://benn.substack.com/p/metrics-layer) two weeks ago, a number of folks reached out to share their take on the problem, and a few recommended potential solutions. But the most impressive reaction came from Airbnb’s data team. In response to the post (I assume), they rearchitected their entire data stack around the idea, built a central metrics repository for thousands of metrics, integrated it with all of their other major data tools, *and *[published a comprehensive blog post](https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70) explaining the whole thing.

It was quite the eight day turnaround. So when several people asked me what I thought about what they’d built, my first answer—”give me a couple weeks to think about it”—felt pretty underwhelming. Though I’m not sure my second answer is any better, I can at least make it longer.[^1]

## What I like about Minerva

Minerva takes on precisely the right problem: It’s really hard to use the same metrics consistently. As the authors’ point out, despite Airbnb having well-maintained “core data” tables—and while they don’t mention it, a famously strong data science team—when execs asked “simple questions like which city had the most bookings in the previous week, Data Science and Finance would sometimes provide diverging answers using slightly different tables, metric definitions, and business logic.” This perfectly highlights the biggest gap in today’s data stack. We have the tools for governing tables, and we have tools for governing dashboards and analysis, but we’re still pretty sloppy about governing the space between the two. Because we don’t manage metrics globally, people define and calculate them in different and often bespoke ways. And the more tools and teams that are involved, the worse the problem gets. 

Minerva, as their graphic shows, was built to literally bridge the gap—to add the layer—between tables and consumption. Structurally, this is exactly the piece our puzzle needs.

![Minerva and the metrics layer](https://substackcdn.com/image/fetch/$s_!Ezr5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb51d71-97b1-4187-857a-4fff023f7f06_760x653.jpeg)
*Who wore it better?*

The Airbnb team took Minerva a step further than I proposed, integrating it with Dataportal, their data catalog. Though the synergy[^2] between a metrics repository and a data catalog is obvious, Airbnb’s integration unlocks a subtle but profound shift in data discovery.

When we think about documenting data within an organization, we tend to think about documenting tables and the columns within them. On the surface, this makes sense, and not just because tables *are *data; tables are also how analysts model data in their heads. As Fishtown’s Claire Carroll outlined in [her insightful post](https://discourse.getdbt.com/t/writing-models-backwards-an-unorthodox-approach-to-data-transformation/2287) about writing queries to answer business questions, her first step is to sketch out the table she needs to answer the question, and work backwards from there. This way of thinking is almost required for analysts who work with relational data; do it enough, and it becomes hard to see data problems in any other way. Just as [musicians hear music differently than the rest of us](https://youtu.be/Zd_UcjMusUA?t=76), analysts see structures in data that others don’t.

But most people aren’t analysts. They don’t think of data as a set of tables, columns, and relationships; they think in terms of metrics and charts. Data isn’t a hosts table and a bookings table with a join key; it’s bookings by month, bookings per host, and average booking price. 

Backed by Minerva, Airbnb’s Dataportal presents data through this lens—in the language of its consumer, not its creator. Most self-serve tools provide ways for people to manipulate data with drag-and-drop interfaces that are abstractions of a SQL query. Minerva goes further, hiding not only the SQL query from its users, but also the very tables and fields themselves.[^3] Rather than the usual data exploration, it’s [pure metric extraction](https://benn.substack.com/p/self-serve-still-a-problem). That’s a clever step forward.

## What I’d do differently

My biggest gripe with Minerva is how you “query” it. As best I can tell, metrics are extracted from Minerva via API; from the [screenshots of the Python integration](https://miro.medium.com/max/4800/0*AvWgz6qImCriZUBn), it looks like the API lets people request metrics over date ranges, with filters, and grouped by different dimensions.

As I mentioned in my initial post, API-based solutions don’t work for analysts working directly in SQL. Though I don’t know how analysts work at Airbnb, in the broader market, most analysts’ work starts in SQL; for many, it also ends there. Unfortunately, Minerva appears to be inaccessible to these workflows. Analysts, when inevitably asked to explain why some KPI is yo-yoing across a Minerva-backed dashboard, will eventually have to recreate that metric, in SQL, on raw data in their warehouse. The more fluid this process, the faster the analyst can answer the question—and just as importantly, the more likely it is that their results match the canonical standard maintained in Minerva. 

In fairness, it’s worth noting that the R and Python clients are nodding in this direction. If most analysts work in these languages over SQL—which may be the case at Airbnb; I don’t know—these clients may be all you need. But given the ubiquity of SQL in modern analysis these days, that’s likely more of the exception than the rule. 

My other objection to an API-based metrics layer is that it’s harder to adopt. Metrics are useful in lots of tools, including dashboards, data catalogs, A/B testing and ML platforms, and operational tools like marketing automation software. Because Airbnb built most of this infrastructure in-house, they were able to integrate them directly with Minerva’s API. It’s work, but it works. Most of us don’t have that luxury. We buy these tools from different vendors, and can only use a metrics layer if each vendor builds a bilateral integration with it. If Minerva operated through a centralized data warehouse instead, most data consumption tools—which often interact with warehouses more or less directly—will “integrate” with the metrics layer automatically, just as they can “integrate” with dbt. 

I have a final question about how metrics are defined. The screenshot of the COVID-19 dashboard configuration is in YAML, so I’d guess that Minerva uses something similar.[^4] Though YAML is an obvious choice, my heart wants something rooted in SQL. SQL is more expressive than YAML; analysts think in tables, and so does SQL; and SQL can also be pulled apart into intermittent steps, making it easier to follow its logic. YAML configurations are harder to trace. That said, I’m not exactly sure how to configure metrics in SQL. But I’d be thrilled if someone figured it out.

In the end, more than any feature or technical detail, Minerva’s adoption is its headline. Over the last decade, companies like Airbnb have been the pioneers for the rest of us, figuring out what problems we’ll run into in a few years and charting paths through them. If Minerva solves as big of a problem at Airbnb as its usage indicates, we’re all likely to need something similar soon. Minerva—and just as importantly, the openness of the Airbnb team to share details about it—put us one big step closer to getting there.


---


[^1]: See also, “Why don’t you use Twitter instead of Substack?”

[^2]: Synergy! Deliverables! Running point! [Read this article!](https://www.vulture.com/2020/02/spread-of-corporate-speak.html)

[^3]: There’s a (very nerdy) debate to be had (among a very small group of people) on how aware people should be of the tables that underlie the data they see. On nearly every website we use, we interact with lots of structured, relational data. Most of us rarely think of it as such—many people probably aren’t even aware of it—and we can all use those apps just fine. But in data applications like self-serve BI tools, we put those tables at the front, so much so that these apps are difficult to use *without* thinking about those tables. Do we do that because fields and tables are actually important for people to understand, or because the data comes in fields and tables, and we haven’t come up with a more creative way to repackage it? I lean more toward the latter.

[^4]: YAML: Yet another magic language.

================================================================================

# It's hard to hate up close

*Bias has no perfect balance.*

---

![](https://substackcdn.com/image/fetch/$s_!ubmk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5fb5d04e-b9ee-41bc-a679-a8ed7d1682be_640x360.jpeg)

*I’ve learned that it’s harder to hate up close. *

*– Michelle Obama,* Becoming

When I first read this line, I saw it as advice to walk in other people’s shoes. Our divisions, Obama is telling us, come from our distance from one another. If we get to know those we disagree with, we’ll realize we aren’t all so different. As is typical of the Obamas’ rhetoric, it was a line about hope and optimism for a more understanding and compassionate world.

But it has a dangerous corollary, highlighted by a tweet from Paul Graham:

Martínez is a former Facebook product manager turned author turned tech Twitter talking head. He was recently hired by Apple, only to be quickly let go after number of Apple employees petitioned against the hire because of comments he made about women in his autobiography *Chaos Monkeys*. *The Verge* has the [full story](https://www.theverge.com/2021/5/12/22432909/apple-petition-hiring-antonio-garcia-martinez-chaos-monkeys-facebook).

The incident sent everyone barreling toward another fight about cancel culture. For now, though, I want to focus on Graham's defense of Martínez. 

Graham’s argument is pretty typical of those defending the "cancelled:"[^1] Yes, he’s done some things I disagree with, but I know him, and he's actually good guy. You can’t judge him from so far away. Up close, you’d wouldn’t hate him.

This is the dark inverse of Obama's quote. If we get to know someone—either personally or by running in the same professional clubs—we don’t just come to understand them as Obama suggests; we also start to like them. The same congeniality that sands down our biases against starts to paper over their real faults. Closeness to someone doesn't make us know them better—it makes us take their side, for better *and* for worse. 

We’re all susceptible to it. Consider, for example, how you feel about your coworkers. We’ve all worked with people that frustrate us, or that we think are bad at what they do. Now ask yourself, are any of your friends bad at their jobs? Are they the people who would irritate you at work? Odds are, some of them are—but our friendships compel us to instinctively take their side, covering up faults we so easily see in others.

This example underscores another gap in Graham's reasoning, which is also common in arguments against cancel culture. Contrast Graham’s judgement of Martínez with that of the Apple employees who signed the petition about him. Graham understands Martínez because he knows him; his personal relationship helps him see past his public persona. But Graham also understands the people at Apple, not because he knows them, but because he doesn’t. He can assess them objectively, based on their one perceived sin, because his judgement is clouded by personal feelings. For Martínez, an entire autobiography isn’t enough to know who he really is. For the Apple employees, a single signature is. That isn’t logic; it’s just defending a friend. 

That’s fine—we’re all humans, and our personal relationships *should* matter. But this instinct highlights why accountability to public opinion—or if you prefer, the “woke mob”—matters, regardless of how you feel about this particular incident. The powerful are often friends, or at least congenial members of the same fraternity.[^2] If they’re left to judge one another, they’ll judge themselves as Graham did Martínez, or as [Dianne Feinstein did Lindsay Graham](https://www.washingtonpost.com/nation/2020/10/16/feinstein-hug-graham-hearings/), or as [Sarah Silverman did Louis C.K.](https://www.huffpost.com/entry/sarah-silverman-defends-louis-ck-sexual-misconduct_n_5bce1592e4b0a8f17eef3af7) And when that happens, we already know the verdict: I know them too well to hate.


---


[^1]: Of course, it must be pointed out that being turned down for a job by Apple is a long way from “depriving someone of their livelihood.” [Millions of people](https://www.quora.com/How-many-candidates-do-Google-Facebook-Apple-Amazon-and-Microsoft-interview-for-software-engineering-positions-in-the-US-every-year-What-are-the-acceptance-rates) have suffered the same fate, and we don’t grieve their cancellation.

[^2]: Notably, powerful people include not just those in legislatures and board rooms. Rich people, white people, men—these are also powerful groups that give each other a benefit of the doubt that they don’t afford to others.

================================================================================

# Analytics is a mess

*You can’t stop it, and you shouldn’t try to contain it. *

---

![](https://substackcdn.com/image/fetch/$s_!maCM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F736d7fbf-6599-4159-83e2-e9568e00447c_880x702.jpeg)

Looking back, I’m not sure if I liked math or if I was scared of writing. The prospect of spending most of my time in college writing papers—staring at a blank page, endlessly editing drafts, never knowing if what I was producing was right, much less good—drove me away from most majors. I took refuge in the math and economics departments, where the problems were structured, bounded, and concrete, and the homework was never an essay. Numbers were tidy, I thought, not messy and creative, and I found comfort in that.

When we think about analytics and data science, we often apply the same connotations. Data is factual; math has rules; science is objective. While we can be clever about how we solve problems, there is little space for creativity in what we deliver (I’ve never heard a single executive say, “This dashboard is great, but can you make it more...*creative?*”). Our jobs, especially those centered around reporting and BI, are to search for the capital-T truth, and represent it as accurately as possible.

The problem is, this is all a lie.

Data isn’t objective, and analysis isn’t structured. It’s just as creative—and just as messy—as the papers I ran away from in college. 

Take an example from Mode. We wanted to measure how well Mode performs against our competitors, and if that performance is getting better or worse. We all agreed that we could track this through our win rate. Informally, win rates are easy to understand: Divide the number of people who choose to buy Mode by the number of people who consider it. The better Mode performs in the market, the more likely people would be to choose it.

But we can’t define metrics informally; our instructions have to [be precise](https://www.youtube.com/watch?v=Ct-lOOUqmyY). And for metrics like win rates, there are a lot of devils in the details. For example, we have to decide what it means for a customer to consider Mode. Is it any individual who signs up for a Mode account? Is it any company that creates an account for their organization? What about companies who reach out to us for a demo but don’t create an account? What about people who create an account but decline the demo? Do we include people we reach out to, or just those who come to us? Do we only consider companies that enter formal trials managed by our sales team? Does the denominator include only the deals we’ve officially closed, or any deal, open or closed, that didn’t choose to buy Mode after a certain period of time? Do we aggregate our win rate by the date that they decided to evaluate Mode, or the date they made their decision to buy it or not? Do we measure it monthly, quarterly, or over a rolling interval?

Like my freshman year essay about mortality in Emily Dickinson’s poetry,[^1] there are no objectively right answers.[^2] There is no correct win rate waiting to be unearthed; one version isn’t true while another is false. Each version is equally accurate because they are tautological: They measure precisely what they say they measure, no more and no less. Our job as analysts isn’t to do the math right so that we can figure out which answer is in the back of the book; it’s to determine which version, out of a subjective set of options, helps us best run a business.[^3]

This choice isn’t a technical one, but a creative one, built on top of “messy” questions, like “How easy is the metric to understand?,” “Do people already have pre-existing ideas of what this number might mean?,” and “How much do we think we can learn from it?” These are questions we can’t answer without trying stuff out and seeing what happens.

Eventually, this expansive exploratory phase reverses. Some numbers make sense; others don’t. Draft metrics are edited down. After a tuning and refining phase, a clean, standard metric emerges. 

## A mess is the process for progress

When we look at companies with mature data practices, we only see the final, stable metrics and dashboards. This, combined with assumptions about the objectivity of math and numbers, leads us to believe that dashboards and BI development should be a fairly linear and direct journey. Standardized reporting, we think, can be built like a house: We create a blueprint and then follow it, building the foundation, the frame, and the finish, one after the other. 

But we aren’t building prefab houses. Because no company is the same,[^4] measuring a business, as was the case for us when we were measuring our win rates, is a creative process. Inevitably, even the best laid reporting plans give way to a lot of exploratory messes. Each potential metric produces a bunch of analyses to assess it; each analysis produces more questions and ad hoc offshoots. Multiply this by all the metrics and dashboards on your blueprint, and complicate it by constantly shifting the business underneath it, and the development process looks less like an organized construction site and more like an [artist’s studio](https://www.artistrunwebsite.com/inspiration/1474/Studio+Sunday%3A+Alexander+Calder) or a [writer’s desk](https://www.reddit.com/r/pics/comments/8h4u14/this_is_the_new_york_review_of_books_office/).

This feels wrong—and if you’re like I was in college, it's uncomfortable. But it's not only normal; it's also necessary. We arrive at better answers when we let analysis be generative and spontaneous. Often, the most useful things we find are the things we weren't looking for. 

Unfortunately, teams sometimes struggle in these disorganized moments. They see the mess they've created, and assume something has gone terribly wrong. In that discomfort, teams feel the need to reset, to burn the whole thing down, and replace the tools and processes that got them there. 

When this happens, it often looks like the right decision. The prior tools and processes were sloppy; the new tools and processes are indeed cleaner and tidier. But this is less about tools and more about timing: The new tools and processes typically get introduced right as the exploratory process is consolidating. If a writer replaces their desk just as they begin editing their book, their new desk will be cleaner, but for reasons completely unrelated to the desk itself.

Obviously, data tools and development processes aren’t desks, and they do have functional differences that go well beyond those of two different desks.[^5] But those differences matter less than the problems the tools are solving. Companies’ first analytics tools, regardless of which ones they choose, will always be disorganized, not because the tool is inherently so, but because the creative process is. 

When approaching any analytical problem—from something small like answering a single question to introducing a data practice at a company that’s never had one—we should expect the first steps to be uneven and uncomfortable. Rather than trying to stifle or control this phase, we’re better off having a plan for how to tidy it up later, by working in a sandbox or reserving space for the polished final drafts that we’ll eventually produce. If you force too much structure on a creative exercise, you’ll end up like me when I did the same thing in my English literature class—with a disappointed professor, and the worst grade on your transcript.[^6]


---


[^1]: Because I could not stop my query— He kindly stopped for me —

[^2]: But there are, as Professor Franco told me, very much wrong answers.

[^3]: You could argue that this is true of all aggregations of data. Metrics aren’t “real;” they’re abstract constructs that we use to simplify and understand the world. All metrics, from win rates to inflation to quarterback ratings, are full of arbitrary choices about what we include in them and what we don’t. The concepts they represent don’t exist outside of the metric itself. Which is a bit of a trip to think about, and maybe a topic for a future (philosophically indulgent) post.

[^4]: While it’s true that many companies are similar, the small variations make all the difference. Slack and Microsoft Teams, for instance, might look 90 percent the same. They both sell per-seat subscription software to business users. However, the 10 percent difference—Slack sells a standalone product and Microsoft Teams is often sold as part of the Office suite—dramatically changes how each company should measure daily active users, the cost of acquiring customers, and every other standard SaaS metric. This is why I’m skeptical of analytical templates. Even among companies that appear to do the same thing, there’s no one-size-fits-all metric. Maybe a topic for another future post.

[^5]: [Maybe.](https://www.wayfair.com/baby-kids/pdp/zoomie-kids-vidrine-kids-study-desk-w005159799.html?piid=)

[^6]: Success is counted sweetest By those who ne'er succeed.

================================================================================

# To the man in 18A: Open the window, you degenerative cusk eel

*Decency dies in darkness.*

---

![](https://substackcdn.com/image/fetch/$s_!g3wl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd76798-bcc1-4a8d-b452-d055e85971e9_956x678.png)
*Source: Hell.*

Outside that window, as far below the ocean's surface as we are above it, live the earth's most revolting creatures. These fish, generously named given their [unkind form](https://www.cbsnews.com/pictures/crazy-looking-fish-from-deep-sea/), float listlessly in the submarine darkness, with nothing to occupy their time—if the concept holds any meaning in their eternal abyss—other than drifting about, waiting for some pathetic crumb to fall into their agape mouths.

They weren't always this way. It was their environment that did them in. Pinned down by mile after mile of impenetrable sea, their [faces sprouted lanterns](https://en.wikipedia.org/wiki/Anglerfish) and their [eyes fell out of their heads](https://www.scienceandthesea.org/articles/201801/faceless-fish-deep-deep-sea). In their world, only monsters can survive.

Fortunately, 18A, we humans have kept our wits and our eyes, our reward for typically being clever enough to avoid such inhospitable habitats. But every once in a while, we plunge ourselves into a hell worse than the deepest trench in the ocean: the inside of an airplane, the nightmare we regrettably find ourselves in today.

When we fly, we pressurize ourselves and hundreds of irritable strangers in a narrow tube, glue two metal boards to its sides, hang a rocket the size of a bulldozer to each, and launch the whole thing into low orbit. It's a grave situation, and that's *before* we hand the entire operation over to United. Like the fish forsaken at the bottom of sea, we've all developed ways to cope. 

There are the drinkers, who sedate themselves on white wine and Jacks and Coke—never beer—at the beginning of a red-eye, and on orange juice and smuggled mini bottles of gin at the end of it.

There are the animal lovers, who try to fit their Bernese mountain "service" dog under the seat in front of them, and assure everyone, over its whines and shuffling, that he never does this and he's a good boy.

There are the workaholics, who crank through hundreds of emails on an company-issued HP Pavilion, emails so urgent that polite asks—and federal regulations—aren't enough to compel them to stow their electronic devices, all so that they can blast out another punctuation-free update declaring "in air now but will review when on the ground thx".

There are the narcoleptics, who travel in sweatpants, socks, no shoes, an oversized UMass hoodie, a Snuggie, a neck pillow, an eye mask, a teddy bear, and a value meal from Wendy's, and refuse to be kept awake by turbulence, crying children, or their 32-ounce Dr. Pepper.

There are the zombies, who stare blankly at the seat in front of them, their eyes glazed over and their heads full of static, drifting in and out of awareness of where they are, who they are, what they are.

And then there are the deep sea fish.

These are the unholy creatures who, upon finding their seat, take a couple glances at the bustling tarmac outside, and push the window shade shut, like a pedestrian waving away a Greenpeace canvasser. To them, the tiny perforations in our barrel of steel and precious life aren't an engineering marvel, or a portal to the divine horizon through which were hurtling, or even a way to get a little bit of pleasing natural light in humming, sterile tube; no, they are irritants to be extinguished, distracting from the more important business on their screens. So they blot them out, trapping us in darkness, illuminated only by the glow of a seat back screen looping ads for a new direct flight to Johannesburg.

You, sir, are one of these creatures.

But the glare, you'll say, the glare is making it difficult to see your iPad. You would open the window, but you *can’t*.

Nonsense. Turn away from the unnecessary Powerpoint you're creating to excuse your unnecessary trip to Dallas, to help an unnecessary pharmaceutical company sell an unnecessary drug to make an unnecessary profit. Instead, take in the unobstructed view from the top of the earth, higher than any point on the planet, and multiples higher than the shallow hills you hike to to peek above a treeline at a few fields and creeping suburban sprawl. Take in miles of the meandering Mississippi, the pulsing lights of a some unknown city crackling to life on Saturday night, the unyielding teeth of the Rockies, covered in glistening snowy enamel, the fading amber sunset refracting through streaks of cirrus clouds, or the entire arm of Cape Cod, with views stretching from Providence to Provincetown in one breathtaking sweep.

But, you might object, the view from my local trail is earned. Its beauty is both seen *and* felt, by the tired legs and blistered feet that made it possible.

And so too is ours! A few lazy clicks on [kayak.com](http://kayak.com/) didn't put us here—centuries of dreams and tireless innovation buoy our ascent! For ages, our ancestors longed to defy the indomitable forces of gravity, to join the birds and clouds in unencumbered aerial locomotion. And now, at this privileged edge of our long anthropological timeline, at this culmination of millennia of patience and tragic trial and error, against the odds and the natural laws governing our universe, we can! What Icarus gave for a fleeting moment of flight! Oh, what we refuse to give—a couple winks of sleep, an hour of doomscrolling on Slack, another rerun of *Beat Bobby Flay*—to take in ours! 

Do your duty, 18A. Open the shade. Show us the splendor you so casually discard behind a cheap plastic shutter. Be our host to the heavens. For without your mercy, we are trapped, as damned as the fish at the bottom of the sea, with nothing to look at but *Failure to Launch* and 17B's growing pile of Sutter Home chardonnay bottles. We beg of you: A magnificent world is out there, but in here, the darkness is devouring us, and we are transforming into monsters.

================================================================================

# The self-serve shibboleth

*We all like to talk about it. But how do we fix it?*

---

![](https://substackcdn.com/image/fetch/$s_!jHMG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5cec544a-78ea-49f7-a0a0-76ac661bea2f_512x341.jpeg)
*[“What exit?”](https://www.elmo.ch/private/Stories-from-a-great-country/what-exit/index.html)*

I'm convinced that every field has “senior shibboleths:'' ideas that sound contrarian to the uninitiated, but are, among people with more experience, pretty conventional. These ideas, which are sometimes wise, sometimes trite, and sometimes dangerously wrong, might start as novel theories, but they eventually become secret handshakes for sorting who’s in the club and who’s not. When someone shares one of them, we signal our membership by nodding along knowingly. The fastest way to be taken seriously by people who take themselves seriously isn’t coming up with your own ideas, but knowing when to quote-tweet “^ this.”

Spend enough time with industry veterans, and you’ll pick up on their passwords. Senior developers joke that the hard part of engineering isn’t technology, but people. Political talking heads on Twitter wink at each other by saying Beltway intrigue is boring and meaningless to “real” Americans.[^1] Baseball pitchers show they’re in the know by complaining that they’ll forgive a bad umpire, but not an inconsistent one. 

Professional data analysts are no different. We tell ourselves that [dashboards are bad](https://twitter.com/EmilyGorcenski/status/1397065798762569730), that [SQL is more useful than machine learning](https://cyberomin.github.io/startup/2018/07/01/sql-ml-ai.html), and that [analytics is as much about asking questions as it is about answering them](https://www.sigmacomputing.com/blog/insightful-analysis-begins-with-asking-the-right-questions/). And in recent weeks, a new  shibboleth is taking shape on Analytics Twitter:[^2] Self-serve analytics is a mirage.

[Grant Winship](https://twitter.com/grantwinship/status/1392993870594682882) first shared this idea, and [lots](https://twitter.com/sarahcat21/status/1394329777557839873) [of](https://twitter.com/chrisalbon/status/1394330985609654272) [other](https://twitter.com/emilieschario/status/1393000303503548416) [people](https://twitter.com/slaterstich/status/1394716957195350029) agreed.[^3] [Never too ashamed](https://benn.substack.com/p/twitter-profile-pictures) to shill for some Klout and likes on Twitter, [I joined the pile on](https://twitter.com/bennstancil/status/1394362958361370630). 

Most of the arguments, including mine, followed the same arc. Self-serve analytics, which promises to open up vast troves of “insight” by making data accessible to everyone at a company, doesn’t actually work. While self-serve tools make it possible for anyone to extract and visualize data, they don’t imbue their users with the skills required to make sense of data and all its complications. Without those skills, data’s not that useful.

This emerging shibboleth is one of the wise ones; these are all good points. Analysis *is* more than hard math and a spreadsheet. And collectively, we’ve gotten good at making them. But they raise a rather obvious question that we’re not nearly as good at answering: *What do we do instead?* The problem self-serve is meant to solve—that there are more questions than analysts, and we don’t have time to answer them all—is still a problem, even if self-serve is a lousy solution. To steal an analogy from yet [another person](https://twitter.com/sethrosen/status/1394384866779844610) you should be following on Twitter instead of reading this Substack, we can stop prescribing ineffective medicine, and the patient will still be sick.

# The manic pixie business user

This predicament reminded me, of course, of Zach Braff.[^4]

For at this moment, we’re all Zach Braff [at the end of ](https://www.youtube.com/watch?v=vQ4HmjXLYCI)*[Garden State](https://www.youtube.com/watch?v=vQ4HmjXLYCI)*. We thought we knew what to do, realized it was a dumb, awful idea, abandoned it, and now have no plan for what’s next other than to breathily whisper “What do we do?” at our (business) partner as *[Let Go](https://www.youtube.com/watch?v=13WAhlE02ew) *rises in the background.[^5]

Surprisingly enough, Zach Braff also provides some decent advice. We should start by doing exactly what Zach Braff didn’t do in *Garden State*—we should treat our partners as more than [shallow characters defined by their relationship with us](https://en.wikipedia.org/wiki/Manic_Pixie_Dream_Girl), but as people with depth, needs, and motivations of their own.

The phrase “self-serve” typically refers to some vague process by which people can answer their own questions to make decisions. But we often aren’t terribly specific about which questions they want to answer, the decisions they want to make, or even who “they” are. Instead, self-serve is often better understood vis-à-vis its relationship with the analyst, not the business user (whatever *that* means) it's purportedly serving. We’re the protagonist; they’re the nameless stock character. 

Because we under-develop who they are, we generalize their motivations, which, in the true spirit of the manic pixie dream girl, become extrapolations of our own. They want to do analysis, just as we do. The challenges we face in our analyses—confounding variables, spurious correlations, outliers, and so on—are the challenges they will face in theirs. And because we have to deal with these things, so too must they.

But what if that’s not actually true? What if our motivations and problems are actually different?

A conversation with a prospective Mode customer started to break this fever for me. The prospect—an analyst on a small data team serving a large company—told me that they were looking for a new self-serve tool to help “people use data in a more meaningful way without having to ask his team for help.”

“The classic self-serve trap,” I thought. “This analyst doesn’t see why this is futile. I need to teach him the handshake, and welcome him to the club.”

Before I had a chance to explain to him why he was chasing a mirage, he continued: His biggest concern was making sure that department leaders have access to consistent reporting. He wanted the business to run on a few critical KPIs, and saw his job as making those metrics available to anyone who needed them. 

In telling me this, he inadvertently taught me a lesson much bigger than anything I could teach him. He showed me that my reflexive dismissal of self-serve was wrong because my definition of self-serve was wrong. I assumed he meant one thing; he had done the research to figure out he needed a different thing. He wasn’t looking to enable people to do the rich analysis that we analysts do, nor did he want to solve some generic self-serve problem that he poorly understood; he just wanted to provide an easy way to pull metrics.

Any analyst looking for self-serve solutions—or rejecting them as lost causes—should do the same thing this prospect did, and ask what exactly you mean by self-serve. Who’s going to use it? What do people actually want to do? If we answer these questions, we may find that the problem is easier to solve than we thought.

[As I’ve said before](https://benn.substack.com/p/self-serve-still-a-problem), I think most people actually want what the prospect wanted: a tool for metric extraction. “They want to choose from a list of understood KPIs, apply it to a filtered set of records, and aggregate it by a particular dimension. It's analytical Mad Libs—show me average order size for orders that used gift cards by month.” Analysis of this form doesn’t end with a new chart built on top of novel manipulations of data; it begins with extracting a set of well-understood charts, laying them out on a desk, and figuring out the story that emerges from them. This problem is much less vulnerable to the pitfalls we commonly attach to “self-serve.”

This is why I believe a [metrics layer](https://benn.substack.com/p/metrics-layer) is such an important piece of the modern data stack. It wouldn’t just fill a gap in our current toolset, but it would also underpin a workable self-serve solution that’s built for the real people who use it, and not an unworkable one for the characters we imagine who might.

# An alternative ending

Just as we can’t generalize business users as superficial reflections of ourselves, we also can’t characterize all of them as mechanical metric extractors. Sometimes, their questions do call for deeper analysis. In these cases, how do we escape between the rock of having too few analysts to answer these questions, and the hard place of analytical thinking being hard?

Bluntly—hire more analysts.[^6] If you want to build a great brand, you invest in great marketers. If you want to build great technology, you invest in great engineers. And if you want to make great decisions with data, you...buy great self-serve tools? No, you hire great analytical thinkers. 

The good news is that self-serve tools make this much easier. There are lots of people—economists, sociologists, political scientists, historians—who are trained to think analytically. The only thing preventing them from being great data analysts is the technical training to extract and manipulate data—which is the exact gap that self-serve tools are meant to close. And, if we hired more people like this, we could close [another notable gap](https://docs.google.com/presentation/d/1lmfSd2eGPjPwQcomeQGdFFclxB18aZE0Fj0y1dibUYs/edit#slide=id.g59cf0171f5_0_303) in our industry.[^7] 

Until then, though, our club will remain the same, with the same members, the same problems, and the same punchline—self-serve is a lie—as our password.


---


[^1]: As I said, not all shibboleths are [actually correct](https://www.washingtonpost.com/opinions/2021/05/27/can-democrats-avoid-pitfalls-2020-new-analysis-offers-striking-answers/).

[^2]: Analytics Twitter is the corner of Twitter that spends a third of its time [staning data tools](https://twitter.com/roliepoly/status/1396895238896029702), a third of  its time [arguing about SQL formatting](https://twitter.com/tayloramurphy/status/1316561842131656704), and a third of its time making data dad jokes (I won’t link this one...but you know who you are). This isn’t to be confused with Data Science Twitter, which is a third A/B testing and causal inference, a third topics that my feeble Analytics Twitter brain can’t understand, and a third Google firing their ethics AI researchers for researching ethics in AI.

[^3]: If you’re interested in whatever this Substack is, follow these people. They’re the source from which I steal all of my good ideas; the Reddit to my Instagram meme account. If you follow them, you’ll get the same substance as you do here, but won’t have to parse through 500 words of weird cultural gloss to get it. Which you’ll have to do again in about three paragraphs.

[^4]: Better to be an angsty millennial than a geriatric millennial.

[^5]: It’s probably time for me to look for [other sources of inspiration](https://mode.com/blog/user-engagement-strategies-and-rom-coms/).

[^6]: Emilie Schario, another person I have to shamelessly steal from to write anything, [made this point first](https://twitter.com/emilieschario/status/1393000594298834946).

[^7]: These slides are from an [old talk](https://www.youtube.com/watch?v=ahIhOAJfXnA) about hiring more analysts.

================================================================================

# When are templates going to happen?

*Do you get deja vu? From trading dashboards, laughing about how small they look on you?*

---

![](https://substackcdn.com/image/fetch/$s_!roTT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Facec5c36-df63-455d-af3c-648ad3b018d1_640x320.jpeg)
*Templates are so fetch.*

Every six months or so, I have the same conversation I’ve been having for the past seven years. I’m talking with someone about some new data technology or trend, and we notice that whatever we’re talking about is making data more standardized. Years ago, we were all consolidating around the same SaaS apps to manage our businesses. Then we started using the same event tracking frameworks like Segment, and the same ELT tools like Fivetran and Stitch. More recently, we’re adopting similar conventions for modeling data with dbt. With each new development, [we say knowingly](https://benn.substack.com/p/self-serve-shibboleth), our data starts to look more like everyone else’s. 

Our businesses are starting to look the same too. We were all tech companies, then we were all internet companies, and now we’re SaaS companies.[^1] 

This, it seems, presents an obvious opportunity for both the [lazy data analyst](https://twitter.com/bennstancil/status/1394469056837832704) and the entrepreneur: We should turn our reports and dashboards into reusable templates. 

Today, you can set up an entire data stack in a day.[^2] But companies still have to build their own reporting on top of that stack. Even though our data is the same, and our companies are the same, there’s no one-click way to spin out an entire suite of dashboards. We have to build it all ourselves. 

If we had community templates for common business concerns, like tracking ad performance, product usage, or SaaS metrics, data teams would no longer have to develop custom versions of the same dashboards everyone else is building. Moreover—and this is when we get really excited—templates could become their own standard. If you build *the* report for measuring support performance, everyone will want to make their products compatible with your dashboard.

Not only does it sound possible, it sounds inevitable. And yet...it hasn’t happened. These dynamics have been the same for nearly a decade, and I’m not aware of a single canonical template like this, or even a popular one. Why hasn’t this happened yet?

One possible answer is that templates are just a hard product to get right. A dashboard isn’t a bunch of text that you can just copy from a webpage. It has to be deployed between your data and a BI tool (or inside the BI tool). Maybe all that’s missing is a way around those logistical obstacles.

It’s also possible that modern data tools are still too fragmented for templates to quite fit. General patterns are popular—SaaS service to ELT to warehouse to transformation to BI—but there are a lot of combinations of vendors in that architecture. That makes it difficult for one template—say, one that works with Stitch, Snowflake, dbt, and Mode—to develop any real gravity. If things consolidate further, we’ll get there.

But there’s a third explanation that I’m partial to: Templates just don’t work.

# When everything is the same, we’re defined by our differences

At Mode, we use Salesforce, like everyone else. Salesforce is our source of truth for customer contracts, as it is for everyone else. We use Stitch and Fivetran to copy Salesforce data into Snowflake, like everyone else. And we're a B2B SaaS business, like everyone else. So why can't we just plug an ARR template on top of a couple Salesforce tables, and have all of the financial reporting we need?

Because we use Salesforce in a way that's specific to our business—like everyone else. We have to decide how to represent multi-year contracts and trials in Salesforce. We have to decide how to represent people, sales opportunities, and companies. We have to decide, when we sell contracts to the subsidiaries of a parent company, if we represent those deals as opportunities on the same account, or on separate accounts. We have to decide if off-cycle upsells are recorded as a new opportunity, or an update to an existing one. 

These choices aren’t dictated by data engineering principles or analytical hygiene; they’re modeled after how we sell. Upsells, for example, follow a particular sales motion, determined by the dynamics of how Mode is adopted, how teams budget for it, and how our sales team is equipped to sell it. The bespoke elements of our Salesforce data that distinguish it from that of other B2B SaaS businesses aren’t analytical blemishes to be polished off of a template that just sums the “amount” field on every Salesforce opportunity. They are the foundations of our business. 

This is why templates are flawed. The core assumption behind them is that everyone's data is mostly the same, and everyone's business problems are mostly the same. There are differences, sure, but those differences are relatively few and relatively small.

Half of this is true. There are relatively few differences; most people use tools like Salesforce in overwhelmingly similar ways. The problem, however, is that people introduce distinctions for a reason—to capture that which is unique about their business. It’s like a set of golf clubs: From a distance, the differences between a 5 iron and a 9 iron are minuscule—just a few degrees of club head tilt and a couple inches of length. But those differences are the very things that define each club as that club. 

This issue exists across nearly every domain a data team works in. Tracking product analytics via Segment? Some companies only have a web client. Some only have mobile clients. Some only have a mobile client for iOS.[^3] Some have mobile clients that are really just web clients. Some have desktop clients. Some, like Substack, are mostly consumed as emails. And some, like Segment itself, have a web client that’s used for intermittent configuration, and most activity in the product happens as interactions between servers, with no humans involved. A daily active user template is useless if everyone defines both activity and users differently. 

Want to measure support performance via Intercom? Do you provide 24/7 support, or just during business hours? When are your business hours? Do you use Intercom to proactively reach out to prospects, like some companies do, or exclusively for reactive support? Do agents use it to chat live with customers, to triage email, or both? A generic definition of initial response time makes no sense unless these very fundamental differences are accounted for. 

Importantly, more data standardization doesn't help solve these problems. The schemas are already standardized, as are the ELT tools that ingest them. The problem is that, across different companies, a Salesforce **`opportunities`** table, a Segment **`pages`** table, and an Intercom **`conversations`** table don't represent the same concepts. 

In other words, the shared foundation that templates are supposed to be built on is often only name deep. 

# Success is business specific

This is only half the problem. Even if every table represented the same concept across every business—if every company used the same Salesforce model, or instrumented their Segment tracking scheme in exactly the same way—the metrics businesses care about are still different.

Take Slack and Microsoft Teams, two products that are as seemingly identical as products can be. Even Teams’ ads [are clones of Slack’s](https://www.theverge.com/2019/11/21/20975952/slack-microsoft-teams-competition-ads-video-ok-boomer). If there was ever a time for a template, this is it.

But it still doesn’t work. Slack is (so far, still) sold as a standalone product that’s marketed, above all, as a new way to communicate that's faster than email. Teams is [bundled with other Office 365 services](https://www.microsoft.com/en-us/microsoft-teams/compare-microsoft-teams-options), including Word, Excel, *and Outlook.*

![](https://substackcdn.com/image/fetch/$s_!I44y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa91e5f3c-c95b-47fa-aeac-c4ce92f4a5cc_3480x1378.jpeg)
*Definitely not email.*

Despite being nearly indistinguishable services, Teams and Slack almost certainly measure their success in very different ways. Slack, which wants you to live in Slack, [emphasizes how much time people spend in Slack](https://slack.com/intl/en-gb/blog/news/intl-en-gb-work-is-fuelled-by-true-engagement). Teams, by contrast, wants you to live anywhere in Office 365, including Outlook. Consequently, Teams promotes “[Microsoft 365 daily collaboration minutes](https://www.microsoft.com/en-us/microsoft-365/blog/2020/10/28/microsoft-teams-reaches-115-million-dau-plus-a-new-daily-collaboration-minutes-metric-for-microsoft-365/),” which tracks engagement across all Office products. Rather than optimizing user time in Teams, Microsoft could prefer the opposite: get people out of Teams and into services like Word or Powerpoint where "real work" gets done—and, I suspect, Microsoft’s real money gets made.

# Will templates ever happen?

I'm skeptical, though I think there’s one approach that still has some hope.

Today, most templates extrapolate semantic meaning from standard source tables. They assume, for example, that the Salesforce **`opportunities`** table represents contracts in some sort of conventional way. A better method—which is similar to the one taken by [Narrator](https://www.narrator.ai/)—may be to invert this: ask for meaning, and tell people to provide a table that matches it.

This type of template could require an opportunity-like table as an input. Rather than sitting on top of a raw table, the template would ask for a derived table that meets a required set of parameters: one row per customer, include a contract start date and end date, how much the customer is paying you, and other relevant fields. 

This isn’t as punchy as a template that works in one click. Somebody has to translate your schema into the template’s schema. Plus, it doesn’t doesn’t solve the problem of different businesses needing different metrics. But mapping existing data into new tables [is already how analysts work](https://discourse.getdbt.com/t/writing-models-backwards-an-unorthodox-approach-to-data-transformation/2287). And within certain domains like finance, where reporting is more rigid and metrics have commonly accepted definitions, formulaic templates may have a place. 

Or maybe not—a couple years ago, I got caught up in a moment about templates myself. I built a [collection of templates](https://mode.com/example-gallery/templates-for-fundraising/?utm_source=founders_meetup&utm_medium=event&utm_content=startup_20190520), using this same approach, for the typical finance metrics VCs want to see when they’re evaluating a startup.[^4]

My templates, like every effort before them, didn’t take off. It could be that [I’m a bad marketer](https://benn.substack.com/p/twitter-profile-pictures). Or it could be a sign that we need to [stop trying to make templates happen](https://www.youtube.com/watch?v=Pubd-spHN-0). 


---


[^1]: As a petty aside, has there ever been a less artfully worded ethos than “software is eating the world?” Not only is it a clumsy phrase, it’s also a bizarre metaphor. It suggests that software is a toxic parasite devouring a healthy host, like “extremists are eating the party” or “TikTok is eating my brain”—which is, presumably, not the intended connotation for the motto of a VC firm that invests mostly in software companies. And there are so many better options! You could actually match the verb to the nouns, with “The world runs on software.” You could come up with a more elegant metaphor, like “Humanity is becoming hardware.” Even if you’re married to “eating” for some inexplicable reason, there’s an obvious alliterative alternative in “Software is eating the earth.” Or, I guess, you could abandon the whole thing and replace it with a line that could’ve been pulled from an awkward press conference for Chuck Schumer to half-heartedly promote another doomed infrastructure bill to a couple dozen Capitol Hill beat reporters: “[It’s time to build.](https://a16z.com/2020/04/18/its-time-to-build/)” (Ok, nope, still [all in on eating](https://a16z.com/2021/06/08/journal-club-urgent-care-costs/).)

[^2]: [Do it live!](https://www.youtube.com/watch?v=xH9Q1GtWYiE)

[^3]: [Not that I’m bitter about anything related to that.](https://benn.substack.com/p/a-slur-on-clubhouse)

[^4]: These templates were created for a talk I gave on how you can use data to raise money. The [slides](https://docs.google.com/presentation/d/1jKOxTbiUNF5DHMGhC_R0nZg7Lj9QY9RVvqLJrd4SuLE/edit#slide=id.g4dfc6015ae_0_30) and a [video](https://www.youtube.com/watch?v=qFdW-QyQUFo) (if you’re having trouble sleeping) from that talk are both available as well.

================================================================================

# The case against SQL formatting

*I went looking for the gospel, and lost my religion.*

---

![](https://substackcdn.com/image/fetch/$s_!_8ED!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5ae6f48c-f09a-43d1-9b14-b93a337a825d_1280x582.jpeg)
*How do you solve a problem like Maria DB SQL formatting?*

I’m a man with a short epitaph. If I were to die tomorrow, the best material I’d leave my eulogizers is three shallow scratches I put on strange corners of the internet: A blog post about [the price of weed](https://mode.com/blog/mapping-marijuana-prices/),[^1] an appeal to [move all the driveways in San Francisco](https://news.ycombinator.com/item?id=6406904), and a sober argument[^2] against [leading commas in SQL queries](https://mode.com/blog/should-sql-queries-use-trailing-or-leading-commas/). “He cared a lot about SQL formatting,” they might say, before butchering a [poetry reading](https://allpoetry.com/late-fragment):

> And did you get whatyou wanted from this life, even so?I did. And what did you want?...To make an overzealous case against an aesthetically controversial but semantically meaningless tic in the geographic grammar of a half-century old query language for databases. Obviously.

Given that I am, in fact, not dead—and evidently satisfied with this as my legacy—I’ve continued to prosecute this case, and canonize the gospel of proper SQL formatting for future generations.[^3] To this end, I recently made an effort to document the broader set of rules that I follow when I write queries. Here is, in what will surely be a peaceful take, The Answer:

![](https://substackcdn.com/image/fetch/$s_!FV_n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1b83a0c-fe74-4a33-97b5-d13799838629_1328x576.png)

To anyone who disagrees: I will die on these hills. There should be a gutter six spaces in, because it looks nice. SQL keywords are uppercase; table and column names are lowercase.[^4] The **`ON`** clauses are on different lines than the joins. The first clause in a join condition (e.g., the **`a`** in **`ON a = b`**) references the table in the join, and the second clause references prior tables. Table aliases should be short, initials if possible. And, as should go without saying, select statements use trailing commas.

After easily recording these rules, my review got more complicated. I needed to come up with commandments for SQL’s more sophisticated maneuvers, like case statements and where clauses with parentheses. 

It was then that my faith wavered.

What’s the best way to format these parts of a query? The answer—an answer every analyst should love, if not many priests—is that *it depends. *Case statements, for example, solve a number of problems, and, as such, come in all sorts of shapes and sizes. There’s simply no format that works for all of them.

That’s because a query doesn’t work just because a computer can execute it; it needs to make sense to both machines and *humans* ([because it’s people, who do the reading](https://mode.com/blog/should-sql-queries-use-trailing-or-leading-commas/)). Readability includes more than aesthetics (though those do matter). Queries—which, above all, transcribe vague business concepts, like revenue and [win rate](https://benn.substack.com/p/analytics-is-a-mess), into precise formulae—should also make their logic legible. We get frustrated by SQL’s [computational anachronisms](https://twitter.com/chrisalbon/status/1399767813569486853) because they confuse our ability to follow that logic.[^5] Bad formatting can do the same. Good formatting does the opposite: It actually making queries *easier* to understand. Formatting is our medium, and [the medium is the message](https://en.wikipedia.org/wiki/The_medium_is_the_message).[^6]

A well-formatted query highlights its important and complex elements while tucking straightforward ideas into the background. Just as data visualizations should be formatted in ways that draw out the salient differences between datasets, queries should do the same for the logical narratives they tell. And just as these differences in datasets are best surfaced by different graphs, different logical structures are best cued by different formats. As query authors, consistency shouldn’t be our goal, no more than we should make it our goal to consistently use bar charts. Instead, our job is to identify the computational studs in our queries, and make those inescapable. 

For example, the case statement below is simple, and the field using it isn’t overly important to the query. Inserting a bunch of line breaks gives that field outsized importance—importance that isn’t warranted. In this example, it’s better to keep the case statement all on one line.

![](https://substackcdn.com/image/fetch/$s_!w_nq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd03f6945-6a7f-4e14-8aaf-168421ecc674_1318x366.png)

This next case statement is a long series of if/then statements. Those pairings are the most important thing for the reader to understand. When each logical couple is on the same line, they’re easy to understand. By contrast, a single-line case statement is hard to read, and a case statement that splits the when/then pairs visually mixes the logically different “if” and “then” clauses, breaking the clear path from one to the other.

![](https://substackcdn.com/image/fetch/$s_!DzNR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F977090ae-4537-4697-8414-ea82f4ff7b7f_1326x554.png)

Compare this approach with the more traditional multi-line format.

![](https://substackcdn.com/image/fetch/$s_!ktF-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F682e675f-7653-48fa-8293-1e032193c952_1328x900.png)

In this third example, the case statements are fairly complex, but each one is similar. Rather than follow either of the formats in the prior example, this query should highlight the single critical difference in each case statement. This is best done by leaving each statement on one line, and adding extra space to align each clause. This makes it easy to see the differences *and *the similarities. Splitting out these case statements onto separate lines hides this pattern, forcing readers to unnecessarily parse and compare each statement individually.

![](https://substackcdn.com/image/fetch/$s_!SteM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6a2eea20-1b67-4901-a92a-9fce80e5da6e_1368x226.jpeg)

The “correctly formatted” version, however, is incomprehensible.

![](https://substackcdn.com/image/fetch/$s_!JumL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F67f04d1e-2d1a-4351-b893-ac0e67108213_1322x1206.png)

Moreover, the ideal format for this query might change if there’s only one case statement. In this instance, the important comparison to highlight is within the case statement rather than across different statements. 

![](https://substackcdn.com/image/fetch/$s_!CfOO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0261267-4d51-473c-b0ce-1d60c8520cd9_1482x336.jpeg)

The same reasoning applies to other elements in a query. Complex where clauses often have nested AND and OR logic. The best way to format these queries is to position them according to their order of operations, much as you would draw a complex mathematical expression in LaTeX.

![](https://substackcdn.com/image/fetch/$s_!KKw5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc19a8f61-5287-454d-9a0d-f784d8a398c5_1316x322.png)

This approach visually outlines the logic. Standard formatting blurs it.

![](https://substackcdn.com/image/fetch/$s_!VaH6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F26748509-9ea7-4b17-b060-ba348ef6f054_1314x624.png)

True, these examples don’t follow The Rules, at least not rules that can be programmed into a SQL formatter. But our job as query writers isn’t to be mechanical scribes; it’s to format our work so that it’s easy to interpret. In many cases, convention is a helpful escort. But in other cases, an unconventional format makes a query more readable than the style guide. So be it—ignore the guide.

The alternative is dogma. Adherence to a prescription simply because it’s prescribed is to be conformist prude; it’s to disappear behind a linter; it’s to elevate etiquette over [creativity](https://benn.substack.com/p/analytics-is-a-mess); it’s to judge our work by how little of ourselves is visible in it. It’s believing that Hemingway’s novels would’ve been better if they’d been edited by Grammarly,[^7] or that Ansel Adams’s photos could’ve been improved by torching them with an [X Pro II](https://www.huffpost.com/entry/instagram-filters-ranked_n_5127006) filter. Rules, even the best documented and most hallowed ones, are sometimes [best ignored](https://www.biblegateway.com/passage/?search=1%20Timothy%202%3A12&version=NIV).


---


[^1]: I still get emails about this one, so apologies to whoever inherits my Gmail account and has to deal with the occasional stoner turned political science grad student looking for data to determine the relationship between cannabis prices and voter turnout.

[^2]: Unhinged diatribe.

[^3]: [The Gospel of Join](https://en.wikipedia.org/wiki/Gospel_of_John).

[^4]: To put this in terms that some databases *cough*Snowflake*cough* might understand, TABLE AND COLUMN NAMES ARE LOWERCASE.

[^5]: Coming this fall, Mem(ento)SQL, the multi-*tenet* database: A story where everything is out of order and nothing makes sense.

[^6]: Counterpoint: [I know nothing of this work, and the whole fallacy is wrong](https://www.youtube.com/watch?v=vTSmbMm7MDg).

[^7]: Or, ironically, the [Hemingway App](https://hemingwayapp.com/).

================================================================================

# In defense of analogies

*And why analysts can’t stop making them.*

---

![](https://substackcdn.com/image/fetch/$s_!M9V6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F837f3edc-3e5f-4169-93ed-f4e63ef7be19_700x488.jpeg)
*Welcome to benn dot substack dot com.*

For better or worse, a couple months in, this Substack is mostly a newsletter of weekly analogies.[^1] SQL style guides are the [First Epistle to Timothy](https://benn.substack.com/p/the-case-against-sql-formatting). Business users are [science journalists](https://benn.substack.com/p/self-serve-still-a-problem) and [Natalie Portman](https://benn.substack.com/p/self-serve-shibboleth). Analytics templates are [golf clubs](https://benn.substack.com/p/when-are-templates-going-to-happen). The modern data stack is the [beer industry](https://benn.substack.com/p/datas-horizontal-pivot). Analysts are [architects, and writers, and confused college students](https://benn.substack.com/p/analytics-is-a-mess) (and, given all the mixed metaphors, confused writers). People who close airplane windows are [sea monsters](https://benn.substack.com/p/open-the-window).[^2]

Some people may say this is too much—”say something real; be less oblique; not everything is like a Billie Eilish song.” To which I say, you can’t have too much of a good thing (and is everything *not* like a Billie Eliish song?). Moreover, though I may be a worse offender than most, it’s always struck me how much other data analysts do the same. If our job is to help people figure stuff, why are we always communicating so indirectly?

Though analogies may seem like misdirection, I believe they’re actually clarifying—even when they don’t seem to make sense. That’s because, unlike SAT test questions that directly compare X to Y, real analogies have a prerequisite step: Turn complex X into simple X. This process, more than the comparison itself, is what makes analogies so useful.

To create an analogy, you have to pull out the essential bones of an idea, and map them onto the bones of another idea. This requires you to make clear what you think the structural skeleton of your point is, and what you think is extraneous. In doing so, you have to create a simple model for your idea. And when we use [simple models](https://krugman.blogs.nytimes.com/2011/02/02/models-plain-and-fancy/), we can’t hide behind complexity or unnecessary details, either intentionally or inadvertently, to obscure our argument's logical core. 

In this way, even mismatched analogies are instructive. We just have to pay more attention to the translation from complex X to simple X than the comparison of X to Y.[^3]

It’s the same principle that explains why it’s better to explain things in simple words than [business jargon](https://www.vulture.com/2020/02/spread-of-corporate-speak.html), even thought the latter might feel more precise.[^4] Simple words, like simple models, leave little room for misinterpretation. Phrases like "solutioning" and "value-add" are an argumentative sleight of hand: They create artificial complexity, and let us appear to make a point without ever having to actually say what that point is. They aren't meant to communicate but to distort, like a series of fun house mirrors that warp and deform whatever they're reflecting—which, as often as not, is nothing at all.  When we try to explain something without translating a complex X into a simple one, we can use sophistication as a shield in the same way. 

There's another analogy for analogies, which may explain why I’m drawn to them: Analogies share a lot of similarities with data analysis.

In many analytical questions, the hardest and most important thing to figure out is what actually matters. What are the critical elements to this question, and what are red herrings? What's the signal, and what's the noise? It’s not possible to consider every nuance that explains why customer retention rates are rising, or [why baseball’s hitters seem to be getting worse](https://www.theringer.com/mlb/2020/8/10/21362068/mlb-babip-low-2020-season), [why the economic recovery has been so unsteady](https://www.nytimes.com/2021/05/18/podcasts/the-daily/us-economy-jobs-report-pandemic-recovery.html). The only way to solve these riddles is to reduce them to smaller questions that are representative of larger problems. To think this way—to create limited models to uncover larger truths—is to think like both an analyst and an analogist.[^5]

There are several other parallels with the analytical work. First, data analysts spend a lot of time searching for patterns. Finding the dozens of trends in a dataset is easy. The real work is figuring out which ones to pair up and how to make sense of them—building, in other words, “quantitative analogies.”

Second, thinking in analogies makes us better storytellers. Data is often boring and confusing. Analogies help us build more [memorable stories](https://twitter.com/emilieschario/status/1405331278728155141) by adding color to the mundane. 

Analogies also have one final benefit that’s even more significant: They force us to seek common ground with our audience. To translate something unfamiliar into familiar terms, we first have to figure out what’s familiar to those we’re talking to, and explain our ideas on their terms—which [is a point I’ve made before](https://mode.com/blog/deliver-analysis-not-charts/), using, ironically, a baseball analogy that only makes sense to baseball fans. 

So maybe, I suppose, you can have too much a good thing. As for me, well—[I ain’t nothing but a lost cause.](https://www.youtube.com/watch?v=S2dRcipMCpw)

[Subscribe now](https://benn.substack.com/subscribe?)


---


[^1]: And footnotes.

[^2]: One bit of evidence I’ve taken analogies too far: Several people assumed this entire piece was itself an analogy for something bigger. Sadly, no. Also, to those of you who do close windows—I’m actually writing this from an airplane, window wide open beside me, screen easy to see, and the spectacular expanse of the Appalachian foothills a glance away.

[^3]: Note that this isn't the same thing as[ using analogies for decision making](https://hbr.org/2005/04/how-strategists-really-think-tapping-the-power-of-analogy). In those cases, the tightness of the analogy—i.e., how closely X resembles Y—matters much more than it does for explaining concepts or telling stories.

[^4]: If you take away any learnings from this post, it’s that you should operationalize all the action items in this article immediately.

[^5]: Analogist (n): An analogy enthusiast.

================================================================================

# Data's big whiff

*How to escape our dashboard rat race, learn from data, and love the job again.*

---

![](https://substackcdn.com/image/fetch/$s_!oiDB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F713907e4-6233-49c3-ab80-dec718a228a2_900x506.jpeg)
*[Lifetime](https://www.mylifetime.com/she-did-that/november-20-1973-a-charlie-brown-thanksgiving-premiered)*

Fool me once, shame on you. Fool me twice, shame on me. Fool me a thousand times, and I might be a data scientist, answering the same ad hoc questions I answered a month ago, wondering why I’m still not working on more interesting projects despite building more dashboards than a Ford factory and writing enough documentation to [land a golf cart on the moon](https://www.hq.nasa.gov/alsj/alsj-LRVdocs.html). 

For many data teams, it’s *Groundhog Day*: We build dashboards and self-serve tools to free us from answering mundane questions, with the promise of working on strategic initiatives as soon as we pay our operational dues. But that day never comes. No matter how much we build, it’s never enough. We keep making “investments for the future,” but we’re stuck in the present, outnumbered by repetitive ad hoc questions that we forget as fast as we answer. 

Are we doomed to this Sisyphean struggle forever? No—but we’re looking in the wrong place for solutions.

# Where the wild things are

Analytics teams can split their work into two rough buckets. The first, which includes making dashboards, is about building systems for people to answer questions. It includes maintaining business intelligence tools for “business users” (even the [manic ones](https://benn.substack.com/p/self-serve-shibboleth)), creating dashboards for executives, and generally supporting some notion of self-serve analysis around an organization. 

When we create these tools, we try to do it systematically. We define governed data models for our tools to read from; we monitor the pipelines that feed these models so that we can reliably detect anomalies; we curate discovery platforms to help people find relevant data and dashboards. We’ve even created a role–the [analytics engineer](https://jasnonaz.medium.com/analytics-engineering-everywhere-d56f363da625)—to own and maintain this system. 

Though we’re [not necessarily good at everything](https://benn.substack.com/p/self-serve-still-a-problem) in this bucket and a [lot of people now dispute its actual value](https://benn.substack.com/p/self-serve-shibboleth), it certainly *feels* valuable. A big reason for that is, in a word, marketing: BI and its associated applications are sold as platforms. They're infrastructure on top of which more things get built. To build this system is to invest in organizing your data, with the promise of a more productive and data-driven future as the payoff. 

This makes it an easy sell, to both ourselves and to those who give us our budgets. Rather than having to make the case for building a dashboard or report, we can argue for expanding the system. We’re not building individual houses; no, we’re laying down the roads and pipes that make unincorporated land a city; we’re civilizing the uncivilized. For all of us—and especially control-hungry execs—that has a lot of emotional appeal. So we expand; we build; we “invest.”

The other half of our jobs is doing analysis directly. This work is mostly commonly referred to as [ad hoc analysis](https://www.datafold.com/blog/dream-stack-for-analytics/#analysis),[^1] though some people call it advanced analytics, or decision science, or just “answering questions.” This is, presumably, what want to do rather than build the dashboards we complain about; we build self-serve tools, we say, so that we can focus on this type of work. Looker [sells this promise directly](https://info.looker.com/looker-201/looker-a-modern-data-science-workflow): “Looker helps to streamline processes to save valuable time, freeing up data scientists to focus on the more rewarding aspects of their job.”

We prefer this work in part because [it’s less tedious than adding the 1,000th filter to a dashboard](https://towardsdatascience.com/dashboards-are-dead-b9f12eeb2ad2), and in part because *this is the work that actually matters.* Ad hoc analysis is meant to support ad hoc decisions. These decisions are, almost by definition, the most important decisions companies make—they’re the ones you only get to make once. Jeff Bezos’ famous [one-way doors](https://www.inc.com/jason-aten/how-amazons-departing-ceo-jeff-bezos-prioritizes-his-time-according-to-one-way-door-rule.html) are the stuff of ad hoc analysis, not a BI report or self-serve dashboard.

And yet, there’s no officially recognized system for storing these conclusions or finding what’s been done before. It rarely exists inside the civilized walls of the self-serve systems we invest so much time into building. Yes, it’s sometimes built on top of the foundational elements that underpin BI tools, like governed dbt models. But its final products—the materials that contain analyses and their associated recommendations—are often scattered around analysts’ computers, buried in emails and Slack posts, and built on top of ungoverned queries and Python notebooks that blend development work with final recommendations.[^2]

To put it more bluntly, the most valuable—and most fun!—things we create as analysts live in the wilderness, while we carefully curate systems to support [trashboards](https://twitter.com/sethrosen/status/1407019976469397514) that their builders hate building and their users don’t use. 

The result is devastating, and it goes beyond [blowing up](https://www.hashpath.com/2020/12/an-analytics-engineer-is-really-just-a-pissed-off-data-analyst) [data scientists](https://towardsdatascience.com/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4). Dashboards and BI reports are operational tools for making immediate decisions. Their usefulness is fleeting; they don’t provide lasting value. We need car dashboards to drive, but we’re not better drivers today because we used our dashboard yesterday. Nothing we learn from them is durable.

There’s a dismal irony in this. The BI systems we build don’t accrue value. Ad hoc analysis—the things we learn, the things that actually make us smarter, like an analysis on the actions we could take to improve our driving—does, but we have no system for capturing it. We invest in the ephemeral, and throw away the enduring.

For a group that fancies ourselves as scientists, we probably shouldn’t ignore science’s [most exhausted cliché](https://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants).[^3] But we do, because we can’t stand on what we can’t remember and can’t find.

# Better dashboards, better deck chairs

I’m not the only one frustrated by this, and sometimes, [other people boil over too](https://towardsdatascience.com/dashboards-are-dead-b9f12eeb2ad2). But so far, most solutions, which focus on dashboards and BI, are incomplete improvements. No matter what we do within the system—no matter what new form factor we give dashboards, or how much observation and governance we layer on top of them—we’re still addressing a tangential problem. The real issue is happening* outside of the system*: ad hoc work is both essential and essentially ungoverned. No amount of “reimagining the deck chair” will keep our Titanic afloat. 

To escape our dashboard rat race—and more importantly, to help our organizations actually learn and build durable knowledge—we should start by organizing our ad hoc analysis. 

Unfortunately, there aren’t any clear answers yet on how to best do this. At best, we’ve produced a swirl of indirect solutions. Older frameworks like [Airbnb’s Knowledge Repo](https://airbnb.io/projects/knowledge-repo/), though probably too clunky and research-oriented for most organizations, could serve as a starting point (shoulders, if you will) for more accessible methods. Curation tools like [Select Star](https://selectstar.com/) and [Workstream](https://www.workstream.io/) don’t quite govern ad hoc work, though they could organize it. And we could draw lessons from other disciplines, including the [processes support teams use to build knowledge bases](https://www.hubspot.com/knowledge-base) and the [mechanics that power social platforms like Reddit](https://blog.getdbt.com/are-dashboards-dead/).

I have a few other thoughts about this, which I’ll save for a later post. In the meantime, I’d propose a simpler—and potentially even more impactful—campaign as a starting point: We stop calling ad hoc analysis ad hoc analysis. 

# The word makes the work

In 1929, while conducting experiments in the Canary Islands, German psychologist Wolfgang Köhler [discovered that he could show people nonsense shapes and words](https://en.wikipedia.org/wiki/Bouba/kiki_effect), and people would consistently pair the same shapes with the same words. 

![](https://substackcdn.com/image/fetch/$s_!_eej!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9feecc8b-b8ac-4927-b6a2-9bdf3c9d8854_1600x816.png)
*[Wikipedia](https://en.wikipedia.org/wiki/Bouba/kiki_effect)*

Adam Alter, [referencing the study in a 2013 ](https://www.newyorker.com/tech/annals-of-technology/the-power-of-names)*[New Yorker](https://www.newyorker.com/tech/annals-of-technology/the-power-of-names)*[ article](https://www.newyorker.com/tech/annals-of-technology/the-power-of-names) about the power of names, proposed a linguistic [Heisenberg principle](https://en.wikipedia.org/wiki/Uncertainty_principle): “As soon as you label a concept, you change how people perceive it.” This idea goes beyond the usual claims [that giving something a name makes it concrete](https://www.psychologytoday.com/us/blog/more-light/201802/the-power-naming); it suggests the name can alter the thing itself. 

In this light, it’s little surprise that ad hoc analysis is undervalued and often discarded—*its name tells us to do it. *

For a class of work that serves as intellectual underpinning behind a company’s most important decisions, the term “ad hoc” carries a lot of bad connotations. It suggests that the work is temporary, like scratch work meant to sketch out an idea and then be thrown away in favor of something more permanent. If you’re not steeped in the language of an analyst and don’t know [our codes](https://benn.substack.com/p/self-serve-shibboleth), you’d be forgiven for not seeing the need to invest in ad hoc analysis and its retention. Imagine an engineer saying they’d like to prioritize ad hoc projects, or a marketer running an ad hoc campaign. These things feel like offshoots, tangential to the primary goal. No wonder we never seem to find the time to do it.

It gets worse. Not only does the term “ad hoc” undermine its value, but—per Köhler—it changes the work itself. By naming it something temporary, we make it temporary. By calling it something that exists outside of a system, we place it outside of the system. We excuse our lack of organization because it’s defined by its disorganization. 

These effects feed on each other. Because ad hoc work is seen as disposable, we don’t invest in making it durable. Because we don’t invest in making it durable, it’s assumed to be disposable. To break this cycle, we need to rebrand it. 

A new name should accomplish three goals. First, we need to highlight that this work is important. “Strategic analysis,” for example, makes clear that it’s not a side project, but the most important part of our job. 

Second, we need to place this work inside of a system, not outside of it. A system that organizes some results and loses others isn’t organized; ungoverned work anywhere undermines governance everywhere. “Core analysis” can’t be an appendage.

Finally, we need to demonstrate that this type of work isn’t meant to be discarded, but should be accumulated. After a year of uncovering new results, we should be organizationally smarter and more informed.[^4] “Analytical research” isn’t meant to be thrown out, but cataloged, organized, and expanded.

Personally, I don’t love any of these names (I’m not a marketer; I’m a [cheap growth hacker](https://benn.substack.com/p/twitter-profile-pictures)). But surely we can do better than ad hoc analysis. We came up with big data; we came up with data science; we came up with decision science, and data mesh, and deep learning. As a field, we aren’t afraid of dressing up twenty-year-old technologies with buzzy puffery that’s simultaneously vague and inaccessible, better suited for a Star Trek set than a company standup.[^5] But when it mattered most, we got cold feet, and opted for understated Latin legalese. 

So, by all means, let’s stop building dashboards. But first, to convince everyone—including ourselves—of the value of what we should be building instead, let’s come up with some better technobabble. 

[Subscribe now](https://benn.substack.com/subscribe?)


---


[^1]: Gleb groups analytics workflows into three buckets: Self-serve analytics, dashboards, and ad hoc analysis. I’d argue that there’s not much distinction between the first two categories.

[^2]: And yes, in Mode.

[^3]: We’re a doctor who is unconcerned by doing harm. We’re a civil rights lawyer who refuses to judge people by the content of their character. We’re a priest who does unto others without a moment’s thought about how we’d have them do unto us.

[^4]: This points to one of the most damning questions we can ask about how we currently manage ad hoc analysis. How much smarter is your organization because of the analysis you’ve already done? How many future decisions will be better because of the analysis you did in the past? Analysis should strive to not just answer one question, but to also give us a head start on the next question.

[^5]: “We swept the data mesh, Captain. The readings found anomalies consistent with new life forms, and the Starfleet’s decision science computer says it's safe to investigate. But the deep learning scans, sir - they found nothing new. In fact, the neural networks are identical to scans from ten years ago. Sir, I think - I think we’ve been here before.”
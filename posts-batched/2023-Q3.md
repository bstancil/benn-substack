# Posts from 2023-Q3

This file contains 13 posts from 2023-Q3.

================================================================================

# Gsnowflake

*Itâ€™s a database; itâ€™s a data cloud; itâ€™s an informationbase! *

---

![](https://substackcdn.com/image/fetch/$s_!X7_k!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90d92c49-ab32-4440-9e7b-9f6260c55549_1076x798.png)

Is Google a database?

In some very abstract sense, it definitely is. I query it, it flips through an infinite filing cabinet for the thing I asked for, and brings it back to me. It is an organized collection of structured information, stored electronically in a computer; it is controlled by some data management system that determines which links should be returned for a given search; there are applications, like the search bar on google.com, that are associated with both Googleâ€™s data and its data management system. This is, according to Oracle, the [exact definition of a database](https://www.oracle.com/database/what-is-database/).Â 

Still, Iâ€™d imagine most of us would say that Google most certainly isnâ€™t a database. When we think of a database, we usually think of the second paragraph from Oracleâ€™s definition: â€œData within the most common types of databases in operation today is typically modeled in rows and columns in a series of tables to make processing and data querying efficientâ€¦.Most databases use structured query language (SQL) for writing and querying data.â€

In other words, a database isnâ€™t just a collection of information; itâ€™s a tightly organized catalog of capital-D Data. Data is a CSV; itâ€™s something you can open in Excel; it can be charted on a graph or aggregated in a pivot table; itâ€™s an input to computational operations and mathematical formulas and analytically useful summary statistics. And using data is loosely synonymous with doing math.[^1]Â 

Googleâ€™s â€œdatabaseâ€ is none of these things. A proper database can be cataloged, and we can look up where things in the database are, and what they mean. Iâ€™m not even sure what â€œcatalogingâ€ Google would even mean. Moreover, Google doesnâ€™t return structured tables or numbers that we analyze; it returns links, ads, a location on a map, or some news widget about [Shoheiâ€™s latest tank](https://twitter.com/JeffPassan/status/1674987004080037891). This isnâ€™t â€œdata;â€ itâ€™s information.[^2]Â 

And so, when we talk about databasesâ€”and when we make [wild](https://benn.substack.com/p/the-data-app-store) [claims](https://benn.substack.com/p/i-snowflake) about the future of databasesâ€”we tend to think in Oracleâ€™s terms, not Googleâ€™s. There are tables, queries, numbers, results, aggregations, catalogs. We think about how databases could [store more data](https://www.linkedin.com/pulse/data-ai-summit-recap-josh-howard/#:~:text=Implementing%20unified%20governance%20for%20all%20your%20data), how they perform calculations against it [more efficiently](https://www.snowflake.com/blog/single-platform-improves-performance-analytics-data-types/#:~:text=Continuously%20improving%20customers%E2%80%99%20price%20for%20performance), or how they could provide new methods for [ingesting](https://roundup.getdbt.com/i/133013596/platform-interop-via-iceburg) or [manipulating](https://www.snowflake.com/blog/single-platform-improves-performance-analytics-data-types/#:~:text=Making%20ML%20accessible%20via%20SQL) or [sharing](https://www.globenewswire.com/en/news-release/2023/06/27/2695742/0/en/Cybersyn-Launches-Two-New-Snowflake-Native-Apps-in-the-Data-Cloud-eCommerce-Benchmarks-App-and-Financial-Economic-Essentials-App.html) that data. And at the core of all of these conversations is a rough assumption that a database is still a database, to us and to Oracle: It stores capital-D Data, and it returns capital-D Data.

But the other version of a databaseâ€”the one thatâ€™s not full of data but information; the information database; the informationbaseâ€”is awfully valuable too. First, it makes us more informed, and probably much more so than a carefully-organized catalog of data. As Iâ€™ve argued before, most people arenâ€™t looking for insight; they [just want to know whatâ€™s going on](https://benn.substack.com/p/insight-industrial-complex). Google is a much better resource for that than any analyst with an almanac could ever be. And second, databases of information make a lot of money. The printing press in Mountain View that stamped out $282 billion dollars last year is powered by demand for facts and information, not data.[^3]Â 

# Search on

Snowflakeâ€”the Data Cloud formerly known as a cloud data warehouseâ€”launched a bunch of new features last week. Because itâ€™s 2023, the year of our new Large Language Lord, most of the announcements [emphasized their AI bona fides](https://davidsj.substack.com/p/all-change): Host LLMs in Snowflake; use LLMs in Streamlit; use ML functions in SQL; use LLMs to write SQL; extract data from PDFs with LLMs; run LLMs on Nvidia chips.Â 

In most of these updates, the focus makes sense; the AI is the feature. But for oneâ€”Document AI, which lets you pull information from PDFs and other filesâ€”the LLM [buries the lede](https://www.snowflake.com/news/snowflake-unveils-new-large-language-model-to-extract-deeper-insights-from-documents-while-continuing-to-advance-platform-speed-and-performance/):

> With new innovations like Document AI (private preview), Snowflake is launching a new large language model (LLM) built from Applicaâ€™s pioneering generative AI technology to help customers understand documents and put their unstructured data to workâ€¦
> Over the next five years, over 90 percent of the worldâ€™s data will be unstructured in the form of documents, images, video, audio, and more according to IDC. This massive volume of unstructured data is routinely stored by organizations, however gaining valuable insight from it has historically required manual, error-prone processes and limited expert skillsets. Building on Snowflakeâ€™s support for unstructured data, Snowflakeâ€™s built-in Document AI will make it effortless for organizations to understand and extract value from documents using natural language processing.Â 

There are four ways you could read this announcement. One is that Snowflake is using LLMs! Theyâ€™re technologists who are up on the latest technology! As a Snowflake customer, you wonâ€™t be forced to use a [51-year old version of COBOL](https://www.gao.gov/products/gao-19-471?utm_source=blog&utm_medium=social&utm_campaign=watchblog#:~:text=System%206-,51,-4); you will always be on the bleeding edge. Just look at that [completeness of vision](https://www.datanami.com/wp-content/uploads/2023/01/Magic-Quadrant-cloud-DBMS-market.png).

The second interpretation is that Document AI is meant to be an *input* to even more LLMs. Hoover up a bunch of unstructured data with Snowflake; build an LLM in Snowflake; create the enterprise chatbot that a thousand YC startups are already feebly chasing.[^4]Â 

The third interpretation is that Document AI is an ETL pipeline for unstructured data. Itâ€™s meant for turning your documents into tables that an analyst can query, just as Fivetran does for objects in Salesforce, or as [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro) does for logs in S3. In this version, the LLM doesnâ€™t matter that muchâ€”itâ€™s just a fancy transformation method, little different than ingestion pipeline casting a date thatâ€™s a string into a date thatâ€™s a date.Â 

The fourth interpretation is muchâ€¦weirder? Document AI may not be an ETL pipeline, but a web crawler.[^5] Itâ€™s a means for Snowflake to index unstructured data, and find the connections between these different pieces of content. Itâ€™s the beginning of Snowflake no longer being a repository for data, but a repository for *information*.Â 

Itâ€™s both a subtle and profound distinction. The difference between data and information is loose and colloquialâ€”the former is tables and numbers and spreadsheets, and requires some expertise and experience to be valuable. The latter, by contrast, is valuable as is. If I want to find all the contracts that a sales manager closed last quarter, or the release date of a new feature that was mentioned in a recent all-hands, or the [lyrics](https://www.tiktok.com/@livbedumb/video/7252433906635443498) to Olivia Rodrigoâ€™s new (and Iâ€™m gonna say it, disappointing) single *[vampire](https://www.youtube.com/watch?v=RlPNh_PBZb4)*, Iâ€™m looking for contracts, and a date, and some lyrics; thereâ€™s no analysis required.Â Â 

All of the hype around natural language querying and chatbot-based BI tools is implicitly built on this understanding. We usually want information more than we want data, and the appeal of a chatbot to tell us the answer we want, without having to do the complicated work of retrieving and interpreting the data ourselves. If Snowflake has access to more indexed information, much like Google does, we donâ€™t always need the LLM middleman; a lot of questions could be answered by returning some document or snippet directly. And over time, [though AI can make those results better](https://www.google.com/search/howsearchworks/how-search-works/ranking-results/#meaning), itâ€™s still fundamentally just searchâ€”search for documents, search for records ETLâ€™ed from third-party sources, and search for the connections between the two.Â 

Does this make sense? Maybe (though Databricks [seem to think it might?](https://www.databricks.com/blog/introducing-lakehouseiq-ai-powered-engine-uniquely-understands-your-business)[^6]). Is it real? I have no ideaâ€”nobody ([still](https://benn.substack.com/p/the-data-app-store#footnote-2-50157390)) tells me nothing. But there are a handful of reasons it might be worth an attempt.

# The Information Cloud

The first is pretty obvious: The market for a database full of information is almost certainly bigger than the market for data. As much as we talk about the importance of data democratization and self-serve analytics, itâ€™s still a relatively niche need compared to the operational questionsâ€”whatâ€™s our teamâ€™s budget? Whoâ€™s in charge of this account? Where are the talking points for this feature?â€”that people have all the time.Â 

Moreover, [in a post from earlier this year](https://benn.substack.com/p/am-i-the-jared-kushner#:~:text=if%20I%20had%20to%20bet%20on%20a%20race%20between%20one%20company%20that%20was%20broadly%20uninformed%20but%20well%2Dresearched%20on%20a%20few%20key%20decisions%2C%20versus%20one%20that%20was%20well%2Dread%20about%20its%20business%20but%20had%20to%20make%20most%20of%20its%20decisions%20on%20that%20awareness), I said that, â€œif I had to bet on a race between one company that was broadly uninformed but well-researched on a few key decisions, versus one that was well-read about its business but had to make most of its decisions on that awareness,â€ Iâ€™d take the latter. If thatâ€™s trueâ€”and I stand by itâ€”it suggests that we should be at least as focused on building automated librarians as we are on building [automated analysts](https://www.atoma.ai/), and that an easily-searchable catalog of basic facts could be just as valuable as a rich warehouse thatâ€™s full of data thatâ€™s primed for analysis.Â 

Which highlights the third benefit: A lot of companies donâ€™t haveâ€”and will never haveâ€”that warehouse. [As Jamin Ball said](https://cloudedjudgement.substack.com/p/clouded-judgement-63023-data-compute#:~:text=we%20need%20organized%20data%20in%20the%20cloud) in his recap of Snowflake Summit, to make use of AI, â€œwe need organized data in the cloud.â€ Call me a cynic, but a lot of us will never get there. Data is too complex, the systems that generate it are too brittle, and the experts who can corral it are too expensive and too hard to find. Any technology that makes solving those problems a prerequisite to using it is a technology thatâ€™ll be out of reach for most people.

The Snowflake InformationbaseÂ®â„¢ would have no such requirements. Dump your documents and emails and Slack messages into it, and FrankRankÂ®â„¢ takes care of the rest. It indexes them, builds a graph of their connections,[^7] and makes them searchable, no complex ETL or data modeling required. Though further enhancements could be addedâ€”data itself could be searchable, the whole thing be conversational, and so onâ€”it'd be built around discovery and retrieval rather than computation and analysis.

Admittedly, this seems like a bit of a wild idea, and a potentially big step away from Snowflake's core serviceâ€”which I would guess, despite its various bells and whistles, is still traditional data warehousing. But it's not completely without precedent. If Snowflake has primarily focused on serving the analytical needs of an organization, these basic questions are a companyâ€™s transactional operationsâ€”theyâ€™re not complex calculations, but simple reads and writes. [Snowflake](https://www.snowflake.com/en/data-cloud/workloads/unistore/) ([and others](https://cloud.google.com/blog/products/databases/transactional-and-analytical-workloads-unite)) are already trying to unify analytical and transactional databases into a single platform. Why would a complete data cloud not want to unify analytical and transactional questions as well?

# Occamâ€™s bomb

Matt Levine, the prominent financial blogger for Bloomberg, commonly says that his only vested interest in the news [is that it's funny](https://www.morningbrew.com/daily/stories/2021/12/17/icebreakers-with-bloomberg-columnist-matt-levine#:~:text=In%20any%20financial%20situation%20I%20pretty%20much%20root%20for%20the%20funniest%20outcome.). â€œ[Bumbling idiot](https://www.bloomberg.com/opinion/articles/2022-11-14/ftx-s-balance-sheet-was-bad#xj4y7vzkg) admits to [running a ponzi scheme](https://twitter.com/matt_levine/status/1591539118865285120) on a podcastâ€ is far better material for an email newsletter than government announcements about the [latest additions to their spreadsheets](https://www.reuters.com/markets/us/slower-still-strong-us-job-growth-expected-june-2023-07-07/), even though the latter might be more important.

Those of us who write blog posts about tech companiesâ€™ product releases can also find ourselves looking for stories in otherwise mundane and mechanical news. So, sometimes, when a company releases a new feature to extract information from a document and put it into a table, we try to extrapolate it into some grand narrative about building Google for the enterprise, and not tell the more obvious story that they probably built it because their customers kept asking for a way to extract information from a document and put it into a table.

The fifth, more-likely-but-less-interesting interpretation of Document AI is exactly thatâ€”itâ€™s a feature that customers want, and Snowflake wants to make things that people will buy. 

Still, even that might be revealing of a larger strategy. A number of Snowflakeâ€™s new features are offered by other vendors in the marketâ€”Document AI is similar to [Snorkel AI](https://snorkel.ai/); [Snowpark ML](https://medium.com/snowflake/snowpark-ml-a-machine-learning-toolkit-for-snowflake-4119b0bf204) could be a simple version of [Modelbit](https://www.modelbit.com/); [dynamic tables](https://snowflakewiki.medium.com/snowflake-dynamic-tables-a-way-forward-to-simply-data-pipelines-ee84d0298fd9) look a lot like Materialize, and [tasks](https://docs.snowflake.com/en/user-guide/tasks-intro) and the [dynamic table dependency graph](https://select.dev/posts/summit-2023#:~:text=They%20come%20with%20first%20class%20UI%20support.%20Users%20can%20easily%20visualize%20DTs%20and%20their%20dependencies%2C%20and%20see%20the%20historical%20runs%20on%20the%20Refresh%20History%20Tab.) look a lot like dbt; the teased [text-to-SQL feature](https://www.montecarlodata.com/blog-snowflake-summit-databricks-data-ai-2023-recap-features/#:~:text=However%2C%20Christian%20did%20discuss%20text%20to%20code%20capabillities%20currently%20in%20development%20where%20a%20large%20language%20model%20will%20automatically%20create%20the%20SQL%20code%20required.) competes with forty YC startups building the same thing. Though none of these things are all that surprising, they at least reveal that Snowflake isnâ€™t shy about stepping on potential partnersâ€™ toes. So the real conclusion from Summit may be that: Though theyâ€™re doing it [more slowly than Microsoft](https://benn.substack.com/p/microsoft-builds-the-bomb), Snowflake is gradually building a bomb of their own.Â 


---


[^1]: Or itâ€™s JSON! Or some other semi-structured format! My point here isnâ€™t that every database is a relational database full of tables; itâ€™s that we tend to think of databases as storing well-organized and cataloged sets of data, and not, like, a bunch of HTML scraped from the internet.

[^2]: Even if Google isnâ€™t a database, itâ€™s definitely powered by a lot of databases. So as an aside, hereâ€™s a brain teaser for some Google interview panel: How many rows of data get created or updated around the entire world, across Google and every other internet service, when I run a single search? My guess isâ€¦122,000.

[^3]: To put that printing press in comparison, between 2010 and 2020, during an era of [unprecedented monetary expansion](https://www.readmargins.com/p/zirp-explains-the-world), or whatever the line now is, the money supply in the United States increased by an [average of $693 billion a year](https://fred.stlouisfed.org/graph/?g=16OhX). So, Googleâ€™s printing press is about forty percent as big as that of the actual Fed.

[^4]: Ok, â€œonlyâ€ [forty](https://www.ycombinator.com/companies?industry=B2B%20Software%20and%20Services&query=ai%20assistant).

[^5]: This blog seems to have two recurring themes: Talking about [how](https://benn.substack.com/p/how-dbt-fails) [things](https://benn.substack.com/p/how-fivetran-fails) [succeed](https://benn.substack.com/p/how-dbt-succeeds) [or](https://benn.substack.com/p/how-analysis-dies) [fail](https://benn.substack.com/p/how-snowflake-fails), and making [weird](https://benn.substack.com/p/i-snowflake) [analogies](https://benn.substack.com/p/the-annual-microwave-conference) to describe Snowflake. Since I probably shouldnâ€™t talk about how [this acquisition](https://mode.com/blog/thoughtspot-acquires-mode/) (hypothetically! *Hypothetically! *Thatâ€™s the whole schtick! To be aware of what *could* go wrong so it *doesnâ€™t* go wrong!) fails until the deal actually closes, seems like itâ€™s option two.

[^6]: I went to Snowflakeâ€™s conference but not Databricksâ€™ because Iâ€™m not a [time turner](https://www.youtube.com/watch?v=lqTLETOYkQg), so I donâ€™t know any details about how this was actually presented. It seems like itâ€™s more of a [natural language query engine](https://www.montecarlodata.com/blog-snowflake-summit-databricks-data-ai-2023-recap-features/#:~:text=obsolete%20just%20yet.-,LakehouseIQ%3A%20Querying%20Data%20In%20English%20Via%20An%20LLM,-Then%20came%20one) than search, but it kinda looks like search?

[^7]: [Powered by RelationalAI!](https://relational.ai/blog/introducing-first-ai-coprocessor) Inside [Snowflakeâ€™s Snowpark Container Services](https://www.snowflake.com/news/snowflake-extends-programmability-for-developers-with-new-snowpark-container-services-with-gpu-support-to-run-generative-ai-notebooks-and-more-apps-securely-in-the-data-cloud/)! (Snowflake, yâ€™all, friend to friend, [this headline](https://www.snowflake.com/news/snowflake-extends-programmability-for-developers-with-new-snowpark-container-services-with-gpu-support-to-run-generative-ai-notebooks-and-more-apps-securely-in-the-data-cloud/), what is happening, itâ€™s so long and angry, I was looking for a press release and I got [Augie Garrido](https://www.youtube.com/watch?v=FuGrwxo0NVU).)

================================================================================

# We donâ€™t need another SQL chatbot

*We want one, to do the tedious parts of our job. But it might be better suited to take the fun parts.*

---

![](https://substackcdn.com/image/fetch/$s_!M2iy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cf4c63f-0f38-4822-ba34-ca0b5e3fe745_3488x1958.png)
*Attack of the the Clones, 2002.*

Another week, another text-to-SQL chatbot.

Ask it a question; itâ€™ll write a query. Ask it a question, itâ€™ll send a thin prompt to ChatGPT: â€œYou are a senior data analyst. Here are some notes about a few tables and a sample query.â€ Ask it a question, and itâ€™ll send your entire schema, your dbt project, and your data dictionaryâ€™s API keys to Claude. It uses LangChain to validate SQL syntax; [it doesnâ€™t use LangChain](https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm#correctness_and_usefulness_can_be_at_odds#llms_are_slow_and_chaining_is_a_nonstarter) because itâ€™s too slow; it uses LangChain to ensure industry-leading accuracy; it doesnâ€™t use LangChain because its errors compound into [a Waluigi](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post).Â 

The bots are everywhere. Theyâ€™re launching on Hacker News; theyâ€™re asking for your upvotes on Product Hunt. Theyâ€™re in your Slack; theyâ€™re in your database; theyâ€™re in your Twitter DMs; theyâ€™re in Section 4, Article C of the update to your terms of surface. Theyâ€™re tacked on to your favorite data tool; theyâ€™re your friendâ€™s startup; theyâ€™re your other friendâ€™s startupâ€™s pivot. Theyâ€™re a YC company; theyâ€™re backed by Accel; they raised $500 million from Masayoshi Son.

Never write a SQL query again; chat with your data. Never file another ticket with a data team; ask Dash, or Alan, or Newton,[^1] your friendly neighborhood analyst. It will respond with an answer; it will respond with charts, and a SQL query. It will ask for your feedback on what it told you. It will be there for you on Slack, or Teams, or over email. Its responses will be full of emojis; its responses will include cutesy jokes written by cheugy millennials.[^2]Â 

This is the future of analysis. ChatGPTâ€™s Code Interpreter is the ğŸš¨BIGGEST AI RELEASE YET ğŸš¨, says a Twitter account that bought 200,000 followers. Data analysts are obsolete, says a Medium post. â€œAre data analysts obsolete?,â€ asks a user on Reddit whoâ€™s learning to become a data analyst.

No they arenâ€™t, says a startup founder. An amateur historian on LinkedIn will say that AI gives them time to solve more important problems. Someone with a blog on a personal website will write a post about the history of technological innovation, citing Paul Graham, Tyler Cowen, and several articles that they found after googling â€œthe history of technological innovation.â€Â 

And some contrarian clown will go on Substack to throw a fit about these bots, to question if they work, and to suggest that they may still replace analysts after allâ€”but, of course, not in the way that we all expect.

# Twenty pages of context in a ten-page window

If you build a product with charts, youâ€™re going to build a BI tool.Â 

[Nobody believes it at first.](https://benn.substack.com/p/microsoft-builds-the-bomb#footnote-4-123990165) We tell ourselves that this time is different. Weâ€™re solving a different problem, for a different audience. We can make something complementary to BI, something narrower, lighter, more focused. We tell ourselves we're more principled than the others; we have more discipline; *we* won't cave and [build](https://twitter.com/barrald/status/1610739623835369472) [pie](https://twitter.com/bobbypinero/status/1679133838113488899) [charts](https://twitter.com/dorianj/status/1610741471568891905).Â 

Maybe we're right, in theory; maybe it can, eventually, be done. But the market can stay irrational [longer than a startup can stay solvent](https://www.goodreads.com/quotes/603621-markets-can-remain-irrational-longer-than-you-can-remain-solvent). And our customers will see our charts and want more of them; they'll want reports, and alerts, and explorable dashboards that can be exported as a PDF. They'll want granular permissions, and to connect to an old version of SQL Server running on a [Dell Optiplex 3020](https://www.walmart.com/ip/Dell-Optiplex-3020-Desktop-Tower-Computer-Intel-Core-i3-8GB-RAM-500GB-HD-DVD-ROM-Windows-10-Black-Used/994517744?wmlspartner=wlpa&selectedSellerId=5787&&adid=22222222227994517744_5787_149080731017_18511067177&wl0=&wl1=g&wl2=c&wl3=665551684524&wl4=aud-430887228898:pla-1879215353022&wl5=1022762&wl6=&wl7=&wl8=&wl9=pla&wl10=113945645&wl11=online&wl12=994517744_5787&veh=sem&gclid=CjwKCAjw5MOlBhBTEiwAAJ8e1ku06SrSKQPKm4A1zuCvo0nJAIbtViM5fqcwKwbmv8XBPBCuulpk1BoCPyUQAvD_BwE&gclsrc=aw.ds) under Jeff's desk.[^3] They'll want exactly what weâ€™re selling, plus just this one feature, and they'll pay us $100,000 if we can build it.

This is the devil that [follows all of us](https://www.youtube.com/watch?v=HkZYbOH0ujw). And *two of them* are hunting for every chatbot and AI-powered SQL writer.

The first monster is BI itselfâ€”most query generators are destined to be BI tools in their own right. If it writes a query, people will want it to run a query; then theyâ€™ll want to chart the results; and then pin them somewhere, and then drill into them by clicking on them, and then, and then, and thenâ€”and then you concede the inevitable and [admit to being BI](https://mode.com/blog/introducing-mode-as-modern-bi/).Â 

But thereâ€™s a second parallel between chatbots and BI: For both, thereâ€™s a canyon between what works well enough to get a few customers and what works well enough to win a market.Â 

An engineer can build a simple data visualization tool in a day. There are open-source libraries; there are well-documented APIs; there are best practices. But simple bar charts donâ€™t support a venture-scale startup. Customers will want faceted bar charts, with four series, and two axes, and filters, adjusted so that weeks start on Monday, with the right date format, and branded fonts, and labels for the first day of every quarter, every *fiscal* quarter, with a particular way of handling nulls, on six million records, and a trend line, thatâ€™s dashed, with the most recent incomplete week excluded from the calculation.Â 

Similarly, getting GPT to write a SQL query on a tidy schema of six tables is relatively easy.[^4] But prompting it to answer a vague question about orders from new accounts on top of 3,500 tables, that are named using abbreviations and legacy terms, and are full of messy and duplicative columns, and four different timestamps, where there are layers of relations to join through, and test accounts to exclude, and â€œnewâ€ is defined in a nuanced way, and any calculation about orders has to account for the way the sales team used to log contracts in Salesforce, for the way that the sales team now logs contracts in Salesforce, and for the way that one sales rogue rep always logged his contracts in Salesforce? Thatâ€™s a very different beast.

For bots to be successful query writersâ€”and even harder, for them to be proper analysts that can answer questions about a businessâ€”LLMs will likely only be a small part of the solution. There will also have to be semantic models, methods for mapping vague requests onto those semantic models, frameworks for governing access control, ways to test if it said the [same answer today as it said yesterday](https://benn.substack.com/p/all-i-want-is-to-know-whats-different), and more. Building a good text-to-SQL bot requires building systems that include all of this context in the prompt, despite it being far more information than can easily fit in the context window of an LLM.

Put differently, the LLM isnâ€™t an application, or even close to one. Itâ€™s a fancy function inside of an applicationâ€”and building the rest of that application likely requires far more work than anything done with OpenAIâ€™s API.[^5] Which isnâ€™t to say that itâ€™s impossible to build a product that writes good queries from fuzzy questions (and the unavoidable pieces of BI that come with it)â€”it just takes a tremendous amount of work and ingenuity.[^6] 

More bluntly, LLMs and query writing are fundamentally mismatched. The former is probabilistic, creative, and *inductive*â€”itâ€™s best used to generate ideas from short prompts. But analysts have to do the opposite when answering questions. They need to be precise, rigorous, and *deductive*â€”they need to know all of the nuanced laws governing how data is used, and apply them to questions that donâ€™t specify those details.Â 

But that doesnâ€™t mean that LLMs arenâ€™t useful for data analysis, or that all of our jobs are safe. It just means that most query bots are focused on the wrong problem.Â 

# We can be good, or we can have fun

In a very rough sense, we can plot all of the tasks we do for our jobs and in our lives along a single axis. At the bottom, there are mechanical tasks that are mostly mindless legworkâ€”digging holes, filing papers in filing cabinets, scheduling a meeting over email, [printing out binders full of charts](https://www.holistics.io/blog/how-amazon-measures/#:~:text=presentation%20that%20contains%20hundreds%20of%20graphs%2C%20charts%20and%20tables) for an executive teamâ€™s weekly business review. At the top, there are â€œstrategicâ€ and creative projectsâ€”designing a building, diagnosing a patient, giving a sales pitch to a prospective customer, making decisions about how to run a company. You could also place the components of an individual task along the spectrum as well: When planning a night out, booking a reservation is lower than choosing the restaurant; when working with data, typing correct SQL syntax is lower than reasoning through a query, which is lower than coming up with an analytical technique for answering a question.Â 

There are lots of ways that people might describe each pole. The bottom is repetitive; low-skill; boring; frustrating; simple. The top is inventive; high-skill; interesting; fun; intellectual. For obvious reasons, given the choice of where we want to spend their timeâ€”either in our careers or in a single taskâ€”most of us would probably say the top.[^7]Â 

Computers, in a very rough sense, usually help us do that. We donâ€™t have to dig as many holes because machines can do it for us. We donâ€™t have to file papers in filing cabinets, or even have files at all; computers can organize millions of documents in milliseconds. We have software that can help us schedule meetings, and BI tools that will replace our three-ring binders with live dashboards that update entirely on their own.Â 

Moreover, over the last several decades, as computers have gotten "smarter," theyâ€™ve typically moved up the spectrumâ€”and let us spend more and more time at the top. They used to only be able to do [basic math](https://www.amazon.com/Sharp-EL-1750V-Two-Color-Printing-Calculator/dp/B00006IF9Q/ref=asc_df_B00006IF9Q/); then they could do [complicated math](https://www.amazon.com/Texas-Instruments-TI-83-Graphing-Calculator/dp/B00001N2QU). Then they [made charts](https://www.microsoft.com/en-us/microsoft-365/excel), and now they can make [interactive forecasts that update by the minute](https://www.nytimes.com/interactive/2020/11/03/us/elections/results-president.html). Computers replaced our [daily planners](https://en.wikipedia.org/wiki/Google_Calendar), then they [helped us](https://calendly.com/) schedule meetings, and now they [shame us](https://www.cbsnews.com/news/shopify-meeting-cost-calculator-tool/) for scheduling meetings.Â 

This patternâ€”computers automating more and more mechanical work; people doing more and more creative workâ€”has become our default assumption about how technology advances, *and* for what we should do with new technology. New inventions push up the boundary that divides what computers can do from what we have to do; with every new tool, we try to figure out how we can hand over our most tedious remaining errands to the machines.Â 

But this doesnâ€™t have to continue forever. Technology couldâ€”again, in a very rough senseâ€”go the other way. It could be more creative than reliable. It could subsume our lives from the top down, outperforming us in strategic tasks while struggling with the tactical ones. If this happens, the best things we could build with it might impose a choice: Do we use it to make us better at something, or do we use it to replace something we donâ€™t want to do?

Because so far, one of the most striking things about LLMs is that theyâ€™re much better at the creative parts of analysis than they are at the mechanical parts.[^8] Ask ChatGPT to write a SQL query against an artificially simple schema; [itâ€™s a junior analyst](https://towardsdatascience.com/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2), at best. But ask it to come up with possible hypotheses to explain why thereâ€™s some anomaly in a metric, and it does better than I would.[^9]Â 

For example, we gave ChatGPT a dataset with a bunch of events that are exactlyâ€”and suspiciouslyâ€”one hour apart. When we asked it why that might be the case, it gave more reasons than I could come up with: Coincidence; programmatically scheduled activities; a time zone issue; system maintenance; the product could be one that people use at regular intervals; and access restrictions or throttling could queue user actions to reset every hour. Thatâ€™s pretty good! And when we asked it how to figure out which one it might be, it had pretty good ideas for that too. It didnâ€™t, however, do a good job of writing the queries that would run that analysis.Â 

In other words, *it inverted the workflow between human and computer. *It was better at coming up with ideas than I was; I was better at doing the work. I was its assistant, its agent, its copilot. It was assigning instructions, and I was following them. It didnâ€™t allow me to work on higher value work; it did the higher value work, and told me how to do the manual labor.Â 

Iâ€™m not sure that I want to say that all the query-writing chatbots should pivot into analytical reasoning bots. Thatâ€™s [the job I want](https://benn.substack.com/p/insight-industrial-complex#:~:text=I%20still%20remember,wait%20a%20minute%E2%80%A6%E2%80%99%E2%80%9D), to be at the top of the creative food chain with an army of machines doing the mundane tedium below me. But the army weâ€™re buildingâ€”creative, unpredictable, and equally prone to both novel ideas and liesâ€”is becoming [more man than machine](https://www.youtube.com/watch?v=UNCxbM50eWQ). If weâ€™re looking for chatbots that make us all better at working with data, without having to rebuild an entire BI tool in the process, our best option may be to let them do the job *we* wanted to do all along.


---


[^1]: These names arenâ€™t real, but theyâ€™re close to the real ones. My point here isnâ€™t to single anything out, so I leave it to the reader to google â€œsql writing bot,â€ or to imagine which one is, in fact, in my Twitter DMs.

[^2]: Somehow, this style has become the standard for these sorts of things. As far as I can tell, Slack started it, with the twee Slackbot aesthetic. Now, every other brand is a clumsy and lovable klutz who enjoys using emojis, attempting humor, and phrases like â€œDâ€™oh!â€

[^3]: Jeff, of course, quit eight years ago.

[^4]: For example, [this paper](https://arxiv.org/abs/2306.00739) evaluates how well a model performs in various text-to-SQL tasks. Though they stress-test different models by using imprecise language (e.g., asking for puppies when pets are labeled as dogs), the tests appear to be run against very small schemas often with fewer than a dozen tables. And even against that benchmark, the SQL is correct only eighty percent of the time.

[^5]: Thereâ€™s a longer post to write here about how LLMs fail (you know, [for](https://benn.substack.com/p/how-snowflake-fails) [the](https://benn.substack.com/p/how-analysis-dies) [brand](https://benn.substack.com/p/how-dbt-fails)). The quick version goes something like this: We use LLMs to write SQL queries; they donâ€™t work out of the box, so we start wrapping them with better prompts, and semantic models, and rules to make sure they return consistent results. Eventually, prompt engineering is just engineering, telling the model exactly what to do and how to handle edge cases, to the point that weâ€™re working around the LLM as much as weâ€™re working with it.

[^6]: This is one of the reasons why we took the callâ€”and eventually, the [acquisition offer](https://mode.com/blog/mode-founders-note-thoughtspot-acquisition/)â€”from ThoughtSpot. Theyâ€™ve built both the BI tool and a [lot of the infrastructure](https://www.thoughtspot.com/product/sage/how-we-built-sage-with-gpt) that can make LLMs useful for translating questions into queries. Obviously, other vendors could do that too, but it takes very real work, for which there arenâ€™t yet any shortcuts. (So why bother, [buy ThoughtSpot](https://www.thoughtspot.com/trial)! Or [Mode](https://mode.com/lp/demo/)! Or, like, skip a step, and just Venmo me, @Benn-Stancil.*)* Iâ€™d rather use the Cash app, because the [youths](https://youtu.be/K6qGwmXZtsE?t=110) tell me itâ€™s the cool one, and because my handle (cashtag?) is $benn. However, I signed up for the Cash app ten years ago using a debit card thatâ€™s long since expired. I need to know that debit card number to log in to the Cash app, and my bank statement forensics have only yielded 10 of its 16 digits. So my handleâ€”and a fortune of tens of dollarsâ€”is locked away forever behind [a code I canâ€™t quite remember](https://www.cbc.ca/radio/asithappens/as-it-happens-friday-edition-1.5875363/this-man-owns-321m-in-bitcoin-but-he-can-t-access-it-because-he-lost-his-password-1.5875366).

[^7]: This is especially true if youâ€™re three years and two jobs out of college, have spent nine months posting â€œcontentâ€ on LinkedIn, and are interviewing to work for a startup. â€œI donâ€™t care that much about the exact role; I just want to be working on stuff thatâ€™s really strategic, [to have the kind of impact that I know I can have](https://getyarn.io/yarn-clip/53cbe1f6-9a3a-4f7d-b143-462206546ad7), you know?â€ says the 26-year old white guy whose rÃ©sumÃ© describes him as being low ego.

[^8]: Analysis, by the way, isnâ€™t creating [various pivots of a dataset](https://arxiv.org/abs/2305.15038) that plot a bunch of dimensions against a bunch of metrics, or asking a computer to find â€œinterestingâ€ correlations in a spreadsheet. At best, thatâ€™s data profiling; at worst, itâ€™s fishing for red herrings. Analysis is asking a hard question, coming with different hypotheses, finding evidence that supports or refutes those hypotheses, and drawing conclusions from it. It is [indeed strange](https://www.oneusefulthing.org/p/it-is-starting-to-get-strange) that computers can now do the former, but itâ€™s a very different thing than the latter.

[^9]: Yeah, yeah, low bar, I know.

================================================================================

# How an acquisition fails 

*And the simple story for how it doesnâ€™t. *

---

![](https://substackcdn.com/image/fetch/$s_!E4VU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F666cab90-c7ad-4e6c-9214-ca5fb767ee93_1280x720.png)

Last fall, I wrote a [handful](https://benn.substack.com/p/how-snowflake-fails) [of](https://benn.substack.com/p/how-fivetran-fails) [posts](https://benn.substack.com/p/how-dbt-fails) about how some of today's most popular data products might fail. The point of those posts wasn't to argue that they *would *fail; it was instead to speculate about how they *might*. It was to be the devilâ€™s advocate, the [tenth man](https://www.youtube.com/watch?v=W_A5j3RuWHM), the host of a [pre-mortem](https://en.wikipedia.org/wiki/Pre-mortem). It was to assume that weâ€™d been visited by our future selves, and instead of [getting an almanac](https://www.youtube.com/watch?v=zorz3SXqjv0), all we got was a look at [Matt Turckâ€™s data landscape](https://mattturck.com/mad2023/) from [2033](https://mattturck.com/mad2033/)[^1]â€”and Snowflake, dbt, and Fivetran were all missing. We didnâ€™t know why; we only knew they were gone. Those posts were my guesses as to what would explain their hypothetical disappearance.Â 

I wrote them for the same reason I write most things on this blog: Because I found it fun to think about, and because there was a Friday those weeks. It was an excuse to live in my head for a minute, to look up some [Griff](https://www.instagram.com/p/CurrWVNIQM9/) and [Taylor Swift](https://www.youtube.com/watch?v=LAzLTY_V6rM) bangers to link to, and to play every talking headâ€™s favorite role: That a [smug critic](https://twitter.com/vboykis/status/1680367759933296641), fastidious and unfit for doing [the rough work of a workaday world](https://www.theodorerooseveltcenter.org/Learn-About-TR/TR-Encyclopedia/Culture-and-Society/Man-in-the-Arena.aspx#:~:text=It%20is%20not,a%20workaday%20world.), with [no skin in the game](https://youtu.be/GVPzGBvPrzw?t=123).Â 

[Play for blood](https://youtu.be/PsJFrwLl6HU?t=39), they [said](https://twitter.com/pdrmnvd/status/1573603915945361408). Iâ€™m just foolinâ€™ about, I said.

[Karma](https://youtu.be/XzOvgu3GPwY?t=131)[^2], it seems, tracked me down. A new BI company launched this weekâ€”ThoughtSpot and Mode, under one roof, [officially](https://www.thoughtspot.com/press-releases/thoughtspot-completes-200m-acquisition-of-mode-analytics).[^3] Though neither Mode nor ThoughtSpot is (yet!) the ubiquitous community staple that dbt is, and we havenâ€™t (yet!) gone public in the [biggest software IPO](https://www.axios.com/2020/09/16/snowflake-ipo) in history like Snowflake, the combined operation is substantial: Booking more than $150 million in annual recurring revenue, supported by $750 million in funding, and serving customers from Wells Fargo, Verizon, and CVS, to Snowflake,[^4] Capital One, Anheuser-Busch, and VMware. If the modern data stack is to produce an enduring and independent business intelligence companyâ€”if there is at least one IPO among the [72 companies](https://www.moderndatastack.xyz/companies/business-intelligence-bi) vying for the ticker symbol of BI[^5]â€”ThoughtSpot and Mode now have as good of a shot at it as anyone.

But that also means we could be the next major disappointment. As a companyâ€™s promise grows, its outcomes become more binaryâ€”IPO or bust; runaway success or cautionary flop; generational triumph or catastrophic collapse.Â 

So, for the sake of avoiding it, hereâ€™s how we find ourselves on the latter path.

# The usual suspects

Tech acquisitions are [notoriously temperamental](https://www.forbes.com/sites/steveblank/2014/04/22/corporate-acquisitions-of-startups-why-do-they-fail/?sh=69f2eaeb364d). Big visions are easier [to sell than deliver](https://en.wikipedia.org/wiki/Fyre_Festival); technology stacks are sometimes incompatible; cultures donâ€™t mix. Innovative roadmaps turn into interminable integration efforts that are more expensive, slower moving, and less rewarding than all of the projections on the spreadsheets. Some idiot with a blog torpedoes the whole thing two days after it closes for a clickbait headline and some likes.Â 

These are the inevitable dangers lurking in any acquisition. You can be aware of them, push on them during due diligence, and be mindful when youâ€™re slipping into one of their slow traps. But youâ€™ll never know if youâ€™re fully compatible until the deal is done and everyone is working togetherâ€”until youâ€™re not just dating, but married and moved in.

Like any leap of faith, itâ€™s a gamble. Sometimes, though, the potential reward is worth the riskâ€”and youâ€™ve to believe youâ€™re the kind of company [that rolls a six](https://benn.substack.com/p/the-case-for-being-biased#:~:text=%22I%27m%20the%20kind%20of%20guy%20who%20rolls%20sixes.%22%C2%A0).

# The bad track record

Outside of those routine challenges, however, this acquisition canâ€™t miss. Just look at the strategic fit!

> [It will create] a complete data science and analytics platform for customersâ€¦The two companiesâ€™ CEOs met about 18 months ago at a conference, and running similar kinds of companies, hit it off. They began talking and, after a time, realized it might make sense to combine the two startups because each one was attacking the data problem from a different angle.
> One company tends to serve business intelligence requirements either for internal use or externally with customers. The other looks at the data science end of the business. Both CEOs say they could have eventually built these capabilities into their respective platforms, but after meeting they decided to bring the two companies together instead, and they made a deal.
> â€œI realized over the last 18 months [as we spoke] that weâ€™re actually building leadership positions into two unique areas of the market that will slowly become one as industries and technologies evolveâ€¦â€
> â€œ[We have been] pulled into this broader business intelligence conversation, and it has put us in a place where as we do this merger, we are able to instantly leapfrog the three years it would have taken us to deliver that to our customersâ€¦â€
> The two executives say this is part of a larger trend about companies becoming more data-driven, a phrase that seems trite by now, but as a recent Harvard Business School study found, itâ€™s still a big challenge for companies to achieve.
> They recognize that many acquisitions donâ€™t succeed, but they believe they are bringing together two like-minded companies.

Exceptâ€”this isnâ€™t a story about ThoughtSpot and Mode; itâ€™s from an article from 2019 [about Sisense buying Periscope](https://techcrunch.com/2019/05/14/sisense-acquires-periscope-data-to-build-integrated-data-science-and-analytics-solution/). And that deal, which has a very similar outline to this one, is widely regarded as a dud. It didnâ€™t vault Sisense to the front of the BI market;[^6] Periscope, one of the early success stories in the modern data stack, has been [twice](https://www.sisense.com/blog/periscope-data-is-now-sisense-for-cloud-data-teams/) [rebranded](https://www.sisense.com/cdt-sisense-fusion-analytics/) out of existence. With this as the most recent precedent, the question to ask might not be how does our acquisition fail, but how can it possibly succeed?

On one hand, itâ€™s easy to overfit on one example, no matter how similar it seems on the surface. Every acquisition, like every company, is an idiosyncratic mix of technologies, personalities, and circumstances. What doesnâ€™t work in 2019, early in the modern data stackâ€™s hype cycle, may work very differently in 2023, as that cycle is receding and generative AI is emerging. 

On the other hand, itâ€™d be foolish to entirely dismiss it. The logic behind Sisense acquiring Periscope is sound; if their customers told them the same things ours tell usâ€”â€our SQL writers love Mode, but we also want more self-serve BIâ€â€”the promise of pairing the two products together is clear. So why did it still not work out? Without knowing the specific details,[^7] I have two general hypotheses.

# The devil is in the details

The first is that strategic fit can only take you so far. As compelling as the headline soundsâ€”BI and analytics, all in one place!â€”it omits a lot of experiential details. How do you reconcile two charting systems that look different? How do you promote a SQL query written in one tool to a dashboard in the other? Do both tools share the same caching layer, or do they remain independent?Â 

These are messy questions, and it can be tempting to answer them in the most technically orderly way possible: Tuck one product into another; refactor everything into tidy and DRY codebases; simplify overlapping org charts into linear pyramids; all the features, *mise en place.Â *

This, I suspect, is a trap. Data consumption tools arenâ€™t infrastructure; theyâ€™re productivity tools. People buy them for their user experiences as much as their technical features. Protecting that experience across the two products has to be the first priority of any acquisition like this; enhancing it has to be the second; and neatly sorting through the technical details of how to do it has to be a distant third.Â 

# The devil is in the entire plan

The second possible explanation behind the Periscope and Sisense result is that *the products donâ€™t actually fit together*.

There are two ways to read the relationship between general BI tools like Sisense and ThoughtSpot, and data team-focused tools like Periscope and Mode. The first is that most customers want both sets of features, but have to choose one. The second is that customers see the two categories as representing competing philosophies, and they choose the one they like over the one they donâ€™t. In this case, they might want a well-structured and tightly controlled data model and explicitly *donâ€™t* want a free-for-all SQL editor; or they might want a tool that can be deployed quickly on top of raw data, and explicitly *donâ€™t* want to deal with modeling data first.Â 

If most customers fall into the latter category, almost no product integration makes sense. The acquisition can still work, as a combination of business units, but the products are best kept at an armâ€™s length from one another. Fox News and MSNBC could be owned by one (very chaotic) parent, but they canâ€™t commingle their lineups.

My view is that itâ€™s not that extreme; very few customers are religiously opposed to one side of the spectrum or the other. However, it suggests that one way these acquisitions go wrong is by imposing the philosophy of one product on the other. Instead of doing that, each philosophy, to the extent that theyâ€™re competing, should be optional and additive, available to customers when they need it.[^8]

# The modern data suite

An easy future to imagine is one where the modern data stack is replaced by a few modern data suitesâ€”all-in-one, end-to-end platforms of warehouses, ETL, and BI, sold by a single vendor. This is Microsoftâ€™s [apparent ambition](https://benn.substack.com/p/microsoft-builds-the-bomb); it should be [Googleâ€™s ambition](https://benn.substack.com/i/58708482/google-is-wasting-all-the-good-cards); [Frank Slootman](https://benn.substack.com/p/the-conglomerate#:~:text=sooner%20or%20later%2C%20Frank%20Slootman%20comes%20around.%C2%A0) may yet [come for us all](https://benn.substack.com/i/133717126/occams-bomb) too.

By bundling related products into a single [cluster](https://www.youtube.com/watch?v=3Z2XPeyXn6A), suite vendors can usually offer their customers both lower prices and an easier buying experience than independent vendors can. The market could [come to prefer](https://benn.substack.com/p/case-for-consolidation) this offering over todayâ€™s more modular one; if that happens, Mode and ThoughtSpot wouldnâ€™t struggle because we consolidated too much, but because we didnâ€™t consolidate *enough*.

For smaller vendor, this can be a huge problem. The bigger a seller, the more they can dictate the market standard. Google, for example, can set everyoneâ€™s expectations for how search on the internet should work. It doesnâ€™t matter if some small startup builds a better search engine, because nobody will use it; if nobody uses it, nobody will know that they want it. But when that search engine gets big enough, people will notice, and start expecting more out of Google.Â 

If bundled suites become more common, thatâ€™ll be the challenge for independent vendorsâ€”to be big enough and be differentiated enough that people at least question if the easy default is the right choice. My sense is that Mode and ThoughtSpot are already there, but, [never turn your back on the Microsoft sales team](https://benn.substack.com/i/53744915/the-idealist-versus-the-army).

# The AI tsunami

Both Mode and ThoughtSpot are, at their core, BI tools. Theyâ€™re centered around charts and dashboards; they let people explore data; theyâ€™re all about operational metrics and insight discovery. Both of us have added significant twistsâ€”ThoughtSpot lets people ask questions with natural language; Mode lets analysts bypass traditional data models and work directly in SQL and Pythonâ€”but weâ€™re still descendants of products like MicroStrategy and Tableau.Â 

Though it may seem counterintuitive in Silicon Valley, Iâ€™ve long viewed this connection to the past as an advantage. For years, any product thatâ€™s similar to a BIâ€”new visualization tools, exploratory data science environments, chatbots that write SQL queriesâ€”[eventually becomes BI](https://benn.substack.com/i/134863380/twenty-pages-of-context-in-a-ten-page-window), and has to build [everything that goes with it](https://youtu.be/1iNaR1ie7YA?t=74). This protects established players who already check most of the BI boxes from narrow upstarts that sell just one reinvented sliver of the category.

So far, generative AI hasnâ€™t altered this pattern. If anything, itâ€™s reinforced it, by allowing companies like ThoughtSpot and PowerBI to get more leverage out of their BI tooling. But if the AI wave is revolutionary enough, it could finally break that cycle, by creating entirely new paradigms for interacting with data. Analysis could become entirely conversational, removing the need for querying data directly at all. We could stop storing data in warehouses, but dump it all into vector databases that are modeled by GPT-6. Bots could simply start telling us what to do, and dashboards could become an obsolete middleman.Â 

If any of these things happen, the deep moat of features that protects todayâ€™s BI toolsâ€™ would suddenly become a cage, trapping us all in the past as the world moved on.Â 

# â€œIâ€™m your huckleberryâ€

For almost two years, I never told anyone inside of Mode what I was going to write about before I published it. I did that partly because I'm an anti-social hermit and egomaniacal perfectionist; partly because of founder privilege, and I could get away with it; and partly because I was afraid of being edited out of my own posts.[^9] If I was putting my name on it, I wanted to own it, for better or for worse.

This post was the first exception. For obvious reasons, yesterday, I asked a handful of folks at ThoughtSpot if they were ok with its rough thesis. Though it may be better to ask for forgiveness than permission, my thinking went, that only works if you donâ€™t get told to leave before you can plead your case on the company Slack.Â 

I was told [to fire away](https://twitter.com/sudheenair/status/1682205722279546881).[^10]

They didnâ€™t have to give me that leash. They couldâ€™ve tapped the sign, and reminded me that [$4.2 billion](https://www.forbes.com/sites/kenrickcai/2021/11/15/thoughtspot-raises-series-f-at-4-billion-valuation-after-cloud-migration/?sh=6aae92a67ef7) is a lot more than [$200 million](https://techcrunch.com/2023/06/26/thoughtspot-acquires-mode-analytics-a-bi-platform-for-200m-in-cash-and-stock/). Instead, theyâ€™ve consistently reminded me that 4.2 is a lot smaller than [15.7](https://techcrunch.com/2019/06/10/salesforce-is-buying-data-visualization-company-tableau-for-15-7b-in-all-stock-deal/), or [57.8](https://www.google.com/search?q=snowflake+ticker&rlz=1C5GCEM_enUS1043US1043). Until weâ€™re *there*, they said, weâ€™re all upstarts, together.

There are lots of convoluted reasons that companies can fail. A market shift can expose an obscure vulnerability; a competitor can outflank a key messaging beachhead; one product strategy can compound in weird ways with another, in a dynamic best explained by a cable news analogy.Â 

These things are fun to talk about. But most companies arenâ€™t killed by some obscure threat lurking in the weeds that they never see; theyâ€™re killed by the thing that everyone knew about, everyone saw, everyone felt, and *nobody fully faced*. Theyâ€™re killed by a product that never finds a market; by chronically low user adoption; by a buggy product with high churn; by unsustainable and broken unit economics. When youâ€™re a critic, itâ€™s fun to find more creative narratives. When youâ€™re in the arena, [everyone already knows whatâ€™s wrong](https://www.youtube.com/watch?v=PTo9e3ILmms&t=108s)â€”and itâ€™s courage that you need, not cleverness.

Like Snowflake, dbt Labs, Fivetran, ThoughtSpot and Mode could fail. Iâ€™m optimistic, though. And not because of 3,000-words of indulgent analysis, but because when I asked ThoughtSpot what they thought of that analysis, they [didnâ€™t shy away from the fight](https://youtu.be/AeHXXgJbn28?t=71).


---


[^1]: Will this ever work? *Do I want it to ever work?*

[^2]: A definite banger.

[^3]: Just in case, I worked for Mode, and as of Wednesday, now work for ThoughtSpot. When I say â€œwe,â€ I typically mean the combined company.

[^4]: Until, of course, karma catches up to them too, and [their business is sunk by its capital letters](https://benn.substack.com/i/71510341/karma).

[^5]: Or do you dance on your ancestorâ€™s grave, and go with [DATA](https://www.marketbeat.com/stocks/NYSE/DATA/)?

[^6]: This is entirely based on two terrible metrics: rumors, and Sisenseâ€™s declining position on the Gartnerâ€™s magic quadrant from [2019](https://images.app.goo.gl/Y6jcp5ajWyb9Ufwe8) to [2023](https://images.app.goo.gl/oUYwiU7sfMhuuUUa9). Could I be entirely wrong, and could Sisense be making money hand over fist, and about to IPO under the ticker [CANT_STOP_WONT_STOP](https://youtu.be/nfWlot6h_JM?t=91)? Maybe!

[^7]: I also donâ€™t know any specifics about the Periscope acquisition, or what happened in the years that followed. If you do, and there are things we should learn from it, let me know! [benn@mode.com](mailto:benn@mode.com)! Or if you donâ€™t, but you want to make up stories to sabotage us, email me at [benn+hot-acquisition-tips@mode.com](mailto:benn+hot-acquisition-tips@mode.com).

[^8]: This extends beyond the product. Arguably, the most damaging thing that Google and Salesforce did to Looker and Tableau was to impose a corporate [community](https://www.geekwire.com/2023/tableau-has-been-killed-by-salesforce-past-and-current-tableau-employees-gather-at-irish-wake/) and [support](https://www.businessinsider.com/google-cloud-layoffs-support-teams-looker-2022-3) philosophy onto companies that had taken very differentâ€”and successfulâ€”approaches to those areas of the business. Customers werenâ€™t upset at these changes just because they lost a periodically useful service; they were mad because they felt a company they loved was losing its identity.

[^9]: Nobody at Mode ever actually did this. Itâ€™s probably PTSD from when I worked in DC, and everything we published had to go through at least three editors.

[^10]: Not Taylor Swift, but [still a low-key banger](https://www.youtube.com/watch?v=ZI-aPHeUDlk).

================================================================================

# Will we ever have clean data?

*Probably not, but maybe we can work with messy data.*

---

![undefined](https://substackcdn.com/image/fetch/$s_!kGZl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8777e48-a0bb-4794-a621-916ea05f8cf7_1920x2179.jpeg)
*[my man](https://en.wikipedia.org/wiki/Sisyphus_%28Titian%29)*

There are two types of people in the data industry. The first are the starry-eyed optimists [who are hypnotized](https://benn.substack.com/p/the-rapture-and-the-reckoning) by possibility and promise. They [ignore the ugly realities](https://benn.substack.com/p/the-truth-is-out-there) on the groundâ€”messy data, mismatched dashboards, broken pipelines, the three thousand single-tenant Db2 databases that are four versions behind and running in a data center that [Jeff](https://benn.substack.com/p/we-dont-need-another-sql-chatbot#footnote-3-134863380) set up in Milpitasâ€”and instead [imagine what might be](https://benn.substack.com/i/122496697/the-fast-and-the-free) over the horizon. They marvel at every release from OpenAI; they see the potential in every new open-source framework; they [write blog posts](https://benn.substack.com/p/gsnowflake) that sketch out some beautiful and impossible future by recklessly hand-waving past how things really work.

The second group are the cynics who like to remind the first group of how things really work. All of these ideas are just hype for a hypothetical world, [they say](https://benn.substack.com/p/the-rapture-and-the-reckoning#:~:text=If%20ChatGPT%20is%20already%20unacceptably%20bad%20in%20impossibly%20agreeable%20environments%2C%20it%20would%20be%20abysmal%20in%20the%20real%20world.). We need quality; we need reliability; fine, dream big, but with [stable infra](https://boz.com/articles/stable-infra). Garbage in, garbage out. Your insights are only as good as your data. [Your SQL chatbot wonâ€™t work.](https://benn.substack.com/p/we-dont-need-another-sql-chatbot) Theyâ€™ve toiled in the trenches; debugged a revenue dashboard twenty minutes before a board meeting; migrated enterprise clients off of and back onto Oracle; [seen things you people wouldnâ€™t believe](https://www.youtube.com/watch?v=ARPCjp0ppEE). Theyâ€™ve walked through the valley of the shadow of death, and fear no evilâ€”only Jeff.Â 

This week, the latter group [got a new tool](https://blog.sdf.com/p/introducing-sdf-the-semantic-data):

> SDF is a compiler and build system that leverages static analysis to comprehensively examine SQL code at warehouse scale. By considering all queries in any dialect simultaneously, SDF builds a rich dependency graph and provides a holistic view of your data assets, empowering you to uncover problems proactively and optimize your data infrastructure like never before.
> The standout feature of SDF is its ability to annotate your SQL sources with rich metadata and reason about them together. SDF metadata can range from simple types and classifiers (PII) to table visibility and privacy policies (Anonymize). When SDF performs its static analysis, it takes this metadata into account, propagates it throughout your SQL sources with [Information Flow Theory](https://www.microsoft.com/en-us/research/publication/bootstrapping-privacy-compliance-in-big-data-systems/), and enforces built-in and user-defined rules. We call these Checks. Here are some simple examples of powerful SDF Checks:

Excellent; we all love these things. Do I want easy ways to anonymize PII? [For sure.](https://docs.google.com/presentation/d/1rIehiqobQWgyCYdgC3LqbcOOaysrsMzA/edit#slide=id.g247fdd77c11_0_427) Do I want to violate some GDPR policy and have Europol [put me in a room](https://youtu.be/Un9SPXQyirI?t=64) thatâ€™s too short to stand up in and not wide enough to lie down in? Nope. Do I worry about accidentally summing dollars and yen and overstating our revenue by [14,000 percent](https://www.google.com/search?q=dollars+to+yen)? Not really, though I bet big companies do, and they spend a lot more money on data infrastructure than me.

But do these features address the underlying problems that make data so unwieldy to work with? Mostly, no. Companiesâ€™ data infrastructuresâ€”including those at many young startupsâ€”are often terminally messy, full of brittle scripts, manually updated CSVs, and â€œpipelinesâ€ that involve transferring an Excel file from one computer to another with a USB drive.[^1] Like most other products that are roughly related to data quality and reliability, tools like SDF can help us find and debug the inevitable problems that emerge from this unworkable stack, but they donâ€™t *fix* them. Theyâ€™re small improvements at the edges, [dwarfed](https://knowyourmeme.com/memes/excavator-digging-out-suez-canal-ship) by the scale of the problem, an [ointment for a cannonball wound](https://youtu.be/B7sgN1Hb2zY?t=725) or a painkiller for a cancer patient: Useful, potentially worth a lot of money and ultimately, more of a temporary comfort than a lasting cure.

To be clear, this isnâ€™t a criticism of SDF, which doesnâ€™t claim to solve these problems, or of any other similar tool. Data observability; data contracts; even â€œsemantic lintersâ€ like SDFâ€”Iâ€™d rather manage a data stack that had all of these things than one that didnâ€™t. And for SDF specifically, Iâ€™m sure their initial launch is just the beginning, with more to come.[^2]Â 

Instead, itâ€™s a question about why this keeps happening. Why do we keep feeding data reliability products into the maw of the problem theyâ€™re trying to solve? Why have hundreds of very smart people spent thousands of hours and millions of dollars on data quality tooling, and most of us still canâ€™t even make trustworthy dashboards? Why is this so hard?

After thinking about it for hoursâ€”hours!â€”I have my guess.Â 

# Data quality is a tradeoff

When people talk about data quality and reliability, they often implicitly frame it as an unambiguous fight against entropy. We win if weâ€™re persistent, prudent, disciplined, and thoughtful; we lose if we are lazy, reckless, inattentive, or foolish. But we would never lose because we *chose *to.Â 

Iâ€™m not so sure thatâ€™s true. Though weâ€™d never quite characterize it this way, I think a lot of data teams implicitly *and reasonably* choose disorder and disorganization.Â 

Consider how a company builds out their data infrastructure. They start with a few basic pipelines that power a handful of simple reports and dashboards. The business grows; more functions need more reporting; some marketing project unexpectedly catches fire and data tooling has to rapidly expand to support it; some partner initiative fails and a data sharing program gets axed. In hindsight, the Frankenstein stack that gets createdâ€”planned one step at a time, full of half-built experiments and partially-deprecated failuresâ€”looks like a huge mistake. But, [just as true of analysis](https://benn.substack.com/p/analytics-is-a-mess), the mess has a purpose:

> Because no company is the same, measuring a business, as was the case for us when we were measuring our win rates, is a creative process. Inevitably, even the best laid reporting plans give way to a lot of exploratory messes. Each potential metric produces a bunch of analyses to assess it; each analysis produces more questions and ad hoc offshoots. Multiply this by all the metrics and dashboards on your blueprint, and complicate it by constantly shifting the business underneath it, and the development process looks less like an organized construction site and more like an [artistâ€™s studio](https://www.artistrunwebsite.com/inspiration/1474/Studio+Sunday%3A+Alexander+Calder) or a [writerâ€™s desk](https://www.reddit.com/r/pics/comments/8h4u14/this_is_the_new_york_review_of_books_office/).

This dynamic actively works against a lot of our existing data quality tools. Those tools typically encourage a slow march towards stabilityâ€”over time, data teams should gradually addÂ more models, tests, policies, and contracts. But data and the things people create with it are often more dynamic than that (and, Iâ€™d suspect, more dynamic than the software systems that theyâ€™re borrowed from). A high-quality dataset is one that is consistent with the business concept it represents. Excluding the datasets that sit behind legally-defined financial metrics,[^3] those business concepts are often fluid. New OKRs get spun up every quarter; projects take off or wind down; new data sources become urgent requirements as business initiatives change. Data teams have to absorb every change from every department they serve.Â 

The good news, however, is that none of this is incompatible with data quality itself. We just have to imagine different ways to provide it. Instead of focusing on stability, for instance, are there ways to make *instability* safer? Or, to take it even further, could we make things easier to refactor, and in fact, encourage *more* rewrites?

Put differently, the modern data stack has created a number of cottage industries in its wake. One of them is for consultants to come in and reset the whole thing, by cleaning up data sources, tidying up dbt, consolidating metrics, and so on. For most companies, the day these consultants wrap up their job is probably the day their data is at its best, because it was designed for the business that exists in exactly that moment.Â 

When consultants leave, companies have two choices: Try to move the business and its data in lockstep, or let them drift and make it easy to snap them back together when they do. Iâ€™m not convinced that the latter wouldnâ€™t be easier to do.Â 

How? I have no idea, but I can handwave some starry-eyed optimism at it.Â 

# Data resiliency 

Hereâ€™s a question related to messy data: How many dbt models is too many?

A thousand seems like a lot? Your average business doesnâ€™t have that many metrics or entities to report on. At first glance, it seems crazy that a vanilla SaaS company or an ecommerce store that runs on Shopify would ever need anywhere near that many tables to describe and analyze their business.

But there are two reasons why dbt projects might get that big, or bigger. The first is boringâ€”some source schemas are huge. Salesforce alone has [more than 1,100 standard objects](https://developer.salesforce.com/docs/atlas.en-us.object_reference.meta/object_reference/data_model.htm) in its application; if you want to query those, your dbt project can get big in a hurry.Â 

The second reason is more interesting. When youâ€™re trying to clean up a few raw tables into something usefulâ€”say, going from Salesforce accounts, opportunities, and opportunity line items to a table of something like customer contractsâ€”there are often a lot of intermediate steps in that calculation. You have to join tables in one step, aggregate them in the second, join the aggregates in the third, apply a lag function to figure out when the accountâ€™s last contract ended in the fourth, dedupe in the fifth, and so on. These are the kinds of calculations that, if done in a single query, would probably happen in a series of CTEs. In dbt, however, it can make sense to pull each one (or some reasonable set of a few) out into their own models. This way, if youâ€™re debugging this giant knot of logic, you can query that intermediate model directly. Or if itâ€™s useful in other calculations, you can recycle it.Â 

In this sense, dbt models can be akin to functions in a piece of software. You want them to be [short and non-repeating](https://see.stanford.edu/course/cs107). To judge a dbt project by how many models it has is like judging a program by how many functions it has: More isn't better, but fewer isnâ€™t better either. Itâ€™s about what the functions do, not how many there are.

Well, itâ€™s *sorta* like that. dbt models reference each in a much cruder and more brittle way than proper functions. They can't take arbitrary inputs, and you can only reuse outputs, not the functions themselves. For example, you canâ€™t create a function for [adding sessions to an event stream](https://mode.com/blog/finding-user-sessions-sql/), and then reuse that same transformational logic on several different tables.[^4] The danger is itâ€™s promising enough *and* uncanny enough that it feels easier to create clean lineage graphs than it is. And because of that, itâ€™s tempting to try but hard to get right, projects can become [sprawling](https://www.youtube.com/watch?v=gfCOnuAVwOg)[^5] and redundant, and rewrites and updates get very hard.Â 

SDF, actually, could start to fix this. From my understanding, the core technology behind SDF Is a generic SQL parser.[^6] If that parser could trace the computational lineage of a column through a series of dbt models, analysts could start inspecting projects more like engineers inspect code. Today, to figure out exactly what `90_day_new_user_retention` means, analysts have to read through every query that sits underneath the model with that column. A parser could, in theory, give a more succinct readout, showing exactly where that column came fromâ€”including beyond dbtâ€”and what logic was applied to it in its upstream queries. That makes debugging and fixing problems much easier, especially if you can reverse the inspection, and see exactly what logic, and not just which queries, is downstream from a potential change.Â It's not a data reliability tool; it's a data resiliency tool.

Maybe it works. Maybe it doesnâ€™t. Maybe Iâ€™m hypnotized by the possibility and promise of three-letter tools; maybe weâ€™re all eventually moving back to Oracle. I donâ€™t know. But the other approachâ€”leaving it all up to Jeffâ€”doesnâ€™t seem to be working.Â 


---


[^1]: Fun fact: The pipeline to count the number of people using some part of Office 365 once included this step.

[^2]: On the other hand, [there is no step two](https://benn.substack.com/p/clear-eyes-full-hearts).

[^3]: Do I want to violate some GAAP policy and have the SEC put me in a room thatâ€™s too short to stand up in and not wide enough to lie down in? Nope.

[^4]: Yes, you could do this in your Python orchestrator of choice, and you maaybe do with dbt macros? Maaaaaybe? But plain SQL is often more accessible.

[^5]: Terrible video; top five single song performance Iâ€™ve ever seen live.

[^6]: If SDF product doesnâ€™t work but the SQL parser does, thereâ€™s probably a business to be had white-labeling it and selling it to other data vendors, all of whom would [love to have one](https://youtu.be/NQmOiEJ8fEs?t=983).

================================================================================

# The emotionally informed company

*The heart has reasonâ€”and convictionâ€”that reason does not know.*

---

![Good Will Hunting (1997) - IMDb](https://substackcdn.com/image/fetch/$s_!D1Zc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9208c762-752a-49cd-8883-a407212e4da3_1472x980.jpeg)
*[Your move, chief.](https://www.youtube.com/watch?v=8GY3sO47YYo)*

â€œCorrelation does not imply causation.â€

We know. Everyone knows. Google [knows](https://www.evernote.com/shard/s58/sh/25478224-9e8b-4b34-83e7-470ec07e6a4b/FzfhPUyCu49nMdI8hr3ucEytBFtjqxEDCfST48kRIgUs757l5jjgLG_Dww). ChatGPT [knows](https://chat.openai.com/share/9c7411b2-ae7c-4756-8745-0cde0dc02f0c). An entire Wikipedia article [knows](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation). We know it so well that the catchphrase has become so ubiquitousâ€”among everyone from professional analysts to [Twitter reply guys](https://twitter.com/paulg/status/1660630231030300673/retweets/with_comments)â€”that weâ€™ve almost become instinctively suspicious of correlations, presuming them to be guilty of being spurious until proven innocent.Â 

Iâ€™d argue that itâ€™s an overcorrection. Correlation may not imply causation, but itâ€™s almost always its [leading indicator](https://xkcd.com/552/).[^1] Just as itâ€™d be silly for detectives to ignore a suspected serial killerâ€™s [ten-year history](https://www.nytimes.com/2023/07/20/nyregion/gilgo-beach-serial-killer.html) of asking Google if police had any new leads on a bunch of unsolved murders, itâ€™s [midwitting](https://knowyourmeme.com/memes/iq-bell-curve-midwit) to dismiss correlations as useless because they havenâ€™t been proven to be causal. Correlations are circumstantial evidenceâ€”but circumstantial evidence isnâ€™t inadmissible evidence.[^2]Â 

When youâ€™re running a company, the same is true for how you feel.

For a bunch of reasonsâ€”the general usefulness of science over intuition, the decades-long mass movement to be more data-driven, the rising cultural popularity of people like Nate Silver who [sold us all](https://techcrunch.com/2012/11/07/nate-silver-as-software/) on the â€œmagicâ€ of the â€œquantified universe,â€[^3] sexism[^4]â€”companies, and society at large, have dismissed emotions as useful inputs for making decisions. They are biased, corrupting, messy human distortions of an unyielding scientific reality. In ways that are both obvious and subtle, and both *de jure* and *de facto*, weâ€™ve all made emotions another form of inadmissible evidence.Â 

For example, in Silicon Valley, itâ€™s trendy to use quantitative jargon that disguises emotion and estimation as mathematical fact. We donâ€™t have opinions; we have Bayesian priors and [robust latticeworks of mental models](https://twitter.com/SteadyCompound/status/1646452980550434816). We donâ€™t guess; we make decisions probabilistically. Stories and anecdotes are small samples and Ns of 1. Unquantifiable quantities are nonsensically measured in [orders of magnitude](https://twitter.com/chamath/status/923683131688923137). And we use â€œemotionalâ€ as a pejorative. The important thing to know about our feelings, we seem to be saying, is that they arenâ€™t feelings at all.

I think thatâ€™s a mistake. Thereâ€™s a lot of signal in our emotions, even if itâ€™s not scientifically robust or â€œstatistically significant.â€ And companies and cultures shouldnâ€™t equate their efforts to be more analytical with dismissing how they feel.

# Inexplicable, but not imaginary

If you torture a financial model for long enough, [you can make it say anything](https://en.wiktionary.org/wiki/if_you_torture_the_data_long_enough,_it_will_confess_to_anything). Youâ€™re launching a big rebrand; thatâ€™ll increase the conversion rate on your website by a couple percentage points. You have a couple big features coming; you can probably charge higher prices once theyâ€™re out. Your business development team is growing; theyâ€™ll bring in more leads. And theyâ€™ll get more efficient because youâ€™re about to make your first sales enablement hire. Combine all of these improvements togetherâ€”all modest, on their ownâ€”and you can hit the aggressive growth target youâ€™re promising your board.Â 

The Excel file says itâ€™s reasonable. Your heart tells you itâ€™s not. You canâ€™t point to the exact inputs that are off, but you know that somethingâ€™s wrong with the output. You know that, somewhere in the ostensibly conservative numbers, a bunch of plausible assumptions are compounding into a gut-wrenching result.Â 

As Iâ€™ve [said before](https://benn.substack.com/p/clear-eyes-full-hearts#:~:text=Startups%20don%E2%80%99t%20die,emotions%20inadmissible%20evidence.), â€œstartups donâ€™t die because leaders miss these feelings; they die because teams ignore them.â€ They die because they assume their numbers are right and their emotions are wrong. They die because their instincts donâ€™t add up but their spreadsheets do, and because â€œin God we trust. [All others must bring data.](https://www.ibm.com/blogs/nordic-msp/in-god-we-trust-all-others-must-bring-data/)â€ 

If weâ€™re going to run companies with punchy quotes, Iâ€™d rather itâ€™d be Blaise Pascalâ€™s: â€œThe heart has its reasons that reason does not know.â€[^5]Â 

â€”

In a viral post about [looking for love](https://www.henrikkarlsson.xyz/p/looking-for-alice), Henrik Karlsson says that the best relationships are often the ones that canâ€™t easily be explained:Â 

> When you talk about people you like, or rather when you talk about that thing that happens between youâ€”you have to transform a very complex impression into a string of words. Some relationships can easily be compressed into a compelling string of words. This is usually because they conform to some sort of trope of how romance should look. In my experience, great relationships are harder to compress into a sensible string of wordsâ€¦
> This will tilt the playing field against the wonderfully incomprehensible and singular relationships that you should be looking for.
> I remember with a cold sweat that I almost turned Johanna [Karlsonnâ€™s now wife] down because I felt confused by my inability to explain what our relationship was and why I liked it.

In other words, analytical thinking and its quantitative cousins have a bias. They favor things we can reason about, quantify, and express in a clear vocabulary. *But those arenâ€™t the only things that exist.* This is obviously true in love, but itâ€™s also true in how we make decisions.[^6] Fear, anxiety, excitementâ€”there is something underneath these emotions. *Something *is making us nervous. *Something* is putting a pit in our stomach. *Something* is getting me out of bed early to work on this new project. â€œIf you suddenly fall in love, if you develop a weird compulsion, if youâ€™re incredibly anxious for no reasonâ€¦there is a reason. [You have to look for the reason.](https://ava.substack.com/p/therapy-as-a-way-of-aligning-with#:~:text=If%20you%20suddenly%20fall%20in%20love%2C%20if%20you%20develop%20a%20weird%20compulsion%2C%20if%20you%E2%80%99re%20incredibly%20anxious%20for%20no%20reason%E2%80%A6%20there%20is%20a%20reason.%20You%20have%20to%20look%20for%20the%20reason.)â€ 

Ignoring that because we canâ€™t rationalize itâ€”or, more frequently, staying quiet about it because we donâ€™t want to be seen as emotional or biasedâ€”is how we walk headfirst into [problems that everyone saw coming](https://benn.substack.com/p/how-an-acquisition-fails#:~:text=But%20most%20companies%20aren%E2%80%99t%20killed%20by%20some%20obscure%20threat%20lurking%20in%20the%20weeds%20that%20they%20never%20see%3B%20they%E2%80%99re%20killed%20by%20the%20thing%20that%20everyone%20knew%20about%2C%20everyone%20saw%2C%20everyone%20felt%2C%20and%20nobody%20fully%20faced.). Itâ€™s how we convince ourselves that our current plan could work even when we know it wonâ€™t. Itâ€™s how we talk ourselves out of being excited about something we canâ€™t quite explain. Itâ€™s how we tell ourselves the lies that [Julie Zhuo warns us about](https://lg.substack.com/i/117399250/lies-builders-tell-themselves):

In all of these examples, we usually know that somethingâ€™s wrongâ€”or right!â€”before we can describe exactly what it is. We shouldnâ€™t ignore that spark, regardless of how inexplicable it is. Not every impulse is a breakthrough, and not every moment of panic will save you from a disastrous decisionâ€”but nearly every big idea starts with an impulse and nearly every company-saving pivot starts with a panic.Â 

# The paradox of emotional disclosure

Several years ago, a Mode executive gave a presentation to the rest of our leadership team to make the case for opening an office in New York City. It was a tight and well-reasoned argument: We had a lot of customers there that could be better served by an in-person presence; we were getting more New York-based leads and face-to-face demos had much higher close rates; we could double our recruiting pool by hiring in New York.Â 

But the pitch has one glaring omission: He never admitted that he *wanted* to move to New York.Â 

In this instance, it didn't matter that much; we all knew he wanted to move, so his â€œoversightâ€ became more of a joke than a problem. Still, it was an interesting thought experiment. Had we not known, should he have told us?

Most peopleâ€™s default answer is probably yes. If he had an ulterior motive to argue for the office, he should tell us, out of some abstract duty to be transparent, or to tell the truth, the whole truth, and nothing but the truth.

But if you push on this further, it gets very weird and confusing. We should base our decision about the office on the business case for it. If he made a compelling argument for that, and we bought it, why do his motivations matter? Moreover, had he told us he wanted to move, it couldâ€™ve actually biased *us,* by making us assume he fudged his numbers or put his thumb on the scale. And thatâ€™s an awkwardly damning thing to admit, because it suggests that weâ€™d be fooled by biased analysis unless we knew it was biasedâ€”and isnâ€™t the whole point of quantitative analysis that we can judge it on its own merits?[^7] 

Or maybe, his partiality was actually good! We [often assume](https://avc.com/2010/04/no-conflict-no-interest/) that having a personal or â€œfinancial interest in something means you've got a conflict and your opinion is somehow â€˜tainted,â€™â€ but if people are of sufficiently upstanding character, itâ€™s just as likely that that â€œinterest implies knowledge and understanding and leads to good decisions.â€ Or, per the point Blaise Pascal made four hundred years ago, his urge to move couldâ€™ve been driven by some insight that his heart felt but his head couldnâ€™t yet describe. Had he told us about that feeling, we mightâ€™ve been able to put it to words, and find the causation in his correlation.[^8]Â 

To make things even weirder, our decision about the office was probably going to be an [emotional one anyway](https://youtu.be/AwpjBs18zPo?t=125)![^9] Numbers and analysis will only take us so far. The final decision wasnâ€™t going to come from a calculator, but from our CEO, taking an emotional leap of faith.

And yet, we all pretend that both of these factsâ€”that the pitch was emotional, that the decision was emotionalâ€”arenâ€™t true. We hide our emotions behind our numerical technobabble, go through two-sided charades of pretending that we all just care about the numbers, and rarely acknowledge we *feel* something. But doing exactly thatâ€”putting our emotions on the table, alongside our graphs and chartsâ€”is probably the only way to untie this knot, and have a truly honest conversation about what to do.

â€”

There was another Mode exec who used to start his one-on-ones with his reports by telling them how he felt. Most notably, when he was mad, he'd start meetings by saying, â€œI want you to know that Iâ€™m mad at you.â€Â 

This is not your typical managerial advice. [Todayâ€™s textbooks](https://www.betterup.com/blog/how-to-give-feedback) tell managers to act more like a neutral bystanderâ€”I observed this, the impact was that. If you confess your emotions to people, the standard guidance implies, theyâ€™ll assume youâ€™re irrational. Though most management seminars[^10] don't explicitly tell people to hide their emotions, they promote a general approach of level-headed detachment, and they certainly donâ€™t recommend *introducing* emotion into any professional conversation.

In the context of a culture that stigmatizes being emotional (and aggressively so against women and people of color), this advice makes sense. But in a vacuum, would it still be right? If youâ€™re mad, youâ€™re going to make decisions with that anger. Youâ€™ll probably look for ways to show people youâ€™re mad, but do it in a professionally deniable wayâ€”by saying â€œwe need to talk about this immediately,â€ by having a stern tone, by using formal and indirect language about observations and business impacts. Would it not be better if we could just say weâ€™re mad? We donâ€™t have to *act* mad, but we donâ€™t have to pretend that weâ€™re Vulcans either. In tough conversations, people are going to be more worried about our emotionsâ€”and spend more time trying to read between the lines to figure them outâ€”than they are whatever substantive thing weâ€™re saying. Hiding those emotions neither eliminates them nor protects people from themâ€”so why pretend that it does?[^11] 

As businesses, we should be honest about the decisions we make in the same way. Theyâ€™re emotional; theyâ€™re going to be emotional. *Weâ€™re* going to be emotional. Analytical histrionics donâ€™t change that; they just cover it up.

# Informed, not driven

One of the first [cyclical corrections](https://astralcodexten.substack.com/i/67112750/phase-involution) to the corporate movement to be more data-driven was a push to be [data-](https://www.youtube.com/watch?v=bKZiXAFeBeY)*[informed](https://www.youtube.com/watch?v=bKZiXAFeBeY)*. Data is useful for generating ideas and validating our intuitions, wise people would say, but we canâ€™t [follow it blindly](https://www.youtube.com/watch?v=DOW_kPzY_JY).

Emotions should be the same. The best companies and leaders arenâ€™t driven by emotions, but they donâ€™t ignore them either. They are informed by them, and make use of themâ€”as correlations, as evidence, and as a way to have honest conversations.[^12] 

And maybe most importantly of all, emotions give people and companies the courage to be daring. One of the other common criticisms of data-driven decision making was that it would lead us to [local maxima](https://52weeksofux.com/post/694598769/the-local-maximum). I think thatâ€™s true, but not because weâ€™ll mechanically A/B test our way to some suboptimal peak; I think thatâ€™s true because [analytical cultures are](https://www.wayfaremagazine.org/p/insight-at-first-sight#:~:text=They%20are%20wary%20of%20dubious%20forces%2C%20without%20and%20within%2C%20that%20might%20wreak%20havoc%20on%20their%20carefully%20constructed%20lives.) â€œwary of dubious forces, without and within, that might wreak havoc on their carefully constructed lives.â€[^13] No force is more dubious than a sudden burn of excitement, or a sinking bout of despair. But the next great idea [is in these feelings](https://www.inc.com/business-insider/amazon-ceo-jeff-bezos-says-his-best-decision-were-made-when-he-followed-his-gut.html)â€”and data alone [will never give us the courage](https://benn.substack.com/p/does-data-make-us-cowards#:~:text=Sometimes%2C%20the%20most%20we%20can%20say%20is%2C%20%E2%80%9CWe%20don%E2%80%99t%20know%2C%20and%20we%E2%80%99re%20not%20going%20to%20know.%E2%80%9D) to move on them.


---


[^1]: You could write an entire essay about this strip. Is it yet another â€œfooled by correlationâ€ joke? Is it actually criticizing people who are over-eager to say correlation doesnâ€™t imply causation? Is it a subtle and brilliant troll thatâ€™s meant to *look* like the former, so that people lazily link to it when they want to say correlation doesnâ€™t imply causation, but is in fact making the opposite point?

[^2]: This is not a legal statement! Iâ€™m talking about making decisions about businesses or whatever, not about convicting people of crimes.

[^3]: NSFW, but, wow, ok, [IsNateSilverAWitch.com](https://isnatesilverawitch.com/) really took a turn. The funniest part of this is that the new page completely ignores that the site is called IsNateSilverAWitch.com.

[^4]: No discussion about emotional and empirical reasoning would be complete without at least acknowledging that they're both deeply gendered, and that, regardless of the merits of either, it's hardly a surprise that society elevates the one associated with the more powerful group. Interpreting that correlation is an exercise thatâ€™s left to the reader.

[^5]: There are a number of translations of this quote. I like this version. Unfortunately, the [most common](https://www.goodreads.com/quotes/559339-the-heart-has-its-reasons-which-reason-knows-nothing-of)â€”and accurate, I guessâ€”one is â€œthe heart has its reasons which reason knows nothing of,â€ which really [J. Alfred Prufrockâ€™s](https://stuffjeffreads.wordpress.com/2013/02/10/the-love-song-of-j-alfred-prufrock-by-t-s-eliot/#:~:text=The%20first%20two%20lines%20are%20kind%20of%20the%20set%20up%2C%20creating%20a%20pleasant%20sense%20that%20is%20then%20slaughtered%20by%20the%20third%20line.) the ending.

[^6]: And in physics, apparently. Do any of the [quantum mechanics](https://phys.org/news/2019-11-quantum-physics-reality-doesnt.html) behind this whole superconductor circus make any sense at all? Not at all, but, [rock still float](https://www.youtube.com/watch?v=X5EoUD-BIss).

[^7]: For better or for worse, I think the answers here are [no](https://benn.substack.com/p/the-case-for-being-biased), thatâ€™s not really the point of analysis; [no](https://benn.substack.com/p/a-better-way-to-lie-with-statistics), we arenâ€™t able to judge it on its own merits; and [yes](https://benn.substack.com/p/tilt-and-tilted), that does mean weâ€™re easily fooled by it.

[^8]: In this case, this particular thing seems unlikely; he wanted to move for personal reasons, not because he had a gut feeling that moving to New York would be good for Mode. But you could easily imagine other situations where this applies. If a product manager really wants to build one feature over another, is that some irrational feeling? Or is it their subconscious, lossily synthesizing dozens of customer conversations into a feeling? More often than not, Iâ€™d bet on the latter.

[^9]: h/t to Robert Yi for [linking to this interview](https://win.hyperquery.ai/p/what-are-our-stakeholders-doing-even#:~:text=Your%20decision%2Dmakers%20might%20be%20far%20too%20emotional). Also, as a tangential aside, his post makes a useful point about how making a decision is much harder than evaluating one. Prior to doing something, the universe of options is enormous; after doing it, all you have to do is judge that one thing. Which might explain [why action](https://commoncog.com/when-action-beats-prediction/)â€”trying more stuff and seeing what worksâ€”may lead to better decisions than analysis.

[^10]: Iâ€™ve been to a number of these, which probably comes as a surprise to everyone whoâ€™s ever reported to me.

[^11]: Some people might say that this isnâ€™t true. If you take a deep breath and act calm, youâ€™ll start to *feel* calm. Fine, fair. However, youâ€™ll never fully get rid of your emotionsâ€”nor, probably, should you, for all of the other reasons mentioned in this post. Furthermore, you can tell people youâ€™re mad without acting mad. Iâ€™d even go so far as to argue that telling them you're mad actually makes it *easier* to act calm, because it relieves you of the urge to show them that youâ€™re mad.

[^12]: And, obviously, to [appear human](https://www.youtube.com/watch?v=N_ndmTtsv44).

[^13]: The [David Brookses](https://www.nytimes.com/2023/05/04/opinion/gen-z-adulthood.html) of the world like to talk about a parallel social dynamic that [they call](https://www.aei.org/op-eds/the-changing-face-of-social-breakdown/) â€œpathologies of passivity.â€ As they describe it, some combination of wokeness, cancel culture, and participation trophy entitlement is discouraging Gen Z from being bold and taking risks. I donâ€™t know about *that*, in part because the David Brookses of the world will blame everything on wokeness, cancel culture, and participation trophy entitlement. I could, however, see the quieting of corporate emotions as having somewhat of the same effect, by causing us to lose our conviction in taking chances.

================================================================================

# The hunt for the Holy Grail

*We always talk about single points of failure. But what is our single point of success?*

---

![Ooh, Jay-Z Just Premiered His Video for "Holy Grail" (Bonus: Justin  Timberlake Is Super Hot in It) | Glamour](https://substackcdn.com/image/fetch/$s_!-LAg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42b02be6-bd28-4c59-a4e3-ef362b4296f3_1500x1071.jpeg)

The rock floats, but [for the wrong reasons](https://twitter.com/Andercot/status/1688598313497694208).Â 

Bummer, I guess. We really could've used that rock, [for Antarctica](https://twitter.com/EliotJacobson/status/1688888228613615616), and [for supersonic trains](https://www.energy.gov/articles/how-maglev-works),[^1] for laptops that donâ€™t melt the lacquer off of a dining room table when you have more than six tabs open in Chrome, and for all the VCs that now have to roll their material sciences *nÃ©e* generative AI *nÃ©e *crypto fund [back into a generative AI](https://trends.google.com/trends/explore?date=today%201-m&geo=US-CA-807&q=lk-99,llm&hl=en) fund, until that goes bust too and weâ€™re all [selling ads](https://news.ycombinator.com/item?id=3480985) again. RIP, LK-99â€”at [sixteen days](https://en.wikipedia.org/wiki/LK-99#Publication_history), we believed in you for a half-[Scaramucci](https://en.wikipedia.org/wiki/List_of_humorous_units_of_measurement#:~:text=no%22.%5B47%5D-,Scaramucci,-A%20Scaramucci%20(or) longer than we believed in Anthony Scaramucci.Â 

Still, while it doesnâ€™t look like weâ€™re going to some magical piece of metal that solves all of our problems out of this fuss, I got at least one thing from it: The knowledge that some magical piece of metal *could *solve all of our problems. Three weeks ago, I was vaguely aware of what a superconductor was, but didnâ€™t realize that, one, they donâ€™t really exist in everyday life, and, two, if they did, lots of problems would almost instantly vanish. To me, electrical resistance was an abstract and distant coefficient that last mattered in high school science class.[^2] Apparently, however, the entire world is built on the assumption that that coefficient isnâ€™t zero. Change that assumption, and [everything changes](https://www.vox.com/future-perfect/23816753/superconductor-room-temperature-lk99-quantum-fusion).Â 

More generally, the LK-99 drama reminded me that big and messy problems donâ€™t always require big and complicated solutions. Sometimes, they can be solved in one swing, or by breaking one foundational assumption (or, like, one physical law of the universe). Even if actually doing that is impossible, it seems interestingâ€”and maybe instructive?â€”to know which limitations are at the root of all of our ills.Â 

As a prisoner to the ills of the data industry, this made me wonder if there was a single solution to the things that we struggle with. Is there some Holy Grailâ€”some single point of *success*, on which our [tower of ](https://xkcd.com/2347/)*[problems](https://xkcd.com/2347/)* are builtâ€”that, if discovered, would launch us into an analytical paradise of clean data, universal access, and instant insights?[^3] If we could have one miracle materialize out a [twenty-year soap opera](https://www.reddit.com/r/singularity/comments/15b27wn/the_story_behind_the_invention_of_lk99_reads_like/), what would it be?

# Stacking cache

Instant data processing. I think that having to process data in steps, in batch, is our electrical resistance. It's the cause of nearly all of our problems, and if we could run a computational data pipeline instantly, on demand, from source to final destination, weâ€™d have our utopia.

Everyone already knows this, but very roughly, hereâ€™s how we do things with data. Data gets created across thousands of different sources. We extract (or stream) that data from its source and put it somewhere that we can manipulate it more directlyâ€”usually a database, but sometimes Excel, or a domain-specific analytical tool. Then, that copy of the source data gets processed through a bunch of intermediate stepsâ€”transformed into tables in a warehouse, automatically aggregated into new Parquet files, manually copied into other Excel worksheets, whatever. And somewhere along the way, people *do something* with some of those copies, like query a database to make a chart, or use a table to send an automated marketing email, or export an Excel file to write a financial report to file for the SEC.Â 

Itâ€™s tempting, and not entirely wrong, to think of this like a software application. There are data sources; there are models that describe those data sources; there are layers of logic that control whatâ€™s done with the data in those sources; there are views of the data that come out of control layers. The analogy with software engineering is [right there](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller).

Lots of people noticed this. They also noticed that software, considering the enormity of what itâ€™s asked to do, works remarkably well. Emails get delivered; planes stay in the air; I have complete confidence that the words I type into a Google Doc wonâ€™t vanish or mysteriously change themselves when Iâ€™m not looking. So over the years, and especially so over the last decade, data people looked at all of this and did the obvious thing: We tried to do our jobs the way software engineers do theirs, so we could build things that work as well as what they've built.

We write more code. We version control that code, and have other people review it when we want to change it. [We define APIs and do integration testing.](https://twitter.com/josh_wills/status/1580968665737883650) We monitor our work with observability software. We talk about the need for [open-source libraries](https://www.fivetran.com/blog/can-sql-be-a-library-language). We have [petty fights about stupid things](https://benn.substack.com/p/the-case-against-sql-formatting).Â 

Butâ€¦it hasnâ€™t really worked? Things are better, no doubt, but lots of things are still broken. Weâ€™re still stumbling over messy and disorganized data; we canâ€™t get dashboards to match; people still worry about stuff [changing without warning](https://benn.substack.com/p/all-i-want-is-to-know-whats-different). The most popular feature in every data tool is â€œExport to CSV,â€ in part because Excel filesâ€”static, manually audited, saved on a computer, and guaranteed not to change on their ownâ€”are still the only thing people trust without hesitation. Imagine if people trusted Slack this much: â€œYes of course, we love it. But if the message is important, we save it as Word doc and email our reply to people, just in case.â€

Lots of people have noticed this too, and our collective response has generally been to double down[^4] on engineering principles. Add [unit testing](https://github.com/dbt-labs/dbt-core/discussions/8275)! [CI/CD bots](https://tobikodata.com/intro-sqlmesh-cicd-bot.html)! [Virtual environments](https://tobikodata.com/virtual-data-environments.html), and something apparently [better than virtual environments](https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/)! Eventually, if data teams copy enough from software teams, this stuff will get sorted out.

To the extent that I understand what these things are, sure, they sound useful enough, I guess. But for us to be able to work like engineersâ€”and, more importantly, build things that are as reliable as what engineers can buildâ€”I think we need to solve a more fundamental problem. In software development, logical pipelines are processed at once. In data development, people are working off of an infinitely splintering collection of caches.Â 

Consider a basic web application, or some other relatively straightforward piece of software. When you load a web page, a bunch of nested logical calculations are executed, and ultimately determine what you see on your screen. That web of computation is probably much more complicated than most data pipelinesâ€”*but it all happens at once. *If you see a bug, you know that bug happened live. If you update the code to fix the bug, you can rerun the entire program almost instantly, and see if the bug got fixed. You donâ€™t have to worry about fixing bugs across a bunch of different states; everything is always live.Â 

Unless, of course, itâ€™s not, and your program caches data. A lot of applications use cachesâ€”and managing them is one of the [hardest problems](https://twitter.com/vboykis/status/1384530995353894914) in [software engineering](https://surfingcomplexity.blog/2022/11/25/cache-invalidation-really-is-one-of-the-hardest-things-in-computer-science/). Though Iâ€™m not an engineer, â€œbe mindful of how you use cachesâ€ seems like reasonable advice.[^5]

Contrast this with the sketch of how we work with data. The entire job is practically cache management. The source dataâ€”a cache. The data thatâ€™s been extracted and loaded into the warehouseâ€”another cache. Every intermediate materializationâ€”more caches. CSVs exports and Excel workbooks and files loaded into Python notebooksâ€”also caches. Emailed screenshots of a dashboard that are used by a financial analyst to put together a PowerPoint presentationâ€”kind of a cache, manually transcribed into something else thatâ€™s also kind of a cache.

Two weeks agoâ€”in yet another example of doubling down on how we could be more like engineersâ€”[I said](https://benn.substack.com/p/will-we-ever-have-clean-data#:~:text=akin%20to%20functions%20in%20a%20piece%20of%20software) that dbt models are â€œakin to functions in a piece of software.â€ To twist that analogy a bit further, imagine if every time you ran a function, it didnâ€™t actually execute the functions nested inside of it; it just took the result from the last time that function ran. This would be basically impossible to work with?[^6] And yet, itâ€™s exactly how much most data infrastructure works today.

No wonder nothing ever makes sense. Every number or chart we see is a cache on a cache on a cache. How many? [We have no earthly idea. We just stack it up, and try to keep it DRY.](https://youtu.be/zXH6fhueO3I?t=64)Â 

Of course, we donâ€™t do this because we want to; we do it because we have to. Processing data is expensive and slow; some marketing funnel visualization that sits on top of Stripe data, and Salesforce data, and a stream of a billion events canâ€™t update everything, end-to-end, every time a marketer loads a dashboard.Â 

But what if it could? What if that coefficient was zero, and every pipeline and dashboard could be updated in an instant?

# Fantasyland

Itâ€™s hard to overstate how much easier that would make our lives. Think about what happens today when something goes wrong, like a finance report shows a suspicious number. First, teams have to back through a series of caches to figure out where that number came from (caches that may have since updated). Once they find the offending problem, they fix itâ€”but often, not by reviewing if the report itself is fixed, since that would often take too long to fully refresh. They instead just verify that the intermediate stepâ€”the source data, ingestion job, the semantic model, whateverâ€”that caused the problem is fixed. And they certainly canâ€™t test, or even spot check, every other report thatâ€™s dependent on that piece of the application.

Moreover, even if the problem gets fixed, the original problem could persist. If a result got exported to Excelâ€”i.e., bad data was written to a long-lasting cacheâ€”it can be next to impossible to track down, much less fix. Though all software has tech debt, data debt has a unique permanence to it that makes it especially pernicious.Â 

If everything could be updated immediately, most of these problems would go away. To investigate a suspicious number in a dashboard, data teams could investigate code across the stack, change any of it, and instantly see how it affected *every* dashboard. They could debug the problem alongside the business partner, like an engineer working with a designer, where the stakeholder could test the impact of every change. They could update source data and see everything change right away. They could fix the problem and know that itâ€™s fixed everywhere. Problems would no longer be permanent, but corrected by â€œrefreshing the page and seeing if that resolves it.â€

To frame this in a different way, think about the internal reporting tools inside of SaaS products like [HubSpot](https://www.hubspot.com/products/reporting-dashboards) and [Workday](https://www.workday.com/en-us/products/human-capital-management/analytics-reporting.html). Theyâ€™re limited in what they can do, but theyâ€™re far easier to manage and debug than a modular data stack. Thatâ€™s partly because they have a narrow scopeâ€”but Iâ€™d argue itâ€™s more because the reports are always live, with no cache between the source data and the eventual dashboard. Update anything, and you update everything.[^7]Â Â 

# But, like, *why*?

But, of course, the rock isnâ€™t real. We donâ€™t have quantum computers that can process data instantly. We have MacBooks, that melt the lacquer off of dining room tables when you open six Chrome tabs. So whatâ€™s the point of talking about some hypothetical Holy Grail before an arXiv preprint pops out of a lab?

My first answer[^8] is that if we know that a room-temperature superconductor solves all of our problems, could an *almost*-superconductor solve *most* of them?Â 

For example, more [streaming systems](https://twitter.com/josh_wills/status/1513623711772864514) might help. Streaming isnâ€™t quite the same thingâ€”it wouldnâ€™t really help you if you fixed an issue that needed to be backfilledâ€”but it could get us closer. Data teams could also be more judiciousÂ in exactly where they introduce â€œcachesâ€ and where they try to avoid them. There are some places where they make sense, and some places where they donâ€™t. ELT and its variants have made us relatively lazy about this; thereâ€™s probably a lot of low-hanging fruit that could be picked off.[^9]Â 

My second answer is that, perhaps, until we have our superconductor, we canâ€™t actually work like engineers. If engineers still had to do things with punch cards, or had to build everything in assembly language, they probably couldnâ€™t work like modern data engineers either. Similarly, if the bulk of what we have to doâ€”cache managementâ€”is the thing that engineers try to avoid doing, we might not want to model our tools and technologies after theirs.Â 

Thatâ€™s not to say there arenâ€™t lessons to be learned from engineersâ€”I donâ€™t think we make anything better by going back to a world of ping-ponging Excel files back and forth. But we also have to acknowledge that the problems we have to solve have different physical constraints than the ones engineers are trying to solve. Until, of course, the rock floats.


---


[^1]: [Copyright ](https://www.boringcompany.com/hyperloop)*[2023](https://www.boringcompany.com/hyperloop)*[?](https://www.boringcompany.com/hyperloop) So weâ€™re still pretending this is real? (Also, haha, *[X](https://www.evernote.com/shard/s58/client/snv?isnewsnv=true&noteGuid=3deffc81-529a-444f-af1e-a2e232805dab&noteKey=c8Vqe6hvfgaC6R21SK7sYmHwqW9xMkvXsoa1NIp6g1S7849-rYV821OQKg&sn=https%3A%2F%2Fwww.evernote.com%2Fshard%2Fs58%2Fsh%2F3deffc81-529a-444f-af1e-a2e232805dab%2Fc8Vqe6hvfgaC6R21SK7sYmHwqW9xMkvXsoa1NIp6g1S7849-rYV821OQKg&title=When%2Byour%2Bcompany%2Bname%2Blooks%2Blike%2Ba%2Bdesign%2Belement)*, lol)

[^2]: This tells you how much I know about physics: The last time I learned about it was in *science* class.

[^3]: If I die and I wake up in a world where everyone is excited about clean data and instant â€œ[insights](https://benn.substack.com/p/insight-industrial-complex),â€ Iâ€™ll know Iâ€™m not in heaven; Iâ€™m in the [Good Place](https://en.wikipedia.org/wiki/The_Good_Place).

[^4]: Me, an idiot, in 2013: Hm, yes, I live in San Francisco, the world needs another tech startup to build some software, I will do that.You, a true American hero: What if, instead of a tech company, we start a bakery. And that bakery will sell muffins. And when you buy a muffin, you can wager it on a coin flip. Lose, and lose your muffin. Win, and get two muffins. *And then*, you can then wager *those* muffins, on another coin flip, and win four muffins? And again, for eight? And sixteen? And on and on forever, until we either reverse-[Martingale](https://en.wikipedia.org/wiki/Martingale_(betting_system)) every dollar out of San Francisco, or someone wins 131,072 muffins and we go bankrupt? And what if we call the bakery *[Double or Muffin](https://www.doubleormuffin.com/)*?Â (When I first moved to SF, I met someone who knew Double or Muffin's founders. Their website says that you could only wager your first muffin, and you got to keep it even if you lost. However, I was told at the time that you actually had to gamble your muffin, that you could double or muffin as many times as you want, and that the biggest ever winner won 32 muffins. I donâ€™t know which version is true anymore, but I choose to believe the story I was told because it is  incredible.)

[^5]: Is it? Is this actually how any of this works? Do I even know what a cache is? I mean, honestly, not really. But Iâ€™m gonna run with it until someone tells me itâ€™s wrong.

[^6]: This is basically why [Joel Grus famously hates notebooks](https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1).

[^7]: Iâ€™ve made the case a [number](https://benn.substack.com/p/case-for-consolidation) [of](https://benn.substack.com/p/microsoft-builds-the-bomb) [times](https://benn.substack.com/p/the-modern-data-experience) that a consolidated modern data stack may be better than a modular one. Add this reason to the [strength box](https://en.wikipedia.org/wiki/SWOT_analysis). (Or does it go in the opportunity one? I dunno man. The closest I ever came to being a consultant was in October of 2008, when I got a job offer from some mid-tier firm in DC, and they fired me in November before Iâ€™d even accepted the offer. [Wild time](https://www.cc.com/video/uc9y5b/the-daily-show-with-jon-stewart-cnbc-financial-advice), [2008](https://www.cc.com/video/m0dbcb/the-daily-show-with-jon-stewart-in-cramer-we-trust).)

[^8]: Well, actually, the first answer is that there is [no](https://benn.substack.com/p/how-much-is-265-billion-dollars) [point](https://benn.substack.com/p/the-internet-2022) [to](https://benn.substack.com/p/moneyballing-the-world-cup) [any](https://benn.substack.com/p/should-we-be-nice-to-chatgpt) [of](https://benn.substack.com/p/open-the-window) [this](https://benn.substack.com/p/a-season-without-bats). [We all just entertainers](https://youtu.be/4kc5_pfNHGk?t=59), and Iâ€™m just here to entertain myself and to [keep linking](https://benn.substack.com/p/how-an-acquisition-fails#:~:text=It%20was%20an%20excuse%20to%20live%20in%20my%20head%20for%20a%20minute) to [Griff bangers](https://www.instagram.com/p/CvNUHeooSjN/) until she releases the song already.

[^9]: â€œWeâ€™ve been saying this for years,â€ some will say. â€œ[IATJK](https://benn.substack.com/p/am-i-the-jared-kushner),â€ I will say.

================================================================================

# The smol analyst

*Maybe thereâ€™s a way to make these chatbots useful after all.*

---

![Netflix Announces 'Boss Baby' Animated Series](https://substackcdn.com/image/fetch/$s_!io5r!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F955754e8-f73d-4cb2-b9b1-6ef478f9fadf_1200x675.jpeg)
*[smol CEO](https://en.wikipedia.org/wiki/The_Boss_Baby)*

A few months ago, [Shawn Wang](https://www.swyx.io/), who is more widely known as [swyx](https://twitter.com/swyx), [launched](https://twitter.com/swyx/status/1657892220492738560) a viral GitHub project called [smol developer](https://github.com/smol-ai/developer/).[^1] At its core, the project is an AI agent for writing code, but it puts an innovative spin on what is quickly becoming a tired category: Rather than offering its users a chatbot that writes snippets of code on request, smol developer â€œscaffolds an entire codebase out for you once you give it a product spec.â€

For example, a user might want to create a Chrome extension that blocks paid tweets.[^2] They would write a short description of the extension, including what the tool is and details about what they want it to doâ€”keep a counter of how many tweets itâ€™s blocked; give me a way to whitelist some accounts; make the icon [Bam Adebayo](https://www.youtube.com/watch?v=WB7a8It_i4k). They submit the spec to smol developer; it returns the entire application.Â 

Smol developerâ€™s particularly clever twist is that, after it produces its first draft, it then provides a kind of feedback-oriented IDE where people can tell it what they like and don't like about the application that it just built. They can ask to change the design, request new functionality, or tell it anything that a product manager might tell an engineer after seeing an initial prototype. The bot takes this feedback, combines it with the original spec, and updates the app. And so the process continues, with the user and bot reinforcing one anotherâ€”the app improves as the spec gets refined; the spec gets more descriptive as the app goes from imagined concept to testable product.Â 

Does this work? Can it create complex apps? I have no idea. Still, the approach struck me as subtly revolutionary. Most of whatâ€™s been created with LLMs so far has either shoehorned them around existing workflowsâ€”e.g., an automated assistant in a code editorâ€”or shoehorned workflows around chatbotsâ€”e.g., a Slackbot for writing SQL queries. Smol developer is a step outside of the lines. It doesnâ€™t directly help its users write code; it instead uses code as a silent intermediary between product and product manager. And rather than a chatbot, thereâ€™s an interface designed specifically for drafting, testing, and improving a product spec. If LLMs do end up changing how we use computers, this type of approachâ€”one that introduces new ways of accomplishing some task rather than accelerating old ways; the car rather than the faster horseâ€”feels like a glimpse of the future.[^3]Â 

# â€œMake me a dashboard of song streamsâ€

One of the small ironies of todayâ€™s SQL chatbots is that they help people do exactly a thing that data teams try to discourage. As analysts, we ask our colleagues to help us understand [how our work will be used](https://www.caitlinhudon.com/posts/2020/09/16/data-intake-form). They shouldnâ€™t request some piece of data; they should instead [tell us what theyâ€™re trying to achieve](https://davidsj.substack.com/p/dear-stakeholder#:~:text=When%20you%20ask%20us%20for%20help%2C%20tell%20us%20what%20you%E2%80%99re%20trying%20to%20achieve.%20Don%E2%80%99t%20just%20say%20%E2%80%9CI%20need%20this%20piece%20of%20data%E2%80%9D%E2%80%A6). And if they donâ€™t tell us what they want to use some data pull for, the less tactful among us [pepper them with demands](https://www.youtube.com/watch?v=wCoJ8YS6Pr0&t=175s) to explain why they need it.

Bad bedside manner notwithstanding, itâ€™s good advice. And importantly, we donâ€™t recommend that analysts ask these questions just to make their jobs easier; we also recommend it because [we canâ€™t give a useful answer](https://win.hyperquery.ai/p/should-we-be-more-persuasive#%C2%A7understand-how-your-work-will-be-used) without it.Â 

So [why](https://www.youtube.com/watch?v=LmICuTivx2A) are chatbots different? If we have to ask a bunch of follow-up questions before we dig up some number for people, why are we excited about LLMs that mechanically do exactly that? Whatâ€™s the difference?

There are two possibilities, I suppose. One is that we donâ€™t actually need that context at all. Our back-and-forth could all be self-serving theater[^4] to hide the fact that [weâ€™re mostly here to answer questions and build reports](https://benn.substack.com/p/day-of-reckoning#:~:text=I%20think%20we%20have%20to%20be%20more%20targeted%20in%20our%20ambitions%2C%20as%20both%20data%20teams%20and%20as%20data%20vendors.%20Focus%20on%20proven%20tooling%20and%20use%20cases%E2%80%94reporting%2C%20dare%20I%20say%20decision%20support%E2%80%94over%20moonshots.).[^5] The second is that *chatbots* *do* *need* that context. Without it, theyâ€™re just another code-free BI tool thatâ€™s useful for basic reporting, but under-delivers on the self-service nirvana thatâ€™s long been promised. But, encoding â€œbusiness contextâ€ into some YAML file sounds ridiculous, and describing every detail to a chatbot anytime you want to answer a question sounds exhausting.[^6] For this reason ([among others](https://benn.substack.com/p/we-dont-need-another-sql-chatbot)) Iâ€™m generally skeptical that these bots will [be that revolutionary](https://twitter.com/gaganbiyani/status/1659204770438213638).

But the smol developer approachâ€”treat the bot like an eager but inexperienced employeeâ€”offers a third possibility. Instead of asking for answers, people describe the report or dashboard that they want to create. The  LLM-powered â€œsmol analystâ€ produces a rough dashboard, and the user provides feedback, just as they would a junior analyst. The spec gets more detailed; the dashboard gets more useful. And business context gets added indirectly, as needed, as the spec gets more precise.Â 

Suppose, for instance, a music producer wants to figure out how a [new release](https://www.instagram.com/p/Cv7yTSVIySW/) is performing.[^7] They could write a couple paragraphs about the report they wantâ€”identical to what theyâ€™d send a data team today. They could say they want to see daily streams, streams by region, and how many people have listened to the song multiple times. The smol analyst could crank out a bunch of charts, write some loose narrative about them, and return it to the producer. Then, just as theyâ€™d do for a junior analyst, the producer would send back feedback: This number looks off; this explanation doesnâ€™t quite make sense; can you dig into this unexpected anomaly? The bot creates another draft, the exec gives more direction, and so on.

This could have a handful of big advantages over the chatbot-based approach. First, and most obviously, it'd be more accurate than a zero-shot bot that has one chance to get the answer right.[^8] Though today's chatbots are â€œtrainedâ€ on prior answers, it's mostly through crude and infrequent up and down votes that only indirectly affect the underlying model. The smol analyst would instead get immediate and direct feedback on what it needs to improve, and could inject that feedback straight into a prompt.[^9] Moreover, by gradually refining their requests, people could probably push this type of bot to answer far more complicated questions than a typical chatbot.

Second, the back-and-forth could also help people *ask* better questions. We often donâ€™t know what we want until we start looking for it. Just as itâ€™s almost impossible to write a perfect product spec without testing an imperfect prototype first, itâ€™s very hard to ask exactly the right question before seeing the answers to a few of the wrong questions. A smol analyst would encourage this sort of iterative exploration, which is good for both user and agent.

Finally, it seems like this approachâ€”if it worksâ€”could be applied to adjacent problems with relatively little difficulty. For example, could we create ETL pipelines in this way? Data models? Orchestration schedules? You could imagine someone describing a data model in plain language, and an smol analytics engineer using it produce some scaffolding in LookML or Malloy.

# The black box

There is, however, at least one very big reason why a smol analyst wouldnâ€™t be as useful as a smol developer. In software, *how *code works is in some sense irrelevant; all that matters is that it works. I can test my ad blocking Chrome extension without knowing a line of Javascript, or that Javascript exists at all. If the tool does what I want it to, it works, no matter how â€œbadâ€ its codebase.

In data, black boxes donâ€™t work. Computational process matters. You canâ€™t validate a dashboard by testing that it produces a reasonable-looking chart; you have to make sure that the logic behind its calculations are correct. SQL is [declarative](https://en.wikipedia.org/wiki/Declarative_programming), but used for [imperative](https://en.wikipedia.org/wiki/Imperative_programming) endsâ€”we need to know how it works, step by step. Software is the opposite: It typically uses imperative means for declarative ends.[^10]Â 

That makes the test-and-refine feedback loop much harder for analytical work than engineering work. Whereas a PM can tell a smol developer that their Chrome extension doesnâ€™t seem to be blocking video ads correctly, a producer canâ€™t easily tell a smol analyst that their dashboard is improperly [counting skips as streams](https://www.loudlab.org/blog/why-is-my-track-getting-skipped-on-spotify/#:~:text=Spotify%20'skips%20rate'%20or%20',the%20action%20as%20a%20skip.). Someone would have to review the code to know that.Â 

One obvious solution to this isâ€¦to have someone review the code. Rather than giving every executive a personal data scientist, the smol approach could give every analyst a team underneath them. People ask (human) analysts questions; (human) analysts ask (smol) analysts for help; (smol) analysts produce the drafts, and (human) analysts review them. This is structurally similar to how a lot of data teamsâ€™ peer review processes work today, just with a lot more analysts.

# Multi-model BI

There could also be another way out of the black box. In most conversations about LLM-based applications, we talk about them as if thereâ€™s a single model underneath the product. A model writes a SQL query, for instance, or responds to a support ticket. And the product is as good as the training is for that model.Â 

[Viable](https://www.askviable.com/), a company that automatically analyzes product feedback and has been building on top of LLMs for several years, found a different way to be successful. Instead of relying on one refined model, Viable [uses a network of them](https://www.askviable.com/docs/reports#:~:text=How%20are%20feedback%20reports%20generated%3F), each of which specializes in a narrow task.[^11] One organizes input data; one finds themes across those inputs; one writes summaries of each theme; one uses those summaries to author a final report thatâ€™s sent back to Viableâ€™s users. Viable also uses ancillary LLMs to help people understand what itâ€™s doing. Thereâ€™s a model that describes the assumptions that the analytical models are making, and gives users a chance to correct them. If people want to clarify something, they explain it to the assumption model, which passes their feedback down to the analytical models, which update their work based on it.

A smol analyst could follow this same approach. When itâ€™s asked to produce something, it could have one model describe a query plan back to the user, who could validate it or correct what it got wrong. Another model could translate queries into English summaries, just as an analyst might when they share their work back to an executive. And a different LLM could automatically create text-based data models like [TextQLâ€™s capsules](https://www.textql.com/blog/introducing-the-capsule) from reports that were marked as correct.Â 

Of course, none of this fully illuminates the black box. The query plan could get misinterpreted by the SQL-writing LLM, or the summary bot could get its explanation wrong. The only way to know for sure what a query or Python script does is by reading the query or Python script. But this kind of smol analystâ€”which, at this point, is more [tol than smol](https://www.urbandictionary.com/define.php?term=tol%20and%20smol)â€”could go a long way in upgrading todayâ€™s bots from novel toys to potentially useful agents.


---


[^1]: â€œSmol,â€ [a popular social media app tells me](https://www.tiktok.com/@my.mini.bakes/video/7146958442693954821), means â€œsmall,â€ but, like, in a cute way. I did not know this either.

[^2]: Or, more accurately, an X extension that [unblocks unpaid posts.](https://www.washingtonpost.com/technology/2023/08/15/twitter-x-links-delayed/)

[^3]: Iâ€™m not saying that this *is* the future; Iâ€™m saying that itâ€™s the future if LLMs live up to their hype. At this point, thatâ€™s an open question. My view on it has swung from [definitely yes](https://benn.substack.com/p/the-rapture-and-the-reckoning) to [absolutely not](https://benn.substack.com/p/we-dont-need-another-sql-chatbot)â€”so, uh, I guess [BS](https://twitter.com/nntaleb/status/1059208408711266304) stands for Benn Stancil?

[^4]: The real self-serve analytics was the analysis we did to make it seem like we were important. (Alternative footnote: â€œSelf-serving theater about dataâ€ isnâ€™t a bad name for this blog.)

[^5]: â€œFinding insights isnâ€™t even our job. And itâ€™s not making decisions, which is a common misconception. Because actually, our job? Itâ€™s justâ€¦[numbers](https://www.tiktok.com/@official.ra1/video/7251730309614554394).â€

[^6]: That said, [TextQL](https://www.textql.com/), a SQL chatbot, is trying to solve this problem in a pretty interesting way. When we define data models today, we usually default to doing it in a very structured way, like a YAML file of joins and metric formulas. TextQL throws that out, and asks its users to define [capsules](https://www.textql.com/blog/introducing-the-capsule) that map questions and business topics to tables and columnsâ€”e.g., â€œto calculate revenue, join these two tables together, filter out these rows, and sum that column.â€ Not only is this easier for a human to readâ€”including those who donâ€™t know SQLâ€”but itâ€™s also probably a better way to express this information to an LLM. One of the hardest parts of building a SQL chatbot is [compressing a huge amount of schema information](https://benn.substack.com/i/134863380/twenty-pages-of-context-in-a-ten-page-window) into a relatively small prompt. Capsules provide a direct way of doing that: â€œIf you get a question about this topic, use these tables and columns.â€

[^7]: Yâ€™all. Itâ€™s happening. [August 31.](https://www.instagram.com/p/CwDaNjKIblW/) (I think? Griff, big fan, but we gotta talk about this date format. 31.08.23? Even [Excel](https://twitter.com/PleaseBeGneiss/status/1328735477923336192) doesnâ€™t [recognize](https://twitter.com/ExcelHumor/status/1691972862435848571) that as a date.)

[^8]: This highlights another irony with LLMs. With bots, we tend to fret a lot about accuracy. We donâ€™t want to use it, unless weâ€™re completely sure we can trust it. But [I used to be a junior analyst](https://www.youtube.com/watch?v=VqHA5CIL0fg), and got a lot of stuff wrongâ€”and people seemed kinda ok with that? I had to fix it, but there was some expectation that it might take me a couple tries. Why are we comfortable with that, but not comfortable with a bot thatâ€™s equally inaccurate?

[^9]: Time and time again, the solution to making LLMs work seems to be â€œtreat them like they work like a person.â€ That could apply here too. What would help a junior analyst produce a better report, a single unexplained like or dislike on their final draft, or immediate and direct feedback telling them exactly what to improve?

[^10]: For more on declarative and imperative languages, check out thisâ€¦[AI-written advice column on LinkedIn](https://www.linkedin.com/advice/1/how-do-you-compare-contrast-imperative-declarative)? [What on earth is that](https://youtu.be/-wl_ws3sbnc?t=87)?

[^11]: Yet again, to make the most of an LLM, treat it like you would a person.

================================================================================

# How much for that DAG in the window?

*There's a theoretical price and a real price. *

---

![What Channing Tatum's New Movie 'Dog' Gets Right About Military Working  Canines | Military.com](https://substackcdn.com/image/fetch/$s_!rWZc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987f6ac0-96cb-4bfc-ac1e-18711da297a5_621x414.jpeg)
*[kill us](https://www.youtube.com/watch?v=V4tAtp-TyzQ)*

If you want to know the id of white man in tech, ask him what kind of bar he wants to open.Â 

The men who see themselves as tastemakers will describe a bougie cocktail bar with a curated list of small batch bourbons and gins. The menu will call tequila â€œagave;â€ it will specify which drinks are served up and which are neat; there will be no substitutions. They will recommend that their staff follow the [menswear guy](https://twitter.com/dieworkwear/).

Insecure tech bros will dream of opening a dive bar in Detroit, or Baltimore, or some rusted-out machine shop in Richmond. Theyâ€™ll talk about serving [Natty Boh](https://en.wikipedia.org/wiki/National_Bohemian) and Jameson, because a [performative appreciation for cheap beer](https://www.youtube.com/watch?v=3zDHSLDY0Q8) will definitely convince the locals that, despite growing up in Alexandria and making four million dollars from the Figma acquisition, theyâ€™re not one of *those* [rich men](https://www.youtube.com/watch?v=sqSA-SY5Hro).

Self-described entrepreneurs will tell you that their bar will be next to a laundromat, will be a vodka bar because Moscow mules are the [highest margin cocktail](https://www.provi.com/blog/operations/most-profitable-bar-drinks), and will be near public tennis courts because studies have shown that recreational tennis players have incomes that are some significant percentage higher than the national median. Everything else will be decided by the market.Â 

Men who wish they were in college will open a sports bar. Men who wish they were in their twenties will open a craft beer bar. And men who wish they were podcasters will tell you that if youâ€™d listened to [Andrew Hubermen](https://hubermanlab.com/what-alcohol-does-to-your-body-brain-health/), youâ€™d never open a bar.

My bar would be Friscoâ€™s, located in Hayes Valley in San Francisco. The outside will be unassuming and understated; the inside will be a gaudy and obnoxious carnival. It will be a celebration of San Francisco, designed by someone whoâ€™s never been to San Francisco. There will be bad murals of SFâ€™s proudest landmarks, like the Golden Gate Bridge, the painted ladies, and [Rice-a-Roni](https://www.youtube.com/watch?v=W4J32Evw5cY). Some tables will look like cable cars; others will be up several flights of curvy stairs. Some will be under heaters, and some will periodically get cannoned by a [misting fan](https://www.amazon.com/Portable-Misting-10000mAh-Rechargeable-Operated/dp/B0BTD6VPLG/ref=sr_1_3). The menu will be full of horrificâ€”and horrifically namedâ€”drinks, like the Mission Margarita, a 32-ounce mix of well tequilas, Sweetâ€™n Low, [Mango Madness Snapple](https://abewholesale.com/product/12-16oz-snapple-mango-madness/), a [Bud Light Lime-a-Rita](https://beverages2u.com/product/bud-light-freeze-a-rita-frozen-margarita-icicles-1-box-12-2-oz-icicles/), and some liqueur that turns the whole thing [royal blue](https://en.wikipedia.org/wiki/Golden_State_Warriors).[^1] When you walk in, the entire staff will yell, â€œWelcome to San Fran!â€[^2]

Ever the cradle of innovation, Friscoâ€™s wonâ€™t simply charge a flat price for their drinks. Instead, you have to pay rent: We charge you for your table, by the hour. Early in the afternoon, prices are affordableâ€”rent is low, and drink prices are set at a discounted rate. Get there earlyâ€”get in on the ground floorâ€”and you can [lock in your rent](https://sftu.org/rent-control/) and rights to lower drink prices for as long as youâ€™re at the bar. As it gets later and more crowded, prices of both will go up. People who show up at peak hours pay steeper rents and a higher dollar-to-drink [multiple](https://eqvista.com/company-valuation/startup-valuation-multiples/).[^3] Until, late at night,[^4] the hype bursts, people start to leave, and prices crash. And then, tomorrow, [we do it all again](https://www.marketplace.org/shows/marketplace-tech/what-silicon-valleys-boom-and-bust-history-tells-us-about-its-latest-slowdown/).

Of course, this probably wouldnâ€™t work, because it is insane. But itâ€™s not *economically* insane. Some people could argue, somewhat reasonably, that traditional, pay-per-drink pricing is unfair, because it lets people idly take up space and enjoy ("enjoy") Frisco's ambiance without paying as much as the steady drinkers. Other people could argue, somewhat reasonably, that they should get a discount for coming early or staying late, when demand for a seat at the bar is much lower. And others could even argue, somewhat less reasonably but *plausibly*, that the point of coming to a bar is to have a drink, and charging for every single one discourages customers from being successful. If I want eight Muni Martinis,[^5] why should I be punished for wanting to be a thriving customer? Plus, that last [Sex on the Ocean Beach](https://en.wikipedia.org/wiki/Sex_on_the_beach)[^6] was actually *bad *for me; is it really fair to charge me for something that I didnâ€™t need but was tempted into buying?

Nobody says any of this though, because it is *socially* insane. This is not how bars work. We have standards for these things, and people expect businesses to stick to themâ€”even when the thing theyâ€™re buying is marked up [500 percent](https://www.investopedia.com/articles/personal-finance/011216/economics-owning-bar.asp#:~:text=Just%20to%20note%2C%20the%20markup%20standard%20is%20400%25%20to%20500%25).

Pricing is like that. Even for something as simple as a bar, itâ€™s not as economically fixed as it seems, nor are there natural laws that make some pricing models inherently right or wrong. Lots of pricing structures could make financial sense; each one has tradeoffs that favor some customers over others. We mostly accept pricing as a matter of precedent. If, for the last hundred years, bars charged by the hour and gave us one drink every thirty minutes, weâ€™d probably see it as equitable and just. And when one started charging us for drinks, weâ€™d riot.

# The best, in theory

Two weeks ago, dbt Labs started charging [by the drink](https://www.getdbt.com/blog/consumption-based-pricing-and-the-future-of-dbt-cloud/):Â 

> Beginning August 8, 2023 we are taking our first step in a journey towards offering more consumption-based pricing for dbt Cloud. After a ton of researchâ€”both via customer calls and looking at the dataâ€”the metric weâ€™re going to use to measure consumption is the number of model materializations that dbt Cloud has successfully built on your behalf.Â 

To admit my bias here, I have an earnest fascination with dbt pricing, because there is no obvious market precedent for how it should work. dbt is part infrastructure application and part developer tool; it could [conceivably](https://benn.substack.com/p/data-contracts) [be](https://benn.substack.com/p/bi-by-another-name) [extended](https://benn.substack.com/p/down-with-the-dag) [in](https://benn.substack.com/p/the-data-os) [a](https://benn.substack.com/p/metrics-layer) [dozen](https://benn.substack.com/p/powder-keg) [other](https://benn.substack.com/p/the-data-config) [ways](https://benn.substack.com/p/will-we-ever-have-clean-data); itâ€™s very popular, but its most popular element is free. [As much asÂ I enjoy](https://www.linkedin.com/feed/update/urn:li:activity:7095808971860500480/) gawking at the hot goss about these sorts of things, in this case, itâ€™s the analyst in meâ€”curious about an economic puzzleâ€”thatâ€™s drawn to this problem.Â 

The interesting exercise here to me isnâ€™t to figure out whatâ€™s wrong with the new model relative to the old one. For bars and SaaS startups alike, every pricing model has tradeoffs, and has its winners and losers. Itâ€™s easy to find fault in a change to these sorts of things, because thereâ€™s fault in all of them. Instead, the question I have is what pricing model would be the bestâ€”[the best!](https://www.youtube.com/watch?v=s1UOk6nVzpE)â€”if dbt Labs were launching today. If it had no customers on its books and no expectations in the market, how should it charge for its products?

[I tried to figure this out](https://benn.substack.com/p/how-dbt-fails) ten months ago, didnâ€™t see a way to make seat-based pricing work, and landed on exactly what dbt Labs did:

> Though dbt Labs was initially [successful](https://www.getdbt.com/blog/next-layer-of-the-modern-data-stack/#:~:text=count%20to%20~1800.-,We%206x%E2%80%99ed%20revenue(!),-We%20hosted%207%2C000) in selling seats, it didnâ€™t scale with their ambition. If we assume the average revenue per seat is about $125 dollars a month, or $1,500 dollars a year, dbt Labs would have had to sell more than 300,000 seats to get to $500 million in revenueâ€”the eventual number required to justify their [2022 fundraising round](https://www.prnewswire.com/news-releases/dbt-labs-raises-222m-in-series-d-funding-at-4-2b-valuation-led-by-altimeter-with-participation-from-databricks-and-snowflake-301489733.html). Thatâ€™s not an impossible figure, but it would be an awfully steep climb, especially when [other products start offering competitive](https://twitter.com/matsonj/status/1574529855131353088)â€”and freeâ€”development environments.Â 
> â€¦
> Usage-based pricing was the obvious choice, though the optics of this model proved tough. Most metered SaaS products charge seemingly trivial prices for vast amounts of usageâ€”[twenty cents](https://aws.amazon.com/lambda/pricing/) per million Lambda invocations; [$0.0075](https://www.twilio.com/en-us/pricing) per text; [five hundred emails](https://mailchimp.com/pricing/marketing/) or [ten thousand synced records](https://www.fivetran.com/pricing) for a dollar. For the math to work out for dbt Labs, each invocation of dbt run would have to cost about 25 cents. At that price, customers start to flinch at pushing the run button, especially if they are also paying the database to actually execute those runs.Â 
> There were other metering options though, like charging for every call to dbt Server or for every run of every model. Ultimately, dbt Labs settled on the latter optionâ€”customers pay each time an individual model gets invoked. This pushed the unit price of each action down to something more psychologically palatable; it scales directly with customer adoption; unlike charging for dbt run, it doesnâ€™t encourage people to create teetering monolithic dbt jobs to save costs; it still makes sense in a world where dbt run [has less meaning](https://benn.substack.com/p/down-with-the-dag); it meters development and production in a fair way, because small development runs cost a fraction of big production ones.

There are other reasons why this model makes sense to me, if you were starting from scratch. People use dbt to create and update tables in their warehouse; for every model a dbt customer â€œbuys,â€ they are ostensibly getting somethingâ€”a new table, a fresher datasetâ€”that they want. Itâ€™s also an expense that customers can control relatively well. Reducing the number of models in use, turning down refresh cadences, and turning some materialized tables into views are easier and more precise adjustments to make than, say, optimizing query runtimes or reducing the number of events logged in a tool like Mixpanel.[^7]Â 

This change also discourages â€œbadâ€ behavior. One complaint that people sometimes have about dbt is that it creates an unwieldy mess of SQL dependencies; another is that it [piles unnecessary compute into metered warehouses](https://benn.substack.com/p/data-and-the-almighty-dollar#:~:text=And%20pile%20on,Run%20queries%20constantly.). Charging by model aligns customersâ€™ financial incentives with best practices for sustainable use.[^8]

Most tellingly, asking how dbt Labs would launch its pricing model today isnâ€™t an entirely speculative question. Though Dagster[^9] isnâ€™t an identical product to dbt, itâ€™s similar. It orchestrates ETL jobs. Itâ€™s open-core. It sells a cloud product. Itâ€™s built [in a lab](https://dagster.io/blog/introducing-dagster-labs). Free to define a pricing model without the weight of thousands of customers on top of it, Dagster launched one that [charges for seats and usage](https://dagster.io/pricing). If dbt was just building momentum too, Iâ€™d argue it should do the same.Â 

# No plan survives first contact with the enemy

Of course, all of this is somewhat academicâ€”dbt Labs isnâ€™t starting from scratch. While thereâ€™s not a clear general precedent for how a product like dbt should be priced, the market has a clear expectation for how *dbt specifically* should be priced. And that, rather obviously and [predictably](https://benn.substack.com/p/how-dbt-fails#footnote-4-77057156:~:text=Sensible%20as%20this,paying%20for%20it.), is the real tension here. dbt is valuable, people are accustomed to getting to use the open source parts for free and the cloud parts for relatively little money, and dbt Labs is not a charity.Â 

This is no doubt a problem, though I think itâ€™s worth recognizing exactly what the problem is: A social one, about pricing *expectations*. Itâ€™s the same problem with Friscoâ€™s novel rental schemeâ€”people are used to using and buying dbt Core and Cloud as they were, and [donâ€™t want to use or buy them as they will be](https://databased.pedramnavid.com/p/what-the-hell-is-going-on-with-data#:~:text=And%20so%20on.%20The%20knives%20were%20barely%20sharpened%20before%20they%20came%20out.).[^10] Thatâ€™s perfectly reasonable, but itâ€™s a different issue than dbt not being useful at all.Â 

That said, Iâ€™m not sure that itâ€™s an easier issue to resolve. If a product isnâ€™t useful, you can make it better, and people will enthusiastically buy it. If your product is already useful but people are used to getting it for free, trying to claim that value is a very delicate line to walk. Even if the final destination is the same in both casesâ€”people getting something they want at a cost thatâ€™s tolerable to both buyer and sellerâ€”one will earn you accolades for being a good, customer-focused business, and the other will get you accused of exploitation.[^11]Â 

So what do you do? Honestly, I donâ€™t know. But, in this light, dbtâ€™s decision to [permanently grandfather existing customers into the old pricing model](https://docs.getdbt.com/docs/cloud/billing#:~:text=LEGACY%20PRICING%20PLANS) could end up making some rough sense.[^12] One way to work around entrenched pricing expectations is to just charge people how theyâ€™re used to being charged. If you expect dbt Cloud to be sold by the seat, you pay by the seat; if youâ€™ve never bought it before and donâ€™t have any specific expectations for how youâ€™ll be chargedâ€”which you probably donâ€™t, since there arenâ€™t clear market precedentsâ€”you pay for usage. I mean, I dunno; itâ€™s clunky, but it could work.

The other, more elegant way out is probably to build more features, in both dbt Core and Cloud, that specifically appeal to buyers who never had expectations of using dbt for freeâ€”i.e., the ever-looming enterprise. If Twitterâ€™s excitement for the modern data stack and its various novelties arenâ€™t representative of the broader marketâ€”[and it probably isnâ€™t](https://benn.substack.com/i/53744915/microscopic)â€”then Twitterâ€™s willingness to glue together homegrown versions of dbt Cloud with [Airflow](https://twitter.com/oldjacket/status/1693990838505009497) and GitHub Actions probably isnâ€™t either. Enterprise buyers donâ€™t want headaches. They donâ€™t want rogue AWS services managed by decentralized teams. They want easy administration, SLAs, and professional services attached to their vendor contracts.Â And if they want dbt Core, that means they probably want dbt Cloud too. 

So do they want dbt Core? I have no idea. But I can certainly imagine there being things that could be added to dbt Core to make it more attractive to these buyers. And thatâ€™s ultimately the pointâ€”to keep growing, dbt Labs may not need to pile features into dbt Cloud; they may need to pile features into either Core or Cloud that appeal to people who wonâ€™t use one without the other.

If the id of a man is in his bar, the id of a company is in its customers. The people who pay for a product are the people who it gets built for. Thatâ€™s why every specialized analytics startup [becomes a BI company for the masses](https://benn.substack.com/p/microsoft-builds-the-bomb#footnote-4-123990165), why community projects become commercial platforms, and why [Clarence Thomas should be impeached](https://www.propublica.org/article/clarence-thomas-other-billionaires-sokol-huizenga-novelly-supreme-court). 

Iâ€™m sure that dbt Labs wants to build for its longtime fans, and wants to keep serving the customers who made it popular. But if those customers prove too stingyâ€”for rational economic reasons, for social reasons, for reason of chance or dbt Labsâ€™ own making, it doesn't matter whyâ€”their ability to keep using dbt will have to be subsidized by some other buyer. And dbtâ€™s id will inevitably come to reflect that.


---


[^1]: Officially, the â€œMission Margaritaâ„¢, brought to you by Rakuten.â€

[^2]: The extra ingredient behind Friscoâ€™s incredible popularity will be its [spicy attitude](https://youtu.be/7Fbxn7Tpk_I?t=33).

[^3]: Unless, I guess, people who got there early sell their drink rights on a secondary market.

[^4]: In San Francisco, â€œlate at nightâ€ is 9 p.m.

[^5]: When you order a Muni Martini, we tell you itâ€™ll be ready in three minutes; no, eight minutes; no, forty minutes; no, actually, itâ€™s been canceled and won't be coming tonight.

[^6]: Peach vodka, [Five Alive](https://en.wikipedia.org/wiki/Five_Alive), topped with mango White Claw. Served on the rocks. No substitutions.

[^7]: People sometimes say this is unfair, because it â€œpenalizes successâ€ or â€œdiscourages customers from using the product.â€ On one hand, this is true. On the other hand, what pricing model doesnâ€™t, in some way, discourage people from consuming a product?

[^8]: You could, I suppose, argue that, by charging per run, dbt Labs is encouraging itself to promote these sorts of behaviors. Thatâ€™s possible, but Iâ€™m not sure you can have it both ways? If one pricing model is bad because it monetizes peopleâ€™s carelessness, Iâ€™m not sure that another one can also be bad for *not* penalizing the same carelessness. Moreover, to return to the bar, if the goal is to protect people from alcoholism, it seems like we should be lobbying for bars to charge per drink and not per hour at the bar. Surely, economic incentives have a stronger effect on peopleâ€™s behavior than public service announcements to drink responsibly.

[^9]: Iâ€™m a [small investor](https://benn.substack.com/p/disclose-your-angel-investments) in Dagster.

[^10]: Case in point: One of the complaints Iâ€™ve seen about the new pricing model is that dbt shouldnâ€™t charge for runs because it doesnâ€™t cost them anything material to run them. But dbt Labsâ€™ margins on provisioning a new user are almost certainly higher. Nobody raised that objection for the last several years, though, because all of us expected to pay for users.

[^11]: dbt is far from the only example here. Elon Muskâ€™s effort to make people pay for Twitter was always going to be a disaster for largely this reason (the other reason being [Elon Musk](https://benn.substack.com/p/the-emperor-and-his-clothes)). And just a couple weeks ago, HashiCorp [sparked a riot](https://blog.gruntwork.io/the-future-of-terraform-must-be-open-ab0b9ba65bca) by trying to make more money from Terraform.

[^12]: Obviously, this particular decision didnâ€™t seem to be part of a master plan, but a response to how customers took the initial change.

================================================================================

# Take the down round

*The greatest trick that VCs ever pulled is convincing startups that down rounds are a disaster. Plus, pricing, again.*

---

![](https://substackcdn.com/image/fetch/$s_!Mf4A!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35faf9b6-346b-4c49-bfd9-f925622ff307_700x308.png)
*Hello from Keyser SÃ¶ze Venture Partners!*

*A brief programming note: *I started this blog a couple years ago mostly for my own entertainment. Though my day job at Mode gave me plenty of opportunities to think about data and technology, the work was necessarily pragmatic, about immediate customer needs and practical product choices. Inside of a startup, there's not much time for idle curiosities.Â 

This blog was a side project for exactly thatâ€”a hobby, for fun. There wasnâ€™t any motive beyond that, and there still isn't. For better or for worse, Iâ€™m here for the fun of it.

Itâ€™s still a good time, and I appreciate all of you who've stopped by. But over the last several months, the structureâ€”one long rant, about data pipelines, or AI, or the same half-dozen companies, or whateverâ€”has started to feel tired. So, I might try out some new things. There might be less on tooling, and more on data in the real world. There might be the occasional longer essay, because the problem with 2,000 word posts about esoteric Silicon Valley minutiae is that theyâ€™re not 5,000 words. I might become a clichÃ©, and start a podcast. I might realize that [I only have one move](https://www.youtube.com/watch?v=lJNYxpEAh68), and keep regurgitating the same three points over and over again with more and more strained analogies. I might spend a few days in upstate New York, fall in love with the mountains and hills, become a farmer, a man of the land, with only his animals and till, and never write another word. I might finally write that piece about [Pitbull](https://benn.substack.com/p/a-brief-programming-note). 

Still, inevitably, every comedian tries to be [a dramatic actor](https://youtu.be/V9lPeprrGHg?si=U-D2VGb3bS9mBeFz). Every TikToker tries to be [a pop star](https://youtu.be/9ZnhZcHEL88?si=LWPMiywXixWF_1pB). And every niche blogger convinces themselves they can be Matt Levine. So this week, in a blatant stylistic and substantive ripoff of *Money Stuff*, thereâ€™s not one big topic; thereâ€™s a couple sections about data and money. Years from now, letâ€™s see if [I regret writing it](https://en.m.wikipedia.org/wiki/Jam_(Turn_It_Up)).[^1]Â 

# Up with the down round

For all that Silicon Valley tries to [celebrate failing](https://nymag.com/intelligencer/2014/03/silicon-valley-failure-fetish.html), it sure punishes struggling.Â 

For a startup, the ultimate sign of struggling is the dreaded [down round](https://carta.com/blog/how-to-handle-a-down-round/#what-is). A down round, for those who are fortunate enough to have never been acquainted with one, is when a startup raises money that values the company less than it had been valued in a previous round.[^2]Â 

Suppose, for example, that a company raises $4 million by selling 20 percent of itself to venture capitalists, valuing the entire company at $20 million. It then spends the money on employeesâ€™ salaries and marketing programs and [Nvidia GPUs](https://www.theinformation.com/articles/nvidia-accelerates-ai-startup-investments-nears-deal-with-cloud-provider-lambda-labs).[^3] If it isnâ€™t profitable by the time that money is gone, the startup will need to raise more money. So it goes to venture capitalists and asks for another round of funding.Â 

The expectation is to raise that money at a higher valuation in whatâ€™s called an up roundâ€”by, say, selling another 20 percent of the company for $8 million, implying that the company is now worth $40 million. But for a variety of reasonsâ€”the business is struggling, a change in market conditions, the founder will be [in jail for the next eleven years](https://www.npr.org/2023/05/30/1178728092/elizabeth-holmes-prison-sentence-theranos-fraud-silicon-valley)â€”the company may not find any investor willing to buy shares at that price, or at any price above $40 million. In that case, the startup, which needs money to continue operating, may have to sell 50 percent of itself to raise $8 million, which would value the entire company at $16 million.Â 

Thatâ€™s a down round. And in the zeitgeist of Silicon Valley, theyâ€™re a disaster. Theyâ€™re [humbling comedowns](https://www.wsj.com/articles/klarna-to-raise-fresh-cash-at-slashed-6-5-billion-valuation-11656692324?st=lnnfmm2405hnycg); big [falls from grace](https://news.crunchbase.com/venture/unicorn-board-june-2022-klarna-stripe-instacart/); [catastrophic events](https://www.growthmentor.com/glossary/down-round/#:~:text=In%20fact%2C%20down%20rounds%20are%20considered%20to%20be%20so%20bad%20for%20business%20that%20Motley%20Fool%20Money%20host%20Chris%20Hill%20went%20on%20record%20to%20say%2C%20%E2%80%9CA%20down%20round%20is%20such%20a%20catastrophic%20sign%2C%20it%E2%80%99s%20the%20worst%20possible%20thing%20that%20can%20happen%20outside%20of%20a%20tragic%20accident%20of%20some%20sort.%E2%80%9D%C2%A0) that are â€œthe worst possible thing that can happen outside of a tragic accident of some sort.â€Â 

For early stage companies, this pessimism makes sense. If two founders and an idea are worth $20 million to some angel investors, itâ€™s a pretty bad sign if those same two founders and that same idea, plus eighteen months of work and four million dollars spent, is only worth $16 million.Â 

As startups get bigger and more mature, however, their valuations are more and more affected by public marketsâ€”markets that, quite reasonably, move up and down over cycles that last months and years. For companies caught raising [across these cycles](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5371e2a1-4fbe-4402-a3e7-27c1d414ef60_1024x547.png), the stigma around down rounds look a lot less like a reflection of an actual disaster, and a lot more like a VC-created boogeyman.Â 

â€”

There are two ways for a startup to raise a funding round that values it at one billion dollars:

The first path is â€œnormalâ€ and â€œcorrect.â€ Itâ€™s the path that venture-backed startups are both supposed to take, and supposed to want to take. The second path is everything thatâ€™s wrong with Silicon Valleyâ€”a company gets caught up in some frothy hype cycle, raises money it doesnâ€™t deserve at a price it canâ€™t justify, and crashes back to earth when the price comes down.Â 

But in both cases, the company is in the same spotâ€”worth $1 billion, with $100 million in revenue and some new money in the back. So why all the catastrophizing over down rounds? I think there are three reasonsâ€”one thatâ€™s legitimate but should be minor; one thatâ€™s misleading; and one that, cynically, explains whatâ€™s really going on.Â 

The legitimate reason is that down rounds harm employees. When an employee joins a startup, theyâ€™re typically granted stock options that they can purchase at roughly the same price that the last investor paid for them. Employees that joined the company between between two fundraising rounds would have their options priced based on the first round, and then valuedâ€”on paper, at leastâ€”based on the second round. A lot of people join startups because of the potential value of their equity, and that potential is a lot harder to get excited about when you have to pay a dollar to buy a share thatâ€™s currently worth seventy cents.Â 

That said, this can get at least partially corrected. [Companies can reprice or exchange options](https://darrowwealthmanagement.com/blog/down-round-employee-stock-options/) so that employees can purchase theirs at the new price. Though these arenâ€™t complete solutions, they can mitigate a lot of the impact.Â Moreover, employees are a lot more affected by layoffs than down rounds. Layoffs are often seen as healthy corrections, whereas down rounds are â€œthe worst possible thing,â€ which betrays the argument that the cultural shame in down rounds is primarily about their damaging effect on employees. 

The second reason that weâ€™re warned about down rounds is, I think, a kind of logical sleight of hand. When people talk about down rounds being punitive and dilutive, they tend to talk about them relative to the prior round or a hypothetical up round. If a company raised a round at a $1.5 billion valuation, raising the next round a $1 billion valuation probably dilutes existing shareholders more than the first round, and way more than an up round would. 

But that strikes me as the wrong comparison. Excluding the occasional [criminally delusional startup](https://www.youtube.com/watch?v=UREIAoL0Spk), most companies are going to do the best they can to be successful, regardless of how much money they raise. A startupâ€™s growth trajectory, from its founding to the point of standing on a precipice of taking a down round, likely wouldnâ€™t have been *higher* had it taken less money.[^4] One way or another, it was going to reach this point, and be about to raise at roughly this valuation. The question to ask, then, is not if a down round is dilutive relative to some hypothetical $2 billion valuation that was never attainable; the question to ask is if how much more dilutive is it to have passed through a $1.5 billion valuation on the way to a $1 billion valuation than it is to have passed through a $500 million valuation? 

The exact answer to that depends on a lot of complex and variable terms in how that intermediate round was raised, but the general answer is, it doesnâ€™t really matter *that* much. Across a number of different permutationsâ€”if the first round was small or large; if it valued the company at under a billion dollars, at just over a billion dollars, or at a lot more than a billion dollars; if the round had structure[^5] or was cleanâ€”founders are roughly financially indifferent. What matters far more than raising a down round is what happens *after* the down round: The only scenarios in which down rounds are particularly punitive are those in which the startupâ€™s valuation continues to fall.[^6] And the risk of taking a bit more dilution might be a perfectly reasonable price to pay for the opportunity that a larger war chest of cash can provide.Â 

So whatâ€™s the real reason for the outsized fearmongering about down rounds? Cynically, Iâ€™d argue itâ€™s because down rounds are bad for VCs, and companies being scared of them is good for VCs.

â€”

On the surface, thatâ€™s just math. Raising rounds at high valuations is expensive for VCs; moreover, at a given valuation, ownership of a startup is zero sum. The more founders and employees own, the less investors own. A startup ecosystem that emphasizes discipline and encourages founders to be more conservative in how they raise helps VCs fund companies at lower prices than one that aggressively chases big valuations and rapid growth.

But I think thatâ€™s actually backwards. VCs donâ€™t make money by buying shares of slow growing startups at a discount; they make money by investing in really fast growing companies, at almost any price. An ecosystem full of cautious companies is no good; they want one full of companies that are [just gonna send it](https://www.youtube.com/watch?v=mzOUgwsQ_hM).Â 

Stigmatizing down rounds protects *that*. Itâ€™s the [side-scrolling left wall](https://www.reddit.com/r/Mario/comments/typ4d1/i_always_wonder_why_cant_you_scroll_to_the_left/) of Silicon Valley, always pushing startups forward to chase something bigger.Â 

The problem is that there are times when itâ€™s not obvious if a company should chase something bigger. Its product may be hitting its plateau; its immediate market may be tapped out, without a good one to turn to next. Unfortunately, in these moments, the incentives of VCs and startups arenâ€™t aligned. Because, for VCs, their portfolios are exactly thatâ€”portfolios. [Famously](https://www.fool.com/investing/general/2015/02/13/7-insights-for-investors-from-peter-thiels-zero-to.aspx), the â€œbiggest secret in venture capital is that the best investment in a successful fund equals or outperforms the entire rest of the fund combined.â€ Better to have every company swing for the fences, have most strike out and have one [hit a grand slam](https://twitter.com/BravesOnBally/status/1697440523525460445)[^7] than to have them all chasing singles.[^8]Â 

But founders and employees donâ€™t make money off of an index. Their prospects are tied to one at bat. For them, a single could change their lives, and a [strikeout could ruin it](https://youtu.be/8ZgfTarNxdY?si=Fm38vsJUg2cy4vFG&t=58). When theyâ€™re down in the count, they may be better off choking up and looking to make contact.

My point here is not that companies should carelessly chase hype, or that big bad VCs are conspiring against meek and righteous startups. Companies have a lot of say in these decisions, and the collective pressure to chase big outcomes is both a faceless cultural force, and exactly what makes Silicon Valley as a whole work. Instead, my point is that whatâ€™s good for the ecosystem may not be good for individual companies within it. The biggest problem with a down round is the optics of a down round. Those optics push companies to [accept punitive terms](https://www.theinformation.com/articles/startups-avoid-valuation-cuts-with-up-rounds-in-name-only) to avoid them, and to keep chasing goals that are drifting out of reach. The sooner we recognize that, the sooner we can soften the stigmaâ€”and the sooner companies can do whatâ€™s best for *them*, and take the down round.

# How much for that DAG compute engine in the window?

The drama of the week is pricing, [again](https://benn.substack.com/p/how-much-for-that-dag-in-the-window). This time, itâ€™s about Snowflake, and how much Instacart spends on it.Â 

To quickly recap, Instacart filed to go public last week; in their S-1,[^9] which is essentially a giant disclosure document about the goings-on of the business, they said they used to spend a [bunch of money on Snowflake](https://twitter.com/modestproposal1/status/1695177654822191184) and now they spend a lot less; people started asking what happened and what it meant; it was clearly bad for Snowflake, and Instacart, and Databricks; no, it was clearly good for Snowflake, and Instacart, and Databricks; Snowflake tried to explain [their side of the story](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/); and now we all believe whatever we believed before this happened, but even more so. Gergely Orosz [has the story](https://twitter.com/GergelyOrosz/status/1697192807801184561).

I have no idea what to make of it, but it doesnâ€™t strike me as a terribly useful signal either way. Instacart is a business that is uniquely data-dependent: They have to optimize a three-sided marketplace, solve 263 million [traveling salesmen](https://en.wikipedia.org/wiki/Travelling_salesman_problem) problems a year, and manage a [$740 million ads business](https://twitter.com/austin_rief/status/1696691282830045676). I donâ€™t know how much that business should spend on Snowflake, but I know that their data needs are more unique than most. Theyâ€™re in [dataâ€™s one percent](https://www.youtube.com/watch?v=2rT7faFnpPM&t=62s); to me, the more interesting question is about whatâ€™s happening inside of the 99 percent.

That said, all of the recent fuss about pricingâ€”and in particular, usage-based pricingâ€”does make me wonder if weâ€™ll ever see a shift back towards [perpetual licensing](https://gocardless.com/en-us/guides/posts/what-is-a-perpetual-license/). Surely, someone will try to differentiate by coming out with a cloud product that promises full price transparency, with a single, lifetime fee of $5,000 per user. Surely, if thereâ€™s a product pendulum that constantly swings back and forth between [bundling and unbundling](https://stratechery.com/outline/bundling-and-unbundling/), there must be a pricing and packaging pendulum too; surely, someone will go back to shipping software [in a box](https://www.youtube.com/watch?v=_NeJ3Kg6OUo&t=219s).

If nothing else, I do think thereâ€™d be one advantage of buying over leasing: It encourages us to stop shopping, and invest in what we have.Â 

In keeping with the tradition of strained analogies, tools are like relationships. A lot could be good, but none are perfect. To make them really work, we have to both figure them out, and we have to adapt ourselves around them. If we work through that process, they can be what we dream of them to be. But if we never need to commit, or are afraid toâ€”[if weâ€™re scared of heights, scared of love; if we run away; if we put our past problems on the new thing](https://www.youtube.com/shorts/l1XwqTtLg78)[^10]â€”itâ€™ll always be insufficient, and weâ€™ll never find something thatâ€™s as good as what we could have if we [just made a decision and stuck to it](https://benn.substack.com/p/the-best-decision-is-one).


---


[^1]: There are 243 words [in this songâ€™s chorus](https://genius.com/Kim-kardashian-jam-turn-it-up-lyrics). Three of themâ€”night, night, and nightâ€”are more than four letters long.

[^2]: I have plenty of personal grudges about various things in Silicon Valley, but this isnâ€™t one of them. Modeâ€™s never took a down round, nor did we ever take some huge up round that made a down round hard to avoid. This was one of the rare benefits to raising money [in the middle of a pandemic-induced economic collapse](https://mode.com/blog/announcing-our-series-d/).

[^3]: Payroll $470kAWS $130kRent $40kNvidia GPUs $3.3mSoftware $60ksomeone who is good at the economy please help me budget this. my startup is dying

[^4]: No doubt, some people will say that less money makes you smarter, that constraints breed creativity, and so on. Which, yes, sure, but thatâ€™s primarily about efficiency, not absolute scaleâ€”and startups valuations are based more on the latter than the former. In other words, itâ€™s one thing to say that less is more efficient; itâ€™s another to say that less is outright more.

[^5]: â€œStructureâ€ refers to terms in fundraising agreements that arenâ€™t just about the amount of money raised and at what valuation. Typically, these are provisions for investors that grant them additional rights, like the ability to participate in future fundraising rounds, veto power over major company decisions, and guarantees that theyâ€™ll get paid back for their investments before anyone else makes any money. Some fundraising agreements include terms that explicitly protect investors from down rounds; these are called [anti-dilution provisions](https://ledgy.com/blog/anti-dilution-provisions).

[^6]: The table shows how much of a startupâ€™s founders and employees would in different fundraising scenarios. In each case, the company raised two rounds of funding, and the second round was always the sameâ€”$100 million at a $1 billion valuation. I also assumed that the first round had a 1x liquidation preference, which [is fairly typical](https://learn.angellist.com/articles/liquidation-preference#:~:text=A%201x%20liquidation%20preference%20is,liquidation%20preference%2C%20is%20less%20common.), and [broad-based weighted-average anti-dilution protection](https://www.cooleygo.com/glossary/broad-based-weighted-average-anti-dilution-protection/), which is less common.I then looked at how exit payouts for founders and employees would be affected by the size of the first round. The first row shows a scenario for which the second round is an up round; the second and third rows show scenarios for which it would be a down round. As the table shows, the only case in which the down round is particularly costly is the one in which the company raised a lot of money *and* its exit valuation is less than $1 billion. In other cases, the down round doesnâ€™t have much effectâ€”and can actually be beneficial.(Itâ€™s also worth noting that this is a severe oversimplification of [how all this actually works](https://www.cooleygo.com/down-round-financings/), but the point generally holds for more complex cases.)

[^7]: [Live video](https://twitter.com/JomboyMedia/status/1697449222696567192) of a venture capitalist watching said grand slam.

[^8]: Also of note: For founders and employees, it doesnâ€™t matter if equity prices fluctuate between when they join a company and when it exits. VCs, however, have to regularly report on the value of their portfolios, and have to report losses after a down round. That creates an incentive for them [to keep intermediate valuations high](https://techcrunch.com/2015/08/06/how-venture-capital-incentives-promote-zombie-companies/) that founders and employees donâ€™t have.

[^9]: Ok, but why are we talking about Instacartâ€™s Snowflake spend and not about its legal name being [Maplebear, Inc.](https://www.sec.gov/Archives/edgar/data/1579091/000119312523221345/d55348ds1.htm)?

[^10]: [Itâ€™s out](https://benn.substack.com/p/the-smol-analyst#footnote-7-136193385) and [itâ€™s a  banger](https://www.youtube.com/watch?v=jNBzgENcBI8).

================================================================================

# Category collapse

*There's no signal in ten million dollars anymore. *

---

![](https://substackcdn.com/image/fetch/$s_!fuCn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F983359d2-74e1-4830-a206-c242a862c43e_800x417.jpeg)
*[Ozymandias](https://www.atlasobscura.com/places/ozymandias-plains)*

â€œThe data industry is going to consolidateâ€ is a [pretty boring prediction](https://benn.substack.com/p/data-and-the-almighty-dollar#:~:text=A%20few%20dominant%20players%20colonize%20most%20of%20the%20landscape%2C%20and%20small%20companies%E2%80%94and%20potentially%20entire%20categories%E2%80%94get%20squeezed%C2%A0out.) to make these days.[^1] Whatâ€™s a lot less boringâ€”or at least, a lot less talked aboutâ€”is *how* itâ€™s going to consolidate.

There are options. Most of the startups in the space could get bought by some mega-vendorâ€”sewn into [Microsoftâ€™s Fabric](https://benn.substack.com/p/microsoft-builds-the-bomb), rebranded as something inspiring like AWS Pipeline Watch, or have their CEO become a [temporary](https://techcrunch.com/2021/03/23/tableau-ceo-adam-selipsky-is-returning-to-aws-to-replace-andy-jassy-as-ceo/) [Salesforce](https://techcrunch.com/2022/11/30/bret-taylor-steps-down-as-co-chair-and-ceo-of-salesforce/) [executive](https://techcrunch.com/2022/12/05/report-slack-ceo-stewart-butterfield-stepping-down-in-january/). They could evaporate, and leave behind nothing more than landing page with farewell post, and a $25 million hole on some VCâ€™s balance sheet. Or, they could get absorbed by one another, not only reducing the number of logos on [Matt Turckâ€™s dizzying pointillism](https://mattturck.com/landscape/mad2023.pdf), but also collapsing the number of *categories*.Â 

The last case is interesting, because there are lots of arrangements that could make sense. The preamble here is well known: The pandemic blew up the economy; interest rates got cut to zero;[^2] startup valuations went [haywire](https://www.meritechcapital.com/benchmarking/historical-trading-data/ev-ntm-revenue). Buyers stopped being so anxious about buying cloud data infrastructure; the modern data stack became the [New New Thing](https://www.amazon.com/New-Thing-Silicon-Valley-Story/dp/0393347818); hundreds of new tools chased the hype; the landscape split into somewhere between [30](https://www.moderndatastack.xyz/categories) and [95](https://mattturck.com/landscape/mad2023.pdf) different categories, with dozens of new startup claiming to be the industry leader in [whatever bit of whitespace](https://benn.substack.com/p/gerrymandering#:~:text=By%20doing%20the%20latter%2C%20we%20gerrymander%20our%20already%20complex%20landscape%20further%2C%20creating%20fractal%20after%20unmanageable%20fractal%20of%20unnecessary%20categories%20and%20subcategories.) they could triangulate out of the latest market map. The [Big Bang](https://www.cnn.com/2020/09/16/investing/snowflake-ipo/index.html) pushed the universe inexorably outwards; gaps opened up between existing products; [YC cranked out hundreds](https://benn.substack.com/i/40868831/the-data-mess) of new companies to fill them.Â 

And then, the [Big Crunch](https://en.wikipedia.org/wiki/Big_Crunch): Markets fell, interest rates went up, valuations collapsed, and the Fed [took away the punch bowl](https://www.nytimes.com/2007/12/23/business/23view.html?ex=1356066000&en=3337604c8708710a&ei=5090&partner=rssuserland&emc=rss#:~:text=to%20take%20away%20the%20punch%20bowl%20just%20as%20the%20party%20gets%20going) around the time the [party turned into a riot](https://www.nytimes.com/2021/05/24/style/adrians-kickback.html).[^3]

This correction,[^4] rather obviously, cooled everything off. Customers, looking to cut costs, bought less stuff. This hasnâ€™t just pushed down spending though; itâ€™s alsoâ€”predictably, as was predicted by everyoneâ€”categorically compressed the market. People stopped looking for specialized tools to solve every problem, and instead started trying to stretch the ones they have across several categories at once. Jacks of all trades may be masters of none, but they are also oftentimesâ€”like, when the Nasdaq is [down thirty percent](https://www.google.com/search?q=nasdaq+composite)â€”[better than masters of one](https://en.wikipedia.org/wiki/Jack_of_all_trades,_master_of_none#%22Full_quotation%22).

Big vendors, sensing customersâ€™ interests in consolidating around fewer tools, and sensing their own interest in making more money, have started to become more acquisitive. Databricks, the Lakehouse Platform, [bought MosaicML](https://www.databricks.com/blog/databricks-mosaicml), a generative AI company. dbt Labs, the makers of dbt, [bought Transform](https://www.getdbt.com/blog/dbt-acquisition-transform/), the makers of a semantic layer. ThoughtSpot, a BI tool, [bought Mode](https://mode.com/blog/mode-founders-note-thoughtspot-acquisition/), a different sort of BI tool. Teradata, a data warehouse provider, [bought Stemma](https://www.stemma.ai/blog-post/stemma-teradata), a data catalog. Alteryx, an older data prep tool, [bought Trifacta](https://www.alteryx.com/blog/trifacta-joins-the-alteryx-family), a newer data prep tool.Â 

Though each of these deals makes sense, they arenâ€™t necessarily obvious, nor do they all follow the same structural pattern. Trifacta and Alteryx were direct competitors. Mode and ThoughtSpot were in adjacent boxesâ€”ThoughtSpot as BI and Mode as a â€œ[data analyst platform](https://mattturck.com/landscape/mad2023.pdf)â€ or a â€œ[data workspace](https://a16z.com/emerging-architectures-for-modern-data-infrastructure/).â€ And Teradata and Stemma were on completely different parts of the map.Â 

If more deals are to comeâ€”and surely they are, as valuations are still well below where they were in 2021, and the [hype multiple](https://kellblog.com/2016/03/22/introducing-a-new-saas-metric-the-hype-factor/) that data companies got then has moved on to AIâ€”what might the map look like when it stabilizes? Which categories will still exist; which ones will get subsumed; which ones will vanish completely?[^5]Â 

Obviously, I have no idea. And predictions like theseâ€”about whoâ€™s breaking up and whoâ€™s getting togetherâ€”are probably [a bad idea, right?](https://www.youtube.com/watch?v=Dj9qJsJTsjQ)[^6] Whatever, itâ€™s fine.[^7]Â 

## The data warehouses march on

Amid all the [discussions about Snowflake and Databricks](https://benn.substack.com/i/136639841/how-much-for-that-dag-compute-engine-in-the-window) recently, one fact is far more important than the movements of their quarterly consumption trendsâ€”they are selling to an enormous market *that already exists*. [According to Expert Market Research](https://www.expertmarketresearch.com/reports/database-management-system-market),[^8] customers spent $60 billion on database management systems in 2022. Combined, Snowflake and Databricks have, one, less than ten percent of that market, and two, popular and easy-to-use products that are built on relatively modern cloud architectures. Pricing, packaging, branding, weird PR statements about customer retention ratesâ€”these affect their stock prices, which are built on expected growth rates and forecasted free cash flows. But they donâ€™t really affect the viability of the core business, or their CFOs eventual ability to [count to ten billion](https://twitter.com/akhenosiris/status/1699909056775782530). (Or, they start counting a lot more slowly at something like four, the stock market decides that itâ€™d much rather that they count faster, and Microsoftâ€”who has no problem counting to [198 billion](https://www.microsoft.com/investor/reports/ar22/index.html)â€”buys themselves a database, or Marc Benioff buys himself another temporary exec.)

That said, there are a couple interesting questions about the warehouse market. First, as every database piles on featuresâ€”[data catalogs](https://www.teradata.com/Press-Releases/2023/Teradata-Acquires-Stemma-Adding-AI-Technology-and-Talent) and [Lakehouse AIâ„¢](https://www.databricks.com/blog/lakehouse-ai) and [app marketplaces](https://www.snowflake.com/en/data-cloud/marketplace/) and [collaborative analytics workspaces](https://cloud.google.com/blog/products/data-analytics/announcing-bigquery-studio) and [100 new features and enhancements that are helping you get business outcomes](https://www.oracle.com/autonomous-database/)â€”will the pendulum ever swing back in the other direction, towards big bare-metal racks that just store numbers and run queries? And second, is there a meaningful market for specialized databases?

My money is on mostly no, to both.[^9] To me, the issue seems like an economic problem. Building warehouse technology is expensive; selling and supporting it is even more expensive. That means these companies will have to sell to enterprise buyers, and enterprise buyers want ecosystems. Plus, thereâ€™s so much money in warehousing and compute, any successful small player will immediately become a targetâ€”to acquire or copyâ€”for a bigger and better capitalized competitor.Â 

The one major exception to that is for warehouses that are built for application development, and sold to engineers. MongoDB pulled this off, and then someâ€”itâ€™s currently trading [at the highest premium](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15a59ec4-e557-41a9-a0ea-e76d94fd6fff_1175x535.png) of any SaaS business in the market.Â 

## Everything becomes BI*

Itâ€™s the first law of the data industry: [Everything with a chart will become a BI tool.](https://benn.substack.com/i/134863380/twenty-pages-of-context-in-a-ten-page-window)

> [Nobody believes it at first.](https://benn.substack.com/p/microsoft-builds-the-bomb#footnote-4-123990165) We tell ourselves that this time is different. Weâ€™re solving a different problem, for a different audience. We can make something complementary to BI, something narrower, lighter, more focused. We tell ourselves we're more principled than the others; we have more discipline; *we* won't cave and [build](https://twitter.com/barrald/status/1610739623835369472) [pie](https://twitter.com/bobbypinero/status/1679133838113488899) [charts](https://twitter.com/dorianj/status/1610741471568891905).Â 
> Maybe we're right, in theory; maybe it can, eventually, be done. But the market can stay irrational [longer than a startup can stay solvent](https://www.goodreads.com/quotes/603621-markets-can-remain-irrational-longer-than-you-can-remain-solvent). And our customers will see our charts and want more of them; they'll want reports, and alerts, and explorable dashboards that can be exported as a PDF. They'll want granular permissions, and to connect to an old version of SQL Server running on a [Dell Optiplex 3020](https://www.walmart.com/ip/Dell-Optiplex-3020-Desktop-Tower-Computer-Intel-Core-i3-8GB-RAM-500GB-HD-DVD-ROM-Windows-10-Black-Used/994517744?wmlspartner=wlpa&selectedSellerId=5787&&adid=22222222227994517744_5787_149080731017_18511067177&wl0=&wl1=g&wl2=c&wl3=665551684524&wl4=aud-430887228898:pla-1879215353022&wl5=1022762&wl6=&wl7=&wl8=&wl9=pla&wl10=113945645&wl11=online&wl12=994517744_5787&veh=sem&gclid=CjwKCAjw5MOlBhBTEiwAAJ8e1ku06SrSKQPKm4A1zuCvo0nJAIbtViM5fqcwKwbmv8XBPBCuulpk1BoCPyUQAvD_BwE&gclsrc=aw.ds) under Jeff's desk. They'll want exactly what weâ€™re selling, plus just this one feature, and they'll pay us $100,000 if we can build it.

I get the temptation to believe that tools for analysts can stand alone; I [really, really do](https://mode.com/blog/welcome-to-mode/). The trap in that belief, Iâ€™ve realized, is that there actually *are* enough buyers of these products to support a handful of large vendors. The problem, however, is that these tools canâ€™t be both big *and* separate enough from BI to be seen by buyers as being a distinct thing. As soon as specialized as â€œadvanced analytics platformsâ€ can command big annual contracts, customers start expecting that spend to be offset by reduced spend in BI. Then, no matter how differentiated the product is from BI, itâ€™ll get evaluated through that lens.

Put differently, software categories arenâ€™t defined by Gartner, or VCs, or marketing taglines; theyâ€™re defined by buyers. And if youâ€™re selling a product that charges non-trivial amounts of money so that people can answer questions with charts, buyers will see you as a BI tool, compare you to other BI tools, and will call you BI.Â 

My unrelenting belief is that every BI-adjacent product is likely bound for one of three destinations: Be subsumed into some bigger and unabashed BI platform; stall, and become a niche player serving a small market; or get acquired by a major SaaS business that wants to start selling better reporting.Â 

## * Unless it becomes an embedding reporting tool

The real competition to general BI products probably isnâ€™t a bunch of specialized analytics products; itâ€™s internal reporting tools that are built directly on top of systems of record. These productsâ€”Adobe, Anaplan, Atlassian, HubSpot, Klaviyo, Salesforce, SAP, ServiceNow, Shopify, Stripe, Workday, and othersâ€”already have critical business data; they can [query it live](https://benn.substack.com/p/the-hunt-for-the-holy-grail#:~:text=but%20I%E2%80%99d%20argue%20it%E2%80%99s%20more%20because%20the%20reports%20are%20always%20live); they donâ€™t need customers to buy ETL or warehousing tools first; and they can build the sorts of [canned templates](https://help.shopify.com/en/manual/reports-and-analytics/shopify-reports/report-types/default-reports) that have been BI vendorsâ€™ [white whale for years](https://benn.substack.com/p/when-are-templates-going-to-happen).Â 

True, these sorts of reporting services almostÂ certainly arenâ€™t robust enough to outright replace BI products, especially at larger companies. Theyâ€™re also limited to reporting on whatever domain they serve and whatever data they have.[^10] But, for these vendors, internal reporting tools are probably compelling enough to customers that theyâ€™re worth buildingâ€”or, more likely, buying on the cheap.

For the visualization tools sitting on a few million in ARR, this is as likely to be a landing spot as within another BI tool. There just arenâ€™t that many BI tools to do the acquiring; plus, BI companies would already have existing pieces of the technologies theyâ€™d be buying. Unless thereâ€™s a very precise fit, the integration efforts likely arenâ€™t worth the cost, nor is a few million dollars of revenue worth defending. These deals are much more likely to be acquihiresâ€”and therefore, priced as suchâ€”than product buys.

SaaS vendors, by contrast, may pay a slightly higher premium. They are less likely to have visualization and data infrastructure experts in house already; if theyâ€™re revamping their reporting products, they may be planning on building a team to do it already; theyâ€™ll also be less sensitive to mismatched technologies or team philosophies.Â 

Thereâ€™s also already some precedent here: [Atlassian bought Chartio](https://www.atlassian.com/blog/announcements/atlassian-acquires-chartio) in 2021. More are probably coming.

## ELT rebrands as â€œenterprise data integrationâ€

â€œSimpleâ€ ELT tools that copy data from a SaaS application into a database are not, in fact, simple. SaaS vendors provide data in ways that canâ€™t be directly copied into a database; APIs constantly change or go down. ELT vendorsâ€”i.e, Fivetran, weâ€™re mostly talking about Fivetranâ€”have to deal with these problems, across hundreds of integrations, for customers that expect zero mistakes.[^11] Thatâ€™s not easy, and is unlikely to be replaced by connectors built by bots or poorly maintained open-source ones.Â 

Still, Iâ€™m not convinced Fivetran will stay as is. Major SaaS vendors can [build their own connectors](https://benn.substack.com/i/73615268/the-problem-is-different); some [already are](https://www.snowflake.com/news/salesforce-and-snowflake-expand-partnership-with-real-time-data-sharing/). One of the dangers of Fivetranâ€™s usage-based pricing model is that these connectors can get cleanly cleaved off. If Fivetran charged a flat fee, customers would likely want to centralize all of their integration on Fivetran, to get more bang for their buck. But if theyâ€™re paying per sync, they have no economic incentive to use Fivetran over another connector thatâ€™s either cheaper or better.Â 

Moreover, major database vendors like Snowflake, Databricks, and BigQuery may want to cut out the middleman themselves. ETL and reverse ETL tools are both essentially ports that are charging import and export taxes on top of a warehouse. Surely, databases would rather collect those taxes themselvesâ€”or, even more likely, get rid of them entirely, because they make most of their money once the goods get on the mainland. And if buying the port to your warehouse means you own the port into all of your competitors, even better.Â Tax them and not you; there is no data stack [WTO](https://en.wikipedia.org/wiki/World_Trade_Organization). 

So how does that play out? Iâ€™ll make two guesses.

First, databases do end up buying their own ports. Probably not Fivetran, but one of the smaller players. (Not a prediction, but Hightouch recently announced a â€œ[strategic investment](https://mode.com/blog/mode-founders-note-thoughtspot-acquisition/#:~:text=strategic%20partnership)â€ from [Databricks Ventures](https://hightouch.com/blog/databricks-invests-in-hightouch). And thereâ€™s a reason these venture arms are [often led by the VP of Corporate Development](https://www.prnewswire.com/news-releases/databricks-ventures-invests-in-hightouch-301913866.html#:~:text=Andrew%20Ferguson%2C%20VP%20of%20Corporate%20Development%20and%20Ventures%20at%C2%A0Databricks).) And second, because of all of these factors, Fivetran will worry less about building an impossibly long list of connectors, and to instead focus on the ones that they can charge a lot of money forâ€”[like Oracle and SAP HANA](https://twitter.com/fivetran/status/1699840705198162074). Theyâ€™ll support fewer startups, and more systems integrators who are paid billions of dollars to figure out how to make IBM talk to NetSuite. And theyâ€™ll compete less with Stitch, and more with MuleSoft.Â 

## Die a hero or live long enough to become Alteryx

There are, I believe, four well-defined categories in the data world. The first three are reasonably well understood: Warehouses, for storing the numbers; BI platforms, for graphing the numbers; and integration tools, for going out and getting the numbers.Â 

The fourth category is basically â€œother.â€ Itâ€™s a giant pileup of somewhat related functions, like data cataloging, governance, lineage, observability, orchestration, transformation, and discovery.Â 

In a booming market, customers might be willing to buy a tool dedicated to each of these problems. In a down marketâ€”or within big companies, where procurement people like signing one big check and IT people like managing one big toolâ€”people buy, like, Alteryx.Â 

Ultimately, I think thatâ€™s where a lot of the various companies in these spaces are headed. The â€œdata managementâ€ space is a lot like BI, in that it does a hundred different things, and every enterprise buyer wants a different combination of twenty of them. There is no killer feature; there is no sharp disruptive wedge. Thatâ€™s why Alteryx makes [almost a billion dollars a year](https://investor.alteryx.com/news-and-events/press-releases/press-release-details/2023/Alteryx-Announces-Fourth-Quarter-and-Full-Year-2022-Financial-Results/default.aspx), despite seeming like an [easy target for startups](https://www.cascade.io/epilogue/taking-on-an-entrenched-incumbent). The maze of complexity is its defense. Trying to build a simplified Alteryx is how you lose to Alteryx.Â 

And so, the pressure for every company in one of these categories is to become a [katamari ball](https://en.wikipedia.org/wiki/Katamari_Damacy), rolling up related products into a similar web of enterprise services. You see this with Alation and its [Data Intelligence Platform](https://www.alation.com/self-guided-demo-overview/); you see this with Collibra and its [Data Intelligence Cloud](https://www.collibra.com/us/en). You see early versions of this taking shape in the growing product catalogs of companies like Atlan and Monte Carlo and Hightouch.Â 

My bet would be that that pattern continues. Standalone data catalogs, discovery tools, and observability applications will end up being products of the times. The individual categories will go away as they all get merged together, or as vendors in one of the other three categories build their own. (This is already happening with catalogs. [Tableau](https://www.tableau.com/products/add-ons/catalog) already has one; [Teradata](https://www.teradata.com/Press-Releases/2023/Teradata-Acquires-Stemma-Adding-AI-Technology-and-Talent) just bought one; [Databricks](https://www.databricks.com/product/unity-catalog) just built one; Snowflake surely now wants one.[^12]) And a handful of surviving variety shops become the ERP tools of the data worldâ€”nobody knows exactly what they do, but the big ones seem to make a lot of money.[^13] 

# Inevitably isnâ€™t what it once was

Ten years ago, [Jason Lemkin said](https://www.saastr.com/from-initial-traction-to-initial-scale-10m-in-arr-dont-get-killed-by-your-success/) that â€œinevitability in SaaS comes around $10m in ARR.â€ In the data world, I donâ€™t think thatâ€™s true anymore. The market is big enough, fragmented enough, and disparate enough that getting to $10 million simply doesnâ€™tâ€”or didnâ€™t, at leastâ€”have that much signal anymore. There are lots of data tools that can find $10 million worth of buyers that canâ€™t find $100 million.

Thereâ€™s also not much signal in being a â€œcategory leader.â€ Last yearâ€™s categories were potentially just as hyped as last yearâ€™s companies. Their kings may run kingdoms, or they may rule over [lone and level sands](https://www.poetryfoundation.org/poems/46565/ozymandias).

So what makes a data company inevitable? Honestly, itâ€™s probably the same as it ever was: Repeatable enterprise sales, solving problems in proven categories.


---


[^1]: [Things that are embarrassing:](https://www.tiktok.com/@morganbcohen/video/7275410637499452715) When somebody points out the most obvious thing, like, the data industry is going to consolidate, and everyoneâ€™s like, oh my goodness, so real, guys, Iâ€™ve been saying this the entire time, when itâ€™s like, obviously, *obviously*.

[^2]: Ibid.

[^3]: The last few years havenâ€™t been a bubble; theyâ€™ve been Adrianâ€™s Kickback. It started with some people who just wanted to hang out and have a good time; it got popular on the internet; a bunch of money showed up to hype the whole thing; and at some point, thousands of people were there, a lot of them werenâ€™t really sure why or what to do, so we all just lit a bunch of stuff (i.e., money) on fire.

[^4]: This euphemism has always been funny to me. Itâ€™s not a downturn; itâ€™s an opportunity for improvement.

[^5]: [Old habits die hard](https://benn.substack.com/p/take-the-down-round#:~:text=But%20over%20the%20last%20several%20months%2C%20the%20structure%E2%80%94one%20long%20rant%2C%20about%20data%20pipelines%2C%20or%20AI%2C%20or%20the%20same%20half%2Ddozen%20companies%2C%20or%20whatever%E2%80%94has%20started%20to%20feel%20tired.%20So%2C%20I%20might%20try%20out%20some%20new%20things.), apparently.

[^6]: Yes, I know that we compete, but can't two rivals reconnect?"I only see them as partners, " the biggest lie I ever saidOh, yes, I know that we compete, but can't two rivals reconnect?I only see them as partners, I just tripped and emailed their corp dev.

[^7]: Why are you reading this?!? [GUTS](https://twitter.com/oliviarodrigo/status/1699996203230888304) is out! Stop talking about data!!

[^8]: Sounds legit for sure.

[^9]: Well, sorta. Iâ€™m a [personal investor](https://benn.substack.com/p/disclose-your-angel-investments) in MotherDuck, which means my literal money is more on yes. [Strong opinions, weakly held](https://nav.al/feedback) is an ironically awkward investment strategy, I guess.

[^10]: Unless, of course, they go out and [get more](https://benn.substack.com/p/scoring-data-predictions#:~:text=Salesforce%20acquires%20Fivetran.).

[^11]: Which, to be clear, is the point. Data syncing doesnâ€™t really work if you canâ€™t trust the sync.

[^12]: I look forward to when the fight between Snowflake and Databricks enters its [Travis VanderZanden phase](https://techcrunch.com/2014/11/05/lyft-sues-travis-vanderzanden/), when we move past disputes about [performance specs](https://www.databricks.com/blog/2021/11/15/snowflake-claims-similar-price-performance-to-databricks-but-not-so-fast.html) and into disputes about execs stealing trade secrets. I donâ€™t understand TPC reports or court filings written in lawyerly legalese, but one has big [tables of numbers](https://tpc.org/results/fdr/tpcds/databricks~tpcds~100000~databricks_SQL_8.3~fdr~2021-11-02~v01.pdf) and the other has screenshots of text messages, and one of those things is way more fun than the other.

[^13]: Forced to say, Iâ€™d probably guess that this is kinda what happens to an independent dbt Labs? They layer on governance and security services, more orchestration features, and a variety of enterprise-y things that compete, in effect, with, like, Alteryx.

================================================================================

# The end of our purple era

*We love to talk about the future of our tools. What about the future of our jobs?*

---

![Olivia Rodrigo's â€œbad idea right?â€: Stream Her New Single](https://substackcdn.com/image/fetch/$s_!9vYl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69ca7484-23b8-4248-ac66-c6ea34d4f4db_1031x580.jpeg)

*I want it to be, like, messy.*[â€œbrutalâ€](https://genius.com/23078978) / *SOUR* / Olivia Rodrigo

â€”

One of the defining articles of the data industryâ€™s last half decade was Anna Fillipova's piece declaring that [data people are purple people](https://www.getdbt.com/blog/we-the-purple-people), who act as the critical translators between the â€œredâ€ business teams and â€œblueâ€ engineers:[^1]

> The business people, the actuaries, know what data they need and can define requirements, but typically donâ€™t have the skill set to design a data architecture that gives them the data they need. Technology people typically donâ€™t understand the business requirements, but they can design the data architectures. Itâ€™s like the people in IT speak blue, the people in business speak red, but we need people who speak purple in order to create an appropriate solution.

This, Anna argues, is the role of analytics engineersâ€”to â€œbuild bridges between disparate entities across the business, connecting technology and people in new and productive ways.â€Â 

Her post, published in mid-2021 as the fervor around the modern data stack was [arcing steeply upwards](https://techcrunch.com/2021/08/31/databricks-raises-1-6b-at-38b-valuation-as-it-blasts-past-600m-arr/), [spoke for the generation](https://www.nbcnews.com/think/opinion/new-olivia-rodrigo-album-sour-stakes-her-claim-being-voice-ncna1268211) of data practitioners who came of age in the last several years. Engineers did the bare-metal work of collecting data and keeping things like Kubernetes running. Marketers and operations teams and executives brought us actual business problems. And we, the purple people, turned the former into solutions and insights for the latter. Purple became the unofficial color of a [movement](https://benn.substack.com/p/the-new-philosophers#footnote-3-113305279:~:text=The%20modern%20data%20stack%20was%20the%20reactionary%20counter%2Dmovement%20to%20these%20problems.)â€”of dbt, of the modern data stack, and of the jobs that analysts, analytics engineers, and business analysts had.Â 

It was [messy, creative work](https://benn.substack.com/p/analytics-is-a-mess), we said, but that was the point. We were at our best when we weren't just building dashboards and mechanically tracking metrics; we were at our best when we were given vague problems, well-sourced data, and the time and tools to [go exploring](https://wrongbutuseful.substack.com/p/analysts-are-explorers).

# The bed we made

*I got the things I wanted, it's just not what I imagined.*[â€œmaking the bedâ€](https://genius.com/Olivia-rodrigo-making-the-bed-lyrics) / *GUTS* / Olivia Rodrigo

â€”

A couple months ago, I attended an online conference on corporate accounting. During one of the sessions, the moderator asked a panel how they felt about the current state of accounting. The panelists all agreed that accounting was both important and, unfortunately, doomed to fail inside of most businesses. Most accounting initiatives stall before they deliver anything useful, they said, and today's cost-conscious executives don't have the patience for that.

Ah, hahaha, no, of course not, this is a lie. Nobody's ever said that about accounting.[^2] When a company needs accounting, they hire accountants who do accounting. Though some teams are probably better at it than others, so long as your books aren't [managed by a incompetent and criminal polycule running a Ponzi scheme](https://www.coindesk.com/business/2022/11/10/bankman-frieds-cabal-of-roommates-in-the-bahamas-ran-his-crypto-empire-and-dated-other-employees-have-lots-of-questions/), most companies' accounting initiatives don't "fail."Â Â 

The conference was, obviously, about data teams. And the panelists' pointâ€”that data projects are prone to failure, and donâ€™t often deliver meaningful insightsâ€”wasnâ€™t some bold new take; to the contrary, their point was that, despite all of the ostensible progress in the data industry over the decade, failure wasâ€”still, [just as it was in 2017](https://nothinghuman.substack.com/p/the-tyranny-of-the-marginal-user)â€”[more common than success](https://benn.substack.com/p/the-annual-microwave-conference#:~:text=so%20much%20so%20that%20the%20Harvard%20Business%20Review%20has%20a%20beat%20writer%20for%20failed%20data%20projects).

Six years ago, we [blamed our problems](https://www.gartner.com/smarterwithgartner/3-top-take-aways-from-the-gartner-chief-data-officer-survey) on â€œculture challenges,â€ and companiesâ€™ unwillingness to accept new ways of doing things. There were also plausible stories about the modern data analyst being a new role, and our tools being too immature. Our jobs were hard, we said, because the context in which we had to do them made them hard.

In the intervening years, weâ€™ve gotten all the things we wanted. There is little debate anymore about the importance of data: The worldâ€™s biggest corporations are [plowing billions](https://www.jpmorganchase.com/news-stories/tech-investment-could-disrupt-banking) into data initiatives; more recently, executives have become [obsessed with AI](https://www.reuters.com/technology/companies-double-down-ai-june-quarter-analyst-calls-2023-07-31/), which, while not exactly synonymous with data, is [awfully close](https://twitter.com/jaminball/status/1683988256076218369). Weâ€™ve been inundated with thousands of tools designed for every conceivable problem we might have.[^3] Though itâ€™s possible that weâ€™re one tool away from finally getting good at this, Iâ€™m not betting on it.[^4] And our rolesâ€”led by posts like Annaâ€™sâ€”are becoming clearer and more broadly accepted.Â 

And so, naturally, weâ€™ve come up with new obstacles that are getting in our way. Today, itâ€™s talent. [According to a Gartner survey](https://www.gartner.com/en/newsroom/press-releases/03-21-2023-gartner-survey-reveals-less-than-half-of-data-and-analytics-teams-effectively-provide-value-to-the-organization), â€œless than half of data and analytics leaders (44%) reported that their team is effective in providing value to their organization,â€ and â€œthe lack of available talent has quickly become a top impedimentâ€ to success. [Other surveys agree](https://www.consultancy.uk/news/35071/attracting-talent-is-biggest-challenge-of-data-leaders): â€œ60% of data leaders were finding it hard to recruit individuals with the necessary skills.â€

Ok, look. Itâ€™s 2023. If our issue is a skill shortage, itâ€™s not an issue. Itâ€™s an excuse.Â 

As early as 2019, Vicki Boykis said she was [overrun with candidates](https://vickiboykis.com/2019/02/13/data-science-is-different-now/): â€œIâ€™ve developed an intuition that the number of candidates per any given data science position, particularly at the entry level, has grown from 20 or so per slot, to 100 or more. I was talking to a friend recently who had to go through 500 resumes for a single opening.â€ Since then, vaguely scammy data science and business analytics masters programs have been [cranking out tens of thousands of new analysts](https://www.bloomberg.com/news/articles/2023-05-05/data-science-degrees-become-hot-programs-at-business-schools), who are often heavily-indebted and desperate to work.[^5] And over the last eighteen months, [hundreds of thousands](https://layoffs.fyi/) of tech workers have been laid off. There are droves of capable, smart, driven, and prepped candidates in the market looking for jobs. We wanted data to be popular; we wanted it to be attractive; [we got it, did it, itâ€™s done](https://genius.com/Olivia-rodrigo-making-the-bed-lyrics).Â 

But for a job to [be mainstream](https://www.gartner.com/smarterwithgartner/2017-the-year-that-data-and-analytics-go-mainstream), it has to be able to hire mainstream talent. So if we still think our problem is that the talent we have canâ€™t do the job we want them to do, the problem isnâ€™t them; the problem is the job.Â 

We have no problem recognizing that our tools need to mature, and speculating about their [eventual](https://synnada.medium.com/modern-data-stack-and-the-data-chasm-part-i-b1c4994c091b) [end state](https://roundup.getdbt.com/p/becoming-pangea). What happens if we extend this tradition to our jobs, and ask what theyâ€™ll look like when they grow up?[^6]Â 

# Die a hero or live long enough to become an accountant

*When am I gonna stop being great for my age and just start being good?When will it stop being cool to be quietly misunderstood?*[â€œteenage dreamâ€](https://genius.com/29483363) / *GUTS* / Olivia Rodrigo

â€”

Here's the thing about growing up: It usually makes us boring. We [mellow](https://www.youtube.com/watch?v=IZ3XMOdOdKM); we [become our parents](https://www.youtube.com/watch?v=eF_hF5_LZlg);[^7] we [become Alteryx](https://benn.substack.com/i/136852343/die-a-hero-or-live-long-enough-to-become-alteryx).Â 

And in our case, we become accountants.Â 

If a singular foundational belief motivates data teams today, it may well be this one: [Thereâ€™s gold in them thar hills](https://en.wiktionary.org/wiki/gold_in_them_thar_hills). Data is full of insights, and our job is to get them outâ€”either through careful analysis, through enabling others to explore it, or through automated systems that push those insights into operational systems that immediately take action on them. Much of what we do is built as a means to those ends.

That has a bunch of problems though. First, there may not actually be gold in a lot of those thar hills. Most of us [may just have](https://benn.substack.com/p/day-of-reckoning) â€œmoderately valuable datasets that can inspire moderate business improvements.â€ Second, finding meaning in data is very hard, as suggested by our continued insistence that most job candidates who are trained to do it actually canâ€™t. Third, even if we do find something interesting, itâ€™s [hard and expensive to make it useful](https://counting.substack.com/p/insights-do-not-equal-utility):

> While the research work needed to arrive at the insight might have been a lot of work, itâ€™s just the start. That work is vastly eclipsed by whatâ€™s needed to turn that seed of an idea into something useful and real. Think of the product vision, infrastructure, engineering, design, and marketing resources neededâ€¦How many hours of work does that equal?Â 

Finally, by chasing bespoke insights, we build bespoke systems. We design clever metrics that perfectly map to our businesses, measure our performance in ways that are meant to handle the nuances of how we do things differently from everyone else, and tell ourselves that thisâ€”thatÂ  â€œit dependsâ€â€”is the right way to do things.Â 

Perhaps it is not. Over the last few weeks, a slow soap opera has been playing out [on Twitter](https://twitter.com/modestproposal1/status/1695177654822191184) and [in financial beat reporting](https://www.cnbc.com/2023/09/11/instacart-clarifies-its-snowflake-payments-in-updated-ipo-prospectus.html) on how much Instacart is spending on Snowflake and Databricks. At its core, the debate is about metrics, and how exactly Snowflake calculates revenue retention. It's an [interesting detail](https://twitter.com/jaminball/status/1694469783197253634), if you're into these sorts of things, but itâ€™s not a conversation that any of the people involved [seem to be excited to be having](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/).

Though this is an uncommonly public example, this sort of dispute happens all the timeâ€”in [other S-1s](https://twitter.com/Kellblog/status/1697408989133443579), in boardrooms, and in weekly business meetings. We do things *our* way; someone else does it *their *way; people come to meetings [with different numbers](https://www.youtube.com/watch?v=N_T3wK2N9eY&t=93s); chaos ensues; people lose trust in the entire disciple.

Our typical solution to this problem is a mix of technical solutions (data contracts and semantic layers!), cultural changes (peer review and documentation!) and education ([donâ€™t abuse your SaaS metrics!](https://kellblog.com/2023/09/06/slides-from-saastr-2023-presentation-the-strategic-use-and-abuse-of-saas-metrics/)). These things are greatâ€”data contracts could work; most of my work is a disaster and definitely needs a editor reviewer; every Dave Kellogg talk is a talk worth watchingâ€”but Iâ€™m not sure theyâ€™re forceful enough. To create [true institutional trust](https://roundup.getdbt.com/i/125728052/personal-vs-institutional-credibility) around the work that data teams do, we might need to do what accountants do: Give our work some [rules](https://en.wikipedia.org/wiki/Generally_Accepted_Accounting_Principles_(United_States)).Â 

Levers Labs, led by Abhi Sivasailam, is [working on exactly this problem](https://www.youtube.com/watch?v=7EINkkJsNkI). Theyâ€™ve developed a set of [Standard Operating Metrics and Analytics](https://github.com/Levers-Labs/SOMA-B2B-SaaS), called SOMA, that is meant to provide universal metrics to measure companiesâ€™ operational performance, much in the same way that GAAP standardizes how we measure companiesâ€™ financial performance. Itâ€™s an ambitious projectâ€”not only does it contain [hundreds of metrics](https://github.com/Levers-Labs/SOMA-B2B-SaaS/tree/main/definitions/metrics) for B2B SaaS businesses alone, it also includes methods for modeling source data, semantic layer configurations, and templated dashboards and analyses. To me, these adornments arenâ€™t strictly necessary, and I hope they donâ€™t distract what increasingly is: A set of understood rulesâ€”not best practices, but expected standardsâ€”about how to do this job.Â 

If we never have that, I think weâ€™ll be lost for a long time. People will have to learn every job [on the job](https://davidsj.substack.com/p/the-human-interfaces-of-data-arynn#:~:text=There%20are%20some%20intangible%20parts%20that%20you%20can%20really%20only%20learn%20on%20the%20job.); trust in data will have to be built project by project, individual by individual. But with something like SOMA, we can lean on our collective wisdom.

# The end game

*You've been callin' my bluff on all my usual tricksSo here's the truth from my red lips*[â€œEnd Gameâ€](https://genius.com/13064409) / *reputation* / Taylor Swift

â€”

As FTX and Sam Bankman-Fried were imploding last fall, David Roberts explained where he thought effective altruismâ€”a rationalist philosophical and philanthropic movement that was closely associated with SBF, and imploding along with himâ€”[went wrong](https://twitter.com/drvolts/status/1594472238186926080):

> It follows that the bigger & more complex the systems you're reasoning about, and the farther out into the future your reasoning extends, the more likely you are to be wrong, & not just wrong, but wrong in ways that flatter your priors & identity. I always feel like this fundamental fact gets underplayed in discussions of [effective altruism] or various other "rationalist" communities. The tendency to bullshit oneself is basically ... undefeated. It gets everyone eventually, even the most self-disciplined of thinkers.Â 
> If we humans overcome this at all, it is not through individuals Reasoning Harder or learning lists of common logical fallacies or whatever. If we achieve reason at all (which is rarely), we do so *socially*, together, as communities of inquiry. We grope toward reason & truth together, knowing that no individual is free of various epistemic weaknesses, but perhaps together, reviewing one another's work, pressing & challenging one another, adhering to shared epistemic standards, we can stumble a little closer.Â 
> That's what science is, insofar at it works -- not some isolated genius thinking really hard, but a *structured community of inquiry* that collectively zigs & zags its way in the right direction. Any one of us will almost certainly succumb to self-BSing. Together? Sometimes not.Â 
> â€¦
> In other words, thanks to our epistemic limitations, a "dumb" heuristic that just says "when in doubt, be decent" will probably generate more long-term utility than a bunch of fancy math-like expected-value calculations. We want *resilient* ethics, not *optimized* ethics.Â 

If you replace "ethics" with "metrics," Roberts' point loses its moral imperative, but it becomes relevant to an indulgent blog post about the role of white-collar data professionals. We make progress as a discipline not by reasoning our way through our problems individuallyâ€”as tempting as that may be, given our rationalist proclivities for thinking from â€œfirst principles,â€ or whateverâ€”but through social inquiry, in which we can all stumble forward together. And resilient standards are better than theoretically perfect ones.

The potential of SOMA isn't in its precise framework of models and expressions. SOMAâ€™s promise is in encouragingâ€”compellingâ€”us to embrace a mindset of reuse over reinvention, and of using a shared standard as an ending point to fit our businesses to, rather than a starting point to adapt from.Â 

One of the [first posts on this blog](https://benn.substack.com/p/when-are-templates-going-to-happen) asked why nobodyâ€™s been able to build a successful library of analytical templates for common metrics and dashboards.[^8] My answer was that the similarities between businesses was a mirage: â€œFrom a distance, the differences between a 5 iron and a 9 iron are minusculeâ€”just a few degrees of club head tilt and a couple inches of length. But those differences are the very things that define each club as that club.â€ Companies, I said, are the sameâ€”to ignore their differences is to ignore their most important features.Â 

I now think that that mindset is the one we need to change. Itâ€™s this refusal to fit ourselves to an imperfect standard that keeps us from moving forward, and sows seeds of distrust in what we do. Weâ€™d go further as an industry if we accepted a standard and demanded that our businesses measure themselves against it. Counterintuitively, weâ€™d go further if we didnâ€™t think of ourselves as intrepid explorers searching for novelty, but embraced our role as accountantsâ€”reliable, pedantic, and, actually, [less boring than us](https://www.cnbc.com/2022/03/22/these-are-the-top-5-most-boring-jobs-according-to-researchers.html#:~:text=Top%205%20most%20boring%20jobs).Â 

If consolidation is the end game for our ecosystem, that feels like the end game for us. We should [borrow and steal from those who blazed the trail ahead of us](https://www.buzzfeednews.com/article/stephaniesoteriou/olivia-rodrigo-vampire-about-taylor-swift-rumored-fallout), become more like other business people, the actuaries, and [rebrand ourselves from purple to red](https://www.tiktok.com/@nicky.reardon/video/7276618256138374446).

# Picking losers

If you start a company that gets big, you will get rich. If you start a company that gets very big, you will get very rich and they might [make a movie about you](https://www.youtube.com/watch?v=lB95KLmpLR4).[^9] Because of these movies and people like Bill Gates and Elon Musk and [Jeff Bezos](https://www.youtube.com/watch?v=lI5w2QwdYik), most people think that founding a company is the best way to get rich in Silicon Valley.

It is not. The best way to get rich is to become an executive at a big company that's on its way to becoming a very big company. These companies will pay their executives handsome salaries and grant them large stock packages. Though these executives will not make as much as a founder, they take on much less risk and do not have to work as long before their equity is liquid. And if you are an executive at a company that goes public or is bought in a big acquisition, you will have a gold star on your resume that will make other companies want to hire you for even more money.

This is a well-understood phenomenon in Silicon Valley, and people will try to exploit it. If you ladder together a few consecutive jumpsâ€”pick a winner, collect the payout from the IPO or acquisition, and, because companies will often hire you at a level above your current role, market that â€œsuccessâ€ into a bigger job at another winnerâ€”you can make a lot of money very quickly.Â 

Itâ€™s not a perfect plan though. Picking winners [is hard](https://www.reuters.com/business/wework-raises-going-concern-doubt-shares-tank-2023-08-08/). And employers also donâ€™t like hiring people who change jobs a lot. If you do this too much, the short stints at successful companies will be black marks, not gold stars.

A new company called [Prospect](https://www.joinprospect.com/about) is trying to solve the first problem. It â€œuses the same data VCs use to give you an independent third party projection of what your equity is likely to be worth,â€ and helps job seekers find companies worth betting on. As someone who [owes their career to a lucky break like this](https://benn.substack.com/p/to-my-parents#:~:text=Yammer%E2%80%99s%20acquisition%20made%20us%20credible%2C%20and%20made%20our%20friends%20rich.), this seems like a useful service. But for the shrewd mercenaries who want to compound these jumps together, Prospect doesnâ€™t solve the second problemâ€”employers still donâ€™t like job hoppers.Â 

There is another option? I was recently talking to a friend who took two consecutive jobs at two startups that both imploded shortly after she joined. In both cases, they fired her and paid her severance.Â 

Perhaps, then, looking for winners is a foolâ€™s game. Theyâ€™re hard to find, and their jobs are competitive. What I want is a service for finding companies that are teetering over the edge. Get hired easily at an inflated title because they are desperate; work stress free because, who cares, deck chairs and Titanics and all that; get laid off; collect severance; use my inflated title to get a bigger job at another time bomb. I may not make as much as I would if I picked winners, but I also would barely have to work, and nobody would question why I left Theranos after a six-month layover.[^10] Plus, if you pick a disaster thatâ€™s big enough, you can become a whistleblower and they [will still make a movie about you](https://www.newsweek.com/tyler-shultz-theranos-whistleblower-dropout-what-happened-where-he-now-1695905).


---


[^1]: This quote is from Anna, who borrows it from [Deloitteâ€™s Thomas Davenport](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/artificial-intelligence-purple-people.html), who heard it from a data engineer named Jim Wilson, who was recounting something that his boss Kimberly Holmes told him. Someone please keep this game of telephone going.

[^2]: Or maybe they have? I have no idea; Iâ€™ve never been to a conference on corporate accounting.

[^3]: Tired: [Buy or Build?](https://www.montecarlodata.com/blog-the-build-vs-buy-guide-for-your-modern-data-stack/)Wired: Buy, Build, or Become a design partner for a YC data company thatâ€™s trying to find product-market fit and bend their roadmap around your exact need so that you get a bespoke solution for free?

[^4]: Sure, and itâ€™s also possible that Iâ€™m the right tennis racket away from [making the U.S. Open](https://youtu.be/uSaCK8xKWtI?si=kRYo-XKGk5r7YW-m&t=77). Itâ€™s just not [that one](https://youtu.be/O7tyTnyAUEM?si=6z8k3k7hG73htE6u&t=1). Or [that one](https://youtu.be/O7tyTnyAUEM?si=IX1orWSQidtz0ON6&t=15). Or [that one](https://youtu.be/O7tyTnyAUEM?si=IX1orWSQidtz0ON6&t=22). (Also, Iâ€™m a casual tennis fan at best, but Iâ€™ve probably watched that Alcaraz-Zverev point fifty times. Those [last two Alcaraz shots](https://www.youtube.com/watch?v=uSaCK8xKWtI&t=77s)â€”the pitch of his yell, the crowd gasping on the first shot and starting to cheer before the second because they knew what was coming, Zverev reacting in almost the exact same way, and the realization after, frozen on the face of man in the crowd with the white shirt and gray hair, that in every other* *Alcaraz shot, heâ€™s been playing at eighty percent, being conservative, doing only what he needs to do to but not what he *can* doâ€”top ten low-key sports highlight Iâ€™ve ever seen.)

[^5]: [Professional mastersâ€™ and â€œcertificateâ€ programs](https://slate.com/business/2021/07/masters-degrees-debt-loans-worth-it.html), like those in business analytics and data science, are notorious for being [exploitative cash cows](https://www.econjobrumors.com/topic/are-all-data-science-masters-cash-cows).

[^6]: Erik Bernhardsson asked a similar question [a couple years ago](https://erikbern.com/2021/07/23/what-is-the-right-level-of-specialization.html).

[^7]: [And Republicans.](https://www.tiktok.com/@netflixisajoke/video/7276439079825345838)

[^8]: [Do you, uh, get deja vu, when this blog quotes Olivia Rodrigo?](https://benn.substack.com/p/when-are-templates-going-to-happen#footnote-3-37347490:~:text=Do%20you%20get%20deja%20vu%3F%20From%20trading%20dashboards%2C%20laughing%20about%20how%20small%20they%20look%20on%20you%3F)

[^9]: If that company is a fraud, they will definitely [make](https://www.youtube.com/watch?v=W7rlZLw9m10) a [movie](https://www.youtube.com/watch?v=UREIAoL0Spk) about [you](https://decrypt.co/118495/all-ftx-films-tv-series-production-right-now), and you might still [get very rich](https://www.forbes.com/profile/adam-neumann/?sh=478c8807474f), but you also might go [to](https://www.npr.org/2023/05/30/1178728092/elizabeth-holmes-prison-sentence-theranos-fraud-silicon-valley) [jail](https://www.nytimes.com/2023/09/14/technology/sam-bankman-fried-ftx-twitter-thread.html).

[^10]: Admittedly, anytime something goes horribly, comically wrong inside of a company like Theranos, FTX, or Twitter, part of me wishes that I worked there, for the stories.

================================================================================

# So you want to sell to the enterprise?

*They'll be here tomorrow. *

---

![undefined](https://substackcdn.com/image/fetch/$s_!u2qP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26646c99-005d-4f82-a475-5990b75f67d1_2560x1866.jpeg)
*[waiting for Geico](https://en.wikipedia.org/wiki/Waiting_for_Godot)*

Ask for ten pitch decks from early-stage B2B startups, and youâ€™ll get nine versions of this master plan:

Itâ€™s a story everyone loves. It feels pure, even nobleâ€”the company isnâ€™t some gauche and cynical sales apparatus, but on a mission to solve a problem and make people happy. It seems intellectually cleverâ€”rather than taking on entrenched and well-funded incumbents directly, the company will outwit them, via a clever maneuver [they definitely will not see coming](https://en.wikipedia.org/wiki/The_Innovator%27s_Dilemma). And it follows in the spiritual footsteps of Silicon Valleyâ€™s biggest iconsâ€”put great products above all else, like Steve Jobs; start with culturally elite customers, [like Mark Zuckerberg](https://en.m.wikipedia.org/wiki/History_of_Facebook); leverage niche success into mass market appeal, [like Elon Musk](https://www.tesla.com/en_gb/blog/secret-tesla-motors-master-plan-just-between-you-and-me).

Itâ€™s also a story that rarely goes as planned. Because for every startup that pulls it off, there are a hundred that do this instead:

They identify a huge addressable market, defined by some vague and squishy term like â€œ[collaborative work management](https://www.gartner.com/reviews/market/collaborative-work-management)â€ or â€œ[application platforms](https://www.gartner.com/reviews/market/application-platforms-reviews).â€ That market, Gartner says, will grow to over $40 billion in 2028, at twenty percent CAGR. Budgets for this category are growing; CIOs are prioritizing it; there are many other impressive statistics and figures.Â 

The company believes the best product will winâ€”[end users have the buying power now](https://en.wikipedia.org/wiki/Consumerization_of_information_technology), not IT teams. The startup eschews expensive sales efforts or traditional ad programs; those are for incumbents, not insurgents. Nearly all their hires are engineers, and only the founders will do sales. One of the foundersâ€™ former colleaguesâ€”â€œJamie, theyâ€™re a rock star, they can do anything, youâ€™ll seeâ€â€”joins as the only go-to-market hire. Jamie and the founders write blog posts, meet prospects, collect tweets of â€œcustomer love,â€ build community. Things are going well. They give the brand the same [authentic, original voice](https://benn.substack.com/p/we-dont-need-another-sql-chatbot#footnote-2-134863380) that every other millennial startup has.Â 

They start selling to small companies first, and mostly other startups. They work closely with these customersâ€”â€œdesign partners,â€ in their pitch decksâ€”to refine the core product experience. They listen carefully to feedback; share Loom videos; get on Zoom calls with customers early in the morning; fix bugs late at night. The CEO tweets screenshots of customers thanking them for it, with a caption that says that grateful emails like thisâ€”not the money or the status, but a note from an appreciative customerâ€”is why they started this company. Their seed investors proudly retweet it.

They fill their private alpha with friends and former coworkers. The alpha becomes a beta. They close their first ten customersâ€”more startups, mostlyâ€”through word-of-mouth referrals. They close their next fifty customersâ€”more startups, mostlyâ€”through a blog post that got to number two on Hacker News,[^1] through a newsletter sponsorship,[^2] and through â€œproduct led growth.â€ [They donâ€™t worry too much about revenue yet](https://benn.substack.com/p/startups-shouldnt-care-about-revenue).Â 

Eventuallyâ€”six months before a fundraise, roughlyâ€”they put an enterprise tier on their pricing page with a â€œcontact us!â€ button underneath it.[^3] The premium features in the enterprise tier are just single sign-on and priority support (which is a bit ironic because the first customers still text the CEO when thereâ€™s a bug). But the enterprise plan isnâ€™t meant to be sold; not really; not yet. Itâ€™s there so that they can build a pipeline, and learn what enterprise customers want. The startup is developing relationships; having conversations; becoming â€œenterprise ready.â€ Itâ€™s a process, they say; itâ€™ll take a few months, a few quarters, maybe, *maybe*, a year.

A handful of companies fill out the contact form. The company talks to them; they get their feedback. â€œDo you mind if I record this call? I want the whole team to be able to hear from you.â€ The prospects tell them they think theyâ€™ve built something really interesting, and theyâ€™d love to stay in touch as the product matures. The company adds each prospect to Salesforce as an opportunity, and adds a slide in their fundraising deck that says they have $1.3 million in the enterprise pipeline. None of these deals will ever close.

The founders go to several VC dinners that are meant to connect entrepreneurs with enterprise buyers. They meet people from Goldman Sachs, Netflix, and Walgreens; from Verizon; from Pepsi; from a [Comcast subsidiary](https://www.sec.gov/Archives/edgar/data/1166691/000119312512073905/d262998dex21.htm) that nobody knew was owned by Comcast. Someone from Walmart reminds people that Walmart thinks of themselves as a technology company. Capital One is there. Everyone seems interested in the startup; they ask for more information. The founders send them more information; nobody responds. The founders stop going to VC dinners.Â 

A champion at an early customer gets a new job at Monday.comâ€”not enterprise, exactly, but a public company. The startup decides to sell hard; they go all in on what Monday.com needs; they build the two features that blocked the sale; they close the deal. Itâ€™s smaller than they wanted, but still, six figures. They make a case study about Monday.com; they make an ad about the case study.

They talk to Target, and get told they wonâ€™t use AWS. They talk to Fannie Mae, and get told that they need more time. They talk to Foxâ€”and they buy. Well, not Fox, but a digital marketing division of Fox Interactive. No matter; this is the plan, the company says; landing and expanding.Â 

Every quarter, the startupâ€™s growing executive team tells the board that theyâ€™re a few months, a few quarters, maybe, *maybe*, a year from breaking through. The enterprise pipeline looks good, and Jamieâ€™s been crushing it, and we have an enterprise sales whoâ€™s been crushing it at Twilio lined up to join as soon as weâ€™re ready. We just need a couple more features. And Jamie, actually, eh, we love Jamie, but we think theyâ€™re more of a zero to one employee; do you know any *world class *marketing leaders? No, we havenâ€™t told Jamie yet, so please keep it confidential.

A year passes, and things are improving. Fox is growing. Autodesk is a customer; Capital One is back. But the goalposts keep moving. Disney was in the works for nine months, but the team just got reorged. The Philadelphia 76ers wanted to buy, but the champion left for a tech startup. JetBlue was a technical win, but their budgets got cut and theyâ€™re sticking with Microsoft, for now.

Another year passes. The product matures. Enterprise buyers stop talking about missing features; procurement teams stop dragging their feet. Security teams stop nuking deals at the finish line. Gartner put the company on their â€œCool Vendorâ€ list. Itâ€™s not the magic quadrant, but Gartner says that cool vendors often make the magic quadrantâ€”although, itâ€™s helpful to be a Gartner client first, so that the analysts can, uh, be fully informed about your technology. The startup says Gartner doesnâ€™t matter anymore, and pays for it anyway.Â 

The company attends a big industry conference as a sponsor. Their boothâ€”a â€œpavilion,â€ officially, included in the $150,000 platinum packageâ€”is busy all week. They rent a F1 simulator. They raffle off a PS5. Their biggest prospect wins it. A fearless SDR lands a demo with Home Depot. And not some tech playground subdivision run by an amorphous executive, like the VP of Digital Innovation and Blockchain, or the [Field CTO](https://www.linkedin.com/feed/update/urn:li:activity:7108081055538012160/)â€”no, itâ€™s the real Home Depot, with VPs who are responsible for making sure that Home Depot makes money. The startupâ€™s best account executive is randomly assigned the deal. The best account executive gives a pitch; the best account executive sticks the landing. Home Depot asks for more information; the VP of Sales sends it immediately after the call; Home Depot responds.Â 

They respond to say that they loved the product, and see its value. But, the current customer base looks like itâ€™s mostly small tech companies? Maybe nowâ€™s not the time, they say, at least not for their team. When we buy software, itâ€™s a three- to five-year investment. But theyâ€™d love to keep talking, they say. They CC the SVP of Digital Innovation and Blockchain.

# Waiting for Geico

Eventually, *eventually*, the dam will break. â€œCommunity-driven, product-led, bottoms-up, consumer-gradeâ€ startups can build enough product and brand and go-to-market machinations to become genuinely appealing to enterprise buyers. Slog through long enough, and Geico will actually show up, ready to be sold. But it takes much longer, happens much later, and is a much more fitful and distracting journey than almost every startup thinks that it is.

In the best cases, interminably flirting with the enterprise is psychological torture. It's hiking a trail that never turns downhill, or hoping for [the trip that keeps getting delayed](https://www.youtube.com/watch?v=PI4Mv8R0mE0).[^4] And in the worst cases, the wait is fatal. A lot of startups model "selling to the enterprise" in their financial plansâ€”they build budgets that assume their enterprise pipeline will start to close, and their average contracts will get bigger. They launch expensive account-based marketing programs to bring in Fortune 500 leads, and hire senior sales reps to close those deals. They assume their unit economics will get better once the accounts get bigger. And when the enterprise cavalry never shows up, the modelâ€”and sometimes, the businessâ€”goes sideways.

So what are startups to do? As best I can tell, most of them have two options.

The first is to assume that [enterprise buyers arenâ€™t coming](https://en.wikipedia.org/wiki/Waiting_for_Godot). Itâ€™s tempting to believe that a companyâ€™s first $10 million of business can come from one segment of buyerâ€”i.e., small businesses or other tech startups[^5]â€”and the next $90 million can come from anotherâ€”i.e., the enterprise. I donâ€™t think thatâ€™s realistic. [The buyers are too different](https://pmarchive.com/guide_to_startups_part5.html), and products and brands are [too hard to change](https://benn.substack.com/p/clear-eyes-full-hearts). Whatever segment powers a startup to $10 million in revenue needs to be able to power it to $100 million.Â 

If thereâ€™s a strategic flaw in most early stage startupâ€™s business plans, itâ€™s this. They assumeâ€”either explicitly, in their â€œmaster plan,â€ or implicitly, in which competitors theyâ€™re planning on replacingâ€”that, once they reach $100 million in revenue, theyâ€™ll be making significant revenue from small companies, large companies, and other startups. Instead, companies should ask themselves if they can get to $100 million with eighty percent of their revenue coming from one of the three.[^6] 

The second option for selling into the enterprise is to start there, on day one.[^7] These startups brand themselves as enterprise software providers in their first TechCrunch article. They donâ€™t source â€œdesign partnersâ€ from former coworkers and friendsâ€™ startups, or plan their launches around Product Hunt; they talk to Honeywell, and Starbucks, and American Express. They win their first customers by selling them, in effect, services instead of softwareâ€”â€œweâ€™ll sit alongside you and try to build exactly what you need.â€ And they donâ€™t try to sell to enterprises *despite* the messy technical and cultural problems[^8] that fester inside of them, but *because* of them.

On a long enough timeline, any company can sell a product to both small companies and big ones. But for most startups, that timeline is well beyond a horizon thatâ€™s worth planning for.Â 


---


[^1]: â€œWe wouldâ€™ve been number one, but OpenAI [put out a new blog post](https://hn.algolia.com/?dateRange=all&page=0&prefix=false&query=openai.com&sort=byPopularity&type=story) that day,â€ says Jamie.

[^2]: â€œIt was a community newsletter, so itâ€™s good for our â€˜No BSâ€™ brand,â€ says Jamie.

[^3]: â€œContact us!â€ had a click-through rate of 1.2 percent; â€œGet in touchâ€ was 0.5 percent; â€œRequest a demoâ€ was 0.8 percent; and â€œcontact us!â€ was 1.4 percent. â€œItâ€™s because Gen Z only uses [all uppercase](https://www.google.com/search?q=when+we+all+fall+asleep%2C+where+do+we+go%3F+track+list&sca_esv=567579423&rlz=1C5GCEM_enUS1043US1043&sxsrf=AM9HkKmXAHE25x35riKTjk3YsdoEYhQAeQ%3A1695383171361&ei=g34NZZfYFb_BhbIPlaqDqAg&ved=0ahUKEwjXpb-9kr6BAxW_YEEAHRXVAIUQ4dUDCBA&uact=5&oq=when+we+all+fall+asleep%2C+where+do+we+go%3F+track+list&gs_lp=Egxnd3Mtd2l6LXNlcnAiM3doZW4gd2UgYWxsIGZhbGwgYXNsZWVwLCB3aGVyZSBkbyB3ZSBnbz8gdHJhY2sgbGlzdDIIEAAYigUYkQIyBxAAGIAEGAoyCBAAGIoFGIYDMggQABiKBRiGA0j2FlDHCFjeFXAAeAKQAQCYAaoBoAGxCaoBAzUuNrgBA8gBAPgBAcICBBAAGEfCAgYQABgHGB7CAgUQABiABMICBhAAGAUYHsICBhAAGAgYHsICBhAAGBYYHuIDBBgAIEGIBgGQBgg&sclient=gws-wiz-serp) or [all lowercase](https://www.google.com/search?q=guts+track+list&sca_esv=567579423&rlz=1C5GCEM_enUS1043US1043&sxsrf=AM9HkKlcsIsqJDVq7sJ2sksPwgNGLYyrTg%3A1695383135898&ei=X34NZbG0NsGEhbIPoZ6IOA&ved=0ahUKEwjx38qskr6BAxVBQkEAHSEPAgcQ4dUDCBA&uact=5&oq=guts+track+list&gs_lp=Egxnd3Mtd2l6LXNlcnAiD2d1dHMgdHJhY2sgbGlzdDIHECMYigUYJzIEECMYJzINEAAYgAQYsQMYgwEYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIIEAAYFhgeGAoyCBAAGIoFGIYDSMIPUIkCWNQOcAN4AZABAJgBjwGgAbkHqgEDOS4yuAEDyAEA-AEBwgIKEAAYRxjWBBiwA8ICDBAAGIoFGLADGAoYQ8ICChAuGIoFGLADGEPCAg0QABiKBRixAxiDARhDwgILEAAYgAQYsQMYgwHCAgQQABgDwgIKEAAYgAQYFBiHAsICEBAAGIAEGBQYhwIYsQMYgwHCAgUQABiABOIDBBgAIEGIBgGQBgo&sclient=gws-wiz-serp&clie=1) now,â€ says Jamie.

[^4]: â€œSome priorities came up we have to handle. So weâ€™re going to have to put our trial on hold. You understand? Just for a couple weeksâ€¦maybe a little longer. Iâ€™ll call you next week, and weâ€™ll iron out the details.â€

[^5]: Nobody does it, but Iâ€™d argue that startups should treat other startups as a different segment than SMBs or mid-market buyers. Startups are weird in that they operate like small companies, have ambitions to be big companies, and plan like the world is ending tomorrow. A 100-person company that was ninety people ten years ago and plans on being 105 people ten years from now is very different from a 100-person startup that was zero people three years ago, plans on being 500 people three years from now, and may well not exist at all in ten years.

[^6]: Admittedly, Iâ€™m shooting from the hip on this one, based on anecdotes and hearsay. For all the VCs in the room that have access to this data, Iâ€™d be very interested in the actual numbers here.

[^7]: And probably, before day one. I have no hard evidence for this either, but Iâ€™d guess that most enterprise startups are founded by people who either worked at or sold to enterprises before.

[^8]: A while back, someone from a major financial company told me that his company is almost fifty years old, â€œwith data in several different data storage technologies and going back decades,â€ built on â€œbroken approaches to problem solving from 20 years ago.â€ When you modernize that, he said, itâ€™s â€œnot about the technology but modernizing how you approach solving problems.â€ A delightful user experience that gets a lot of votes on Product Hunt doesnâ€™t help much there.Â (Which, to be clear, doesnâ€™t mean you canâ€™t build a great business on a delightful user experience that gets a lot of votes on Product Hunt! Itâ€™s just not a business thatâ€™s likely to win this guy as a customer.)

================================================================================

# The potential gap

*There's a difference between what tools can do and what tools do do.*

---

![Limitless - Plugged In](https://substackcdn.com/image/fetch/$s_!MvdN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0088105a-66f4-4569-8fd4-abbd00ce4f65_1912x810.png)
*[Bradley Cooper, using 100 percent of Azure Data Factory.](https://www.youtube.com/watch?v=4TLppsfzQH8)*

When I worked at Microsoft, the Office team periodically put out internal â€œvisionâ€ videos that showed how customers could use the suite of products that we were building. They werenâ€™t raw demos, but glossy feature-length commercials that were stylistically similar to every [overfunded](https://sandwich.co/work/one-card/) [tech](https://www.youtube.com/watch?v=IKmmJElKrY8) [companyâ€™s](https://www.youtube.com/watch?v=mQI-I3qbCZ0) [launch](https://www.youtube.com/watch?v=GC5Gmkn92Bg) [video](https://www.youtube.com/watch?v=B6zVzWU95Sw): Full of sappy narratives, conspicuous diversity, and a small business selling a safely inoffensive productâ€”quirky socks, dog toys, colorful bicycles, environmentally-responsible bagsâ€”to customers who want to express themselves in safely inoffensive ways.[^1] A busy architect needed to make it to his daughterâ€™s soccer practice; with SharePoint and Publisher, he could review blueprints on the go. A doctor didnâ€™t want to miss her motherâ€™s birthday; with Outlook and [Lync](https://www.microsoft.com/en-in/microsoft-365/previous-versions/microsoft-lync-2013), she could still care for her patients from Mexico City. A sixth grader had to write a report on butterflies; with PowerPoint and Internet Explorer, she could make it beautiful. And in a [Clinkle](https://www.youtube.com/watch?v=utNYF3BqiCM)-worthy twist, the architect is married to the doctor, the sixth grader is their daughter, and they have an [Office Family plan](https://www.microsoft.com/en-in/microsoft-365/p/microsoft-365-family/cfq7ttc0k5dm?activetab=pivot:overviewtab) that keeps their crazy, modern lives connected. They were stories about love, humanity, always being home for dinner, and OneDrive from Microsoft 365.Â 

Of course, nobody is like this; it would be insane to be like this. No family has ever perfectly organized everything they do in Office. Our digital lives arenâ€™t beautifully choreographed dances through shared calendars, family Teams chats, collaborative OneNote grocery lists, and loving and supportive comments left in homework assignments. Our digital lives are chaos. The mother keeps her notes on her iPhone, and prefers using the desktop version of Excel. The daughter is copying her report from ChatGPT; she is [making the periods a bigger font](https://youtu.be/IPv7Q2v5ej0?si=LSHzEgSQivrjHkNq). The father is still trying to figure out how to log in to SharePoint. The dance isnâ€™t a ballet; itâ€™s a mosh pit.

But the fairy tale that Microsoft showed us was *possible*. It wasn't built around vaporwareâ€”every feature that the family used was generally available; everything that they did with it was well within the bounds of reasonable use. Any family with a license to the Office suite and a [proficiency in Microsoft Word](https://benn.substack.com/p/scoring-data-predictions#:~:text=It%20will%20skip%20directly%20from%20a%20buzzword%20to%20a%20throwaway%20bullet%20in%20the%20Skills%20section%20of%20a%20resume%2C%20alongside%20%E2%80%9Ccreative%20problem%20solver%2C%E2%80%9D%20%E2%80%9Cexcellent%20written%20and%20verbal%20communication%20skills%2C%E2%80%9D%20and%20%E2%80%9Cproficient%20in%20Microsoft%20Word.%E2%80%9D) *could* live the life we saw, if they were sufficiently invested in using Microsoft products, and in learning their way around the features in those products.

Put differently, in some rough sense, all of us are choosing the lives we have over those we saw in the commercials. Office has all the productivity features and collaborative capabilities we could ever need; in them, Microsoft is offering us water that could, Iâ€™m certain, make our lives more orderly and organized. But itâ€™s us who are choosing not to drink it.

# A new buffet

A couple weeks ago, I got a demo of [Y42](https://www.y42.com/). In their words, itâ€™s â€œthe complete analytics engineering platformâ€ that can â€œbuild, orchestrate and govern data pipelines end-to-end.â€ In my words, itâ€™s squarely in [the fourth, â€œotherâ€ category of data tooling](https://benn.substack.com/i/136852343/die-a-hero-or-live-long-enough-to-become-alteryx): Not a warehouse for storing data; not an import or export pipeline for moving data; not a BI tool for analyzing and sharing data; but a catch-all data management tool thatâ€™s meant to ensure that teams can reliably turn the messy data they collect into the clean, organized, and regularly refreshed data that they need.Â 

I had two reactions to the demo. The first was that Y42 is an impressively capable product. You can develop and test dbt models in a rich IDE; you can work across different environments and branches, and quickly deploy and roll back changes; you can create DAGs of data dependencies, from sources like Fivetran to BI tools and operations applications; you can manage and orchestrate the entire operation in a single interface, and manage it all in code. Hundreds of data teams struggle with [various versions of these problems](https://www.blef.fr/modern-data-stack-upgrade/?ref=data-news-newsletter#:~:text=When%20it%20comes%20to%20incrementally%20change%20a%20data%20platform%20this%20is%20a%20bit%20different%2C%20you%20need%20to%20find%20what%20is%20going%20wrong%20and%20what%20could%20be%20improved.%20Like); though Y42 isnâ€™t perfect, I suspect that a team with a well-managed, well-integrated, and well-maintained Y42 instance at its center can probably do its job more reliably than one without it.Â 

My second reaction was that none of that will matter. Though teams *could *solve lots of their problems with Y42, very few will. Most teams will never use it; those who do will probably only use a fraction of its features. Itâ€™ll be a dbt IDE, or an orchestration tool, or a place for tracking the lineage of data pipelines, but not â€œthe complete analytics engineering platformâ€ that it could be, like it was in the demo, or as it is inside of Y42 itself.Â 

This isnâ€™t a statement about Y42; itâ€™s a statement about us. The market has given us products like Y42 beforeâ€”not of its exact combination of features, but of some mix of orchestration management, governance configuration, lineage tracking, data development, observability, cataloging, and discovery. Even in the most successful cases, most customers use these tools in relatively basic ways. Alation, Alteryx, Astronomer, Atlan, AWS Glue, and Azure Data Factoryâ€”roughly 1/26th of the tools in the â€œotherâ€ categoryâ€”could all probably make similar claims about being underused relative to their potential. Y42, if I had to bet, is bound for the same fate.Â 

Which is kinda weird! If we pushed these tools to their limits, they could probably solve a lot of stubbornly persistent problems.[^2] Instead, itâ€™s as though weâ€™re hungry, go into a buffet, and never bother to walk past the appetizer station. We keep complaining about how we havenâ€™t had enough to eat; new entrepreneurs keep creating new buffets that reconfigure the stations; we keep only eating whatâ€™s by the door, and declaring the restaurant insufficient.Â 

We should probably figure out what thatâ€™s about? We invest billions in building new restaurants; we spend billions trying them out; we waste billions by being hungry all the time. Why? Why, just as we all choose to live our chaotic lives instead of the organized one that Office theoretically makes possible, do we continue to grind through problems that tools like Y42 can probably solve? Why do we only use [twenty percent](https://www.youtube.com/watch?v=4TLppsfzQH8) of our ([data toolingâ€™s](https://benn.substack.com/p/powder-keg#:~:text=a%20different%20territory%3A-,The%20brain,-%E2%80%94or%20operating%20system)) brain?

# â€˜Cause letâ€™s be honest, we kinda do sound the same

One answer is that our products have the features, but theyâ€™re poorly built. Theyâ€™re too convoluted to understand, and too clunky to use. Sure, platforms like [Oracle](https://www.oracle.com/cloud/) and [Microsoft Fabric](https://benn.substack.com/p/microsoft-builds-the-bomb) can do everything, but theyâ€™re hard to buy, set up, and maintain. The features donâ€™t matter as much as the experience of using them. If ChatGPT was only available on IE7, or if you had to be an [industry-leading distinguished engineer](https://www.linkedin.com/in/shwetha-bhandari-2830b45/) to post a video on TikTok, theyâ€™d both be woefully underused too.

So weâ€”i.e., people who talk about this stuff on Substackâ€”segment the market into the startup cinematic universe, where the products are delightful but incomplete, and the Microsoft cinematic universe (or Oracle, or whatever), where the products are cumbersome but comprehensive. We arenâ€™t so much choosing the chaotic life over the organized one; weâ€™re choosing the cloud over big box software; modern over legacy; affordable over expensive; usage-based, month-to-month subscriptions over perpetual licenses; buying on vibes over buying on RFPs.Â 

But that doesnâ€™t feel complete. It doesnâ€™t explain why we use so little of the modern data products that we buy.[^3] Nor does it explain why, when I mentioned Y42, most people's reaction was probably not to think, "oh wow, at last, something that solves this long-standing and painful problem!," but to roll their eyes, wonder if I was an investor shilling for my portfolio ([Iâ€™m not](https://benn.substack.com/p/disclose-your-angel-investments)), and think, â€œ[a different tool now, but thereâ€™s nothing new.](https://genius.com/22619941)â€Â 

That responseâ€”*my* response, of skepticism and [dÃ©jÃ  vu all over again](https://en.wikipedia.org/wiki/Deja_Vu_All_Over_Again)â€”suggests to me that weâ€™re collectively held back by self-reinforcing pessimism. Our problems have been around a long time; weâ€™ve been promised solutions for years; we no longer believe those pitches. Unless a new product hits us square in the face with an idea thatâ€™s dead simple to test or obviously revolutionary, we wonâ€™t believe its potential.[^4] But when that potential requires some investment to realizeâ€”when we have to neatly organize everything in Office, or learn all of Y42â€™s featuresâ€”we need that initial faith. Without it, we briefly kick the tires, assume weâ€™ll be disappointed again, and move on. And our problems become [Lindy](https://en.wikipedia.org/wiki/Lindy_effect): They persist because they have been persistent.Â 

# I believe that we will make commercials

I'm not sure any of this is right, and I'm even less sure of what to do about it if it is.[^5] It would be a tough trap for the industry, though, because what can startups like Y42 do to change my reaction to their demos? They can't force their customers to invest in making the most of their products. If anything, buyers are demanding the oppositeâ€”we want [low-commitment contracts](https://twitter.com/saranormous/status/1707030552023384113), to be safe from technical lock-in, and open platforms that are easy to upgrade to the next new thing. But all of these things are anathema to the investments that most data products need from their buyers to be successful.[^6]Â 

For individual vendors and for the market as a whole, it seems as though weâ€™re reaching a point where solving that riddleâ€”which is marketing and storytelling, basicallyâ€”is becoming more important than cranking out new features. Thatâ€™s not because the products are done, but because thereâ€™s no sense in building more product that wonâ€™t get used. And people wonâ€™t use products unless they [believe in them](https://www.youtube.com/watch?v=BBre0xNcr5c).

Time to make some commercials about [chairs](https://www.youtube.com/watch?v=SSzoDPptYNA), I guess.


---


[^1]: I want more startups to make videos of their products being used by, like, hedge funds and hardened executives at chemical manufacturing conglomerates. Because be honest, you donâ€™t want to sell your new [CPQ](https://www.techtarget.com/searchcustomerexperience/definition/CPQ-software-configure-price-quote-software) to a design studio in Oakland with eight employees and three office dogs; you want to sell it to the SVP of Corporate Sales Strategy at DuPont. Whereâ€™s the product video where the main character isnâ€™t a loving mother who left her job at a law firm to start a ceramics studio, but is [Jim Young from J.T. Marlin](https://youtu.be/S63kIH96Bi0?si=RzP5mRNVDLcKIswC)?

[^2]: Case in point: Vendors using their own tool. When Acme Data, Inc. uses the Acme Data Platform inside of Acme Data, Inc., they can usually get a lot more out of it than their customers. Iâ€™d argue that most of that comes from a commitment to use the tool, and a willingness to push how far it can go. In other words, internal deployments of tools are decent measures of toolsâ€™ limits, and those limits are often pretty far beyond where most customers go.

[^3]: I mean, citation needed, but it [feels](https://www.cc.com/video/wfrwar/the-colbert-report-truth-from-the-gut) right? (And itâ€™s a *potential* gap, in every sense of the word.) Itâ€™s certainly possible, however, that the entire premise here is wrong, that we actually redline most of our products, and theyâ€™re still insufficient. Weâ€™re pushing the limits of our data orchestrators, but we struggle [because they arenâ€™t declarative](https://benn.substack.com/p/down-with-the-dag). Weâ€™re hacking together development workflows on top of [dbt environments](https://docs.getdbt.com/docs/environments-in-dbt) and [zero copy clones](https://youtu.be/uGCpwoQOQzQ?si=KpNVWV2H2m_3LU7w), and Y42â€™s [virtual data builds](https://docs.y42.dev/docs/core-concepts/virtual-data-builds) are the solution weâ€™ve long needed.Â I doubt thatâ€™s the whole story, but what do I know?

[^4]: This seems like why AI-based features are an exception to this. They are often pretty easy to try out, and they appeared, before AI became a ubiquitous buzzword in everything, so otherworldly that they broke through our cynicism.

[^5]: This is a fun four-word sequence.

[^6]: [As Iâ€™ve said before](https://benn.substack.com/p/take-the-down-round#:~:text=If%20nothing%20else%2C%20I%20do%20think%20there%E2%80%99d%20be%20one%20advantage%20of%20buying%20over%20leasing%3A%20It%20encourages%20us%20to%20stop%20shopping%2C%20and%20invest%20in%20what%20we%20have.%C2%A0), I think having one foot out of your current technology stack is far more harmful than having two feet in an inferior one.
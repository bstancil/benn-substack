# Posts from 2022-Q1

This file contains 12 posts from 2022-Q1.

================================================================================

# Data's trillion dollar question mark 

*How a data warehouse could become a data platform—and an organizational brain.*

---

![](https://substackcdn.com/image/fetch/$s_!VSKC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe266f31c-f01c-4440-bd0d-83d59d0c118b_688x470.jpeg)

“Workday, Salesforce, Adobe—they’re going to be reimplemented as apps on top of the data layer.”

When Martin Casado, a partner at Andresseen Horowitz, was asked why he thought the data industry was a trillion dollar market and [what bets he’d make](https://www.youtube.com/watch?v=q1nERFM9brA&t=3454s) *[[updated link](https://www.youtube.com/watch?v=tPSvCq5846w&t=61s)]* about where it’s going, this was his response. The answer stuck with me over the last several months, for two reasons. First, why? Why do we need to rebuild Workday, and what, exactly, is better about the tool that replaces it? And second, how? The modern data stack is defined as everything between an architectural diagram[^1] and a state of mind,[^2] but nobody would characterize it as a coherent layer suitable for widespread application development. 

I came around on the first question pretty quickly. To borrow an analogy [I've used before](https://benn.substack.com/p/the-future-of-operational-analytics), we couldn’t have created Yelp by adding stars to listings in the yellow pages. Yelp isn’t valuable because it has reviews; it’s valuable because the entire product is built around them and the data they contain. Restaurants are organized by reviews; results are filterable by them; listings are enriched with details about what they say. It’s an entire experience, with the review at the center.

Similarly, a sidebar in Zendesk that shows a customer’s most recent activity makes Zendesk marginally more useful. But a help desk centered on that activity, one that cataloging tickets into queues based on user behaviors and prioritizes them according to their payment plan or recent sales conversations, could be many times more valuable. Embedding dashboards in Marketo is nice; a marketing automation tool that’s organized entirely around campaign performance data, from the top of the funnel, through product usage metrics, all the way to sales conversions and renewals, is revolutionary.[^3] 

Even if Casado’s prediction is a bit too bold, it’s easy to imagine a future full of smaller, more focused data apps.[^4] Surely, even if we don’t all leave Salesforce.com for getsalesforce.ai (YC S22), we’ll at least have dedicated analytical tools for managing remote workforces, for connecting online marketing campaigns with offline retail sales, and for reliably computing SaaS metrics.

Still, Casado’s vision is missing a key piece: The bridge between where we are today, and the platform that’s needed to make these apps possible. The modern data stack—and more specifically, the centralized data warehouse—is a good foundation; data apps are a cool idea. How do we plug the latter into the former? We [have the underpants](https://www.youtube.com/watch?v=a5ih_TQWqCA) and we want the profit, but what do we do in the middle?

# Reverse ETL, we barely knew ye

Our current answer, such as there is one, is reverse ETL. Reverse ETL tools pull data from warehouses, map it to records and fields in destination apps like Salesforce and Hubspot, and sync the two. In doing so, they do things like create new lead records in Marketo, or update user attributes in Intercom.

In today’s world, these tools work well and are valuable.[^5] If you use Salesforce, you need data in it—and wiring up your own pipelines that make sense of Salesforce’s tedious APIs is what nightmares are made of. So long as Salesforce is useful, reverse ETL products’ syncing function will be useful. But as platforms for backing data apps,[^6] they’re a fudge.

The problem isn’t the reverse ETL tool itself, but how its sources are defined. Most tools simply extract tables, via either a [SQL query](https://hightouch.com/docs/getting-started/create-your-first-sync) or a [pointer to a dbt model](https://docs.getcensus.com/models/native-dbt-integration) that creates a table. This is brittle, at the beginning, in the middle, and at the end. Schema changes can easily break the queries that load data. When creating multiple syncs, it’s easy to define the same concept—say, leads to be synced to Hubspot and Drift—in slightly different ways. And without stronger guarantees about how data will be exposed, you can’t build apps *on top of* reverse ETL tools; reverse ETL tools are mostly meant to integrate into existing products.

This is not fertile ground for a fledgling ecosystem. Reverse ETL tools, even [those built](https://airbyte.com/blog/airbyte-strategy-to-commoditize-all-data-integration#anchor_3) on open frameworks, have an incentive to integrate with products that are already serving a lot of customers. Without a clean platform to build on—without a way to *pull* data, rather than waiting on someone else to *push* it to you—emerging data apps have to integrate with the warehouse directly. And so they do: Sisu [connects directly](https://support.sisudata.com/hc/en-us/articles/360061318971-Sisu-101) to your warehouse; Narrator asks users to [write queries](https://docs.narrator.ai/docs/set-up-your-data-system) in the app to build event streams; [smaller, free apps](https://www.saasgrid.com/) require people to define the custom datasets that they depend on. This makes data apps [expensive for companies to build](https://twitter.com/jthandy/status/1442840153345695744) and expensive for customers to maintain, stressing the [already-questionable economic dynamics](https://benn.substack.com/p/data-and-the-almighty-dollar) of the modern data stack even further.

Piecemeal architectures like this are also a broken experience for customers. Business concepts like users, customers, transactions, and events are defined in a mix of dbt models, queries in reverse ETL tools, configurations in third-party apps like those above, and the application code of internal tools. This can be, as [was the case for metrics](https://benn.substack.com/p/metrics-layer), a mismatched mess. 

But metrics—and some recycled ideas from the early 1990s—[offer a solution](https://twitter.com/sarahcat21/status/1479149989360128001).

# An entity layer

The metrics layer is, at its core, a semantic abstraction. It provides a way for people (and applications) to access business concepts directly, without having to know how to translate those concepts into precise formulas.

This same principle applies to other forms of data. The metrics layer provides a standard API for interacting with a KPI; a generalized “entity layer” could provide the same thing for business objects like customers, products, and users; for activities like web events, downloads, and purchases; and for temporal snapshots like a daily accounting of customers’ subscription types. 

The structure of an “entity query” could be identical to a metric query; the only difference is the noun. Both expose properties that are specific to the type of object being requested; both render SQL queries that are directly readable by the warehouse; both push compute down to the warehouse; both can be used as standalone queries or mixed in with raw SQL. 

![](https://substackcdn.com/image/fetch/$s_!Youq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6f632cb3-2aa2-45d3-b96e-7407ec328d75_2048x1524.png)
*There are two types of people in the world: Those who admit to having no idea how to format a case statement, and liars. *

At first glance, this doesn't look much different than a table or view. Is the customer entity, for instance, equivalent to `dim_customers`, or the purchases entity the same as `fact_purchases`?

![](https://substackcdn.com/image/fetch/$s_!NinK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faf2b92ec-83c2-413a-b8a8-f56fd60b12c1_556x500.png)

Potentially—but even if entities are just tables, they’re tables with semantic meaning. To most people, `dim_customers` is an unremarkable set of rows and columns in a database, a node in a DAG, distinguishable from every other table only by how bold its name is in some data dictionary. It’s an object, with meaning tacked on. Entities are semantic objects first, and tables second. And just as metrics layers encourage teams to canonize one version of a metric, an entity layer could pressure teams to do the same with other semantic concepts.

Moreover, entities could be more complex than flat tables. A customer entity could be best represented through nested relationships, where each customer record contains a list of associated events and user actions.

Isn’t this, then, just a semantic layer with an SQL-like API?

![](https://substackcdn.com/image/fetch/$s_!CnZ9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85f33bee-1594-4f18-b4cc-ed479a30bac8_556x500.png)

It is. That’s *exactly* what it is. But today’s semantic layers aren’t wrong in concept; they’re just problematic in execution. They’re typically only accessible via the tool they’re embedded in, as LookML is in Looker, for example. They’re also often required in BI tools, which encourages teams to overextend them to model every element of the business. 

A better semantic layer would be universally accessible, interoperable with other means of querying the underlying data, and optional. This last point is particularly useful: It promotes discipline in choosing which objects are worthy of semantic definition, and which can exist as regular tables. Even within Looker, calls for this sort of architecture are coming [from inside the house](https://github.com/looker-open-source/malloy).

If you squint, Segment’s [Personas product](https://segment.com/product/personas/) offers a sketch of what this might look like. As it exists today, Personas is too embedded within Segment to be a general entity, but it points in an interesting direction—what if we had an more API like this, for people, for customers, for purchases, and for other core business concepts, that translated requests into queries that ran directly against the database? This is much more stable ground for app developers to build on than that of custom queries against a warehouse. 

# Four steps to the entity

Entity layers, I believe, could serve as the foundation for a [true OS](https://benn.substack.com/p/the-data-os) for the modern data stack. But for the sake of being [even more concrete](https://benn.substack.com/p/the-intergalatic-data-stack#:~:text=we%E2%80%99ve%20got%20a%20lot%20of%20details%20to%20fill%20in%20about%20how%20we%E2%80%99d%20build%20a%20decentralized%20BI%20layer%20on%20top%20of%20a%20data%20OS) about how we get there—and for [taking some wild swings](https://www.tiktok.com/@samgary3) to see what ends up on the fairway—here are four guesses about what else might change.

First, metrics layers will expose entities as well as metrics. These concepts are already [implicitly defined](https://blog.transform.co/metrics-framework/) in model-based metrics layers, and dbt’s [macro-based approach](https://coalesce.getdbt.com/talks/keynote-metric-system/) is likely flexible enough to support something similar. Entities, which are essentially an additional form of governance, fit naturally in this space. 

Second, reverse ETL tools will incorporate these concepts directly. Census, which integrates with [Looker](https://docs.getcensus.com/models/looker) and [Segment](https://docs.getcensus.com/segments/getting-started) today, is already gesturing in this direction; entities would just generalize this approach. This also frees up reverse ETL tools to focus on their core competencies—building fast, reliable pipelines to dozens of destinations—rather than worrying about how data is sourced for those pipelines. 

Third, an ecosystem of data applications begins to grow on top of entity APIs. While that likely starts with established companies like Sisu and Narrator, it spreads to—and more importantly, encourages—smaller projects like the [SaaSGrid](https://www.saasgrid.com/). 

Finally, entities eventually become writable. Entity APIs, whether or not they’re SQL-like queries or traditional rest APIs, provide ways for updating the data underneath them, enabling the apps that sit on top to be interactive *and* transactional.[^7] And the data warehouse finally evolves from being a dumping ground for organizational data into being its centralized operational brain, the production database for a company.

All it takes is an idea we had [thirty years ago](https://patents.google.com/patent/US5555403).


---


[^1]: That, at this rate, will turn into a [pointillism painting](http://46eybw2v1nh52oe80d3bi91u-wpengine.netdna-ssl.com/wp-content/uploads/2021/12/Data-and-AI-Landscape-2021-v3-small.jpg) by 2023. (*small*.jpg, lol)

[^2]: The modern data stack is just, like, your opinion, man.

[^3]: Is this exact idea revolutionary? I don’t know. But you get the idea.

[^4]: The term data app is [sometimes used](https://towardsdatascience.com/the-analytical-application-stack-eead8ce6b70) to describe interactive reports or notebooks with lots of filters and toggles. To me, these are blinged-out dashboards, not data apps. A data app is a product—a CRM, an applicant tracking system, a design tool, a task management app—for which data is central to the experience of using it.

[^5]: We use Census at Mode and are happy customers.

[^6]: Which, in fairness, they don’t really claim to be.

[^7]: Thereby, reversing reverse ETL.

================================================================================

# The metadata money corporation

*The only enduring standard is greed.*

---

![](https://substackcdn.com/image/fetch/$s_!qJ8_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9b6c32d4-2ae6-4dc5-8f02-76aef3c35a58_1200x800.png)

There are a few mandatory slides in every data startup’s fundraising deck. You need a two-by-two matrix, with your company's logo in the upper right, preferably larger and in more vibrant colors than your competitors’ logos. You need a slide with a single number, in enormous font and sourced from a paywalled Gartner report that you didn’t read but found a reference to in a competitor’s press release about their inclusion in it, claiming your total addressable market is worth tens or hundreds of billions of dollars. You need to extrapolate six months of paid trials, sold to friends and previous coworkers, into a five-year revenue projection that exceeds $100 million. You need at least one quote about how data scientists spend [80 percent](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) of their time cleaning data, or how we’ve created [90 percent](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/?sh=5847a1260ba9) of all the world’s data in the last two years. And you need to show how your solution will create a new standard for some messy data practice, finally consolidating disparate approaches into a unifying consensus. If you’re feeling particularly bold, you’ll call it a protocol. 

This last ambition—to create a new open standard—is the "network effects" of modern data companies. For a time, consumer startups (and some enterprise ones) sold their ideas to investors on the promise that growth begets more growth. As more people use the product, network effects make it more valuable, drawing more people to the product and making it more valuable still. 

This phenomenon is real and powerful—for many social apps like Facebook and Twitter, it’s both their growth engine and competitive moat. But it’s also rare. Most companies create very weak network effects,[^1] and attempts to build around them are often dangerous distractions from focusing on product and marketing fundamentals. Network effects often make for better funding pitches than actual flywheels.[^2] 

For data companies, there's a tempting parallel: The open standard. The data ecosystem is messy and fractured, and lots of tools solve similar problems in inconsistent ways. To bring order to this chaos, we’re constantly rolling out new standards to [unite the factions](https://xkcd.com/927/), the one ring to rule them all. Superficially, the reasoning makes sense: [Standardized markets](https://en.wikipedia.org/wiki/Intermodal_container) can be very efficient for everyone—and very profitable for whoever owns the standard. More subtly, much like network effects, selling the potential to be a standard lets companies spin stories about how their growth curves can go vertical.

To this end, we’ve built [various](https://snowplowanalytics.com/) [libraries](https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/) for logging data; there are [multiple](https://www.singer.io/) [standards](https://airbyte.com/) for data ingestion; we’ve created [lots](https://parquet.apache.org/) [of](https://en.wikipedia.org/wiki/JSON) [different](https://orc.apache.org/) [formats](https://csvchain.com/) for data storage; [transformation](https://www.getdbt.com/) [and](https://docs.looker.com/data-modeling/learning-lookml/what-is-lookml) [metric](https://transform.co/product/) [governance](https://github.com/looker-open-source/malloy) are defined in different paradigms; we’ve [open-sourced](https://mode.com/blog/open-sourcing-our-analysis/) [analytical](https://docs.looker.com/data-modeling/looker-blocks) [packages](https://docs.getdbt.com/docs/building-a-dbt-project/package-management); [orchestration](https://airflow.apache.org/) [and](https://dagster.io/) “[DataOps](https://meltano.com/blog/our-next-steps-for-building-the-dataops-os/)” are configured in different ways; there are [several](https://openlineage.io/) [competing](https://open-metadata.org/) [models](https://datahubproject.io/) for metadata management; and we have visualization frameworks [coming](https://vega.github.io/vega/) [out](https://www.chartjs.org/) [of](https://ggplot2.tidyverse.org/index.html) [our](https://www.highcharts.com/) [ears](https://d3js.org/).

Sometimes, the new product [blows up on Hacker News](https://news.ycombinator.com/item?id=4912076) and turns into a $3 billion exit. Other times, it gets a handful of stars on Github, and few signups for its community Slack, and fades into oblivion. But in nearly every case, the idea lives or dies on its own merits. And—as wouldn’t be the case if they truly created durable standards—past performance doesn’t guarantee future success. Stitch pioneered the idea of an open standard for ingestion; Airbyte, a two-year old startup, recently raised $150 million to replace it. Looker built a popular new way of modeling data with code; three months ago, one of Looker's founders [launched an overlapping idea](https://twitter.com/lloydtabb/status/1450538694130110464). Standards that are only standards until a better idea comes along aren’t really standards at all.

Instead, the story of most companies’ success is pretty dull: They built useful products and beloved brands, in markets with lots of willing buyers. The rest is fable and folklore, the conflation of social proof and a high NPS with something [much more mythical](https://www.nfx.com/post/70-percent-value-network-effects/).

# Squaring up

The data industry is running headlong into this reality. For years, as the space has been exploding outwards, companies have been able to operate in the breach. Silicon Valley, which calls competition either [scary](https://www.theverge.com/2020/5/26/21270421/slack-ceo-stewart-butterfield-microsoft-teams-competition) or [stupid](https://www.wsj.com/articles/peter-thiel-competition-is-for-losers-1410535536), taught us that this is the winning strategy: elbow out space for yourself; carve a niche that you can own; be a category creator; define a new standard. This tendency has been further encouraged by today’s community-defined modern data market, in which most companies are more comfortable building brands around open platforms than overtly hawking a corporate product for cash.

But the landscape is getting crowded, and its unincorporated territories are becoming too small to represent new categories in the eyes of the customer. Buyers don’t spend nearly as much time studying the distinctions between vendors as the vendors themselves do, and what can seem like category-defining differences from the inside are minor details to everyone else.

![](https://substackcdn.com/image/fetch/$s_!XuwV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F97fe2652-ac65-4074-a957-ea76942155be_990x666.png)
*[modern data stack](https://ballotpedia.org/Kansas%27_4th_Congressional_District#:~:text=Incumbent%20Ron%20Estes%20defeated%20Ron%20M.%20Estes%20in%20the%20Republican%20primary%20for%20U.S.%20House%20Kansas%20District%204%20on%20August%207%2C%202018.)*

In light of this, there's something refreshing—and foreboding—about the ongoing fight between [Census](https://blog.getcensus.com/reverse-etl-benchmark-series-pt-2-census-89x-faster-than-hightouch-for-marketing-syncs/) and [Hightouch](https://hightouch.io/blog/hightouch-vs-census/), and the [performance wars](https://blocksandfiles.com/2021/11/15/snowflake-rebuts-databricks-snowflake-performance-comparison/) between Snowflake and Databricks. In both cases, nobody’s making bones about where they stand: toe-to-toe, in direct competition. Rather than trying to out-maneuver one another with bank shots about [coopetition](https://en.wikipedia.org/wiki/Coopetition) and open-source standards, they’re squaring up. And ironically enough, everyone probably ends up winning: consumers get better products; companies get more focus.[^3] Because there are so few actual “standards” and therefore, so few actual winner-take-all markets, multiple vendors can—and in the case of all four of these companies, I believe, will—build very valuable businesses.

There’s a lesson to be learned in this. As much as we all want to be the [$3 trillion platform and marketplace](https://www.nytimes.com/2022/01/03/technology/apple-3-trillion-market-value.html), there’s plenty of money in the apps built on top. And for now, as others are jockeying for position in an unwinnable race to standardization, it’s easier money too, just as it was in the early days of the App Store. Eventually, though, the space will tighten, the gold rush will get crowded, and we’ll turn our picks and shovels on one other.

# Privatizing metadata

One area where this philosophy could be immediately applicable is in metadata management. Today’s data stack is quickly fracturing into smaller and more specialized pieces, and we need something that [binds it all together](https://benn.substack.com/p/the-modern-data-experience). 

There are a few projects and products that fill this space, [including](https://openlineage.io/) [open-source](https://datahubproject.io/) [frameworks](https://open-metadata.org/) that aspire to be the new “open standard for metadata.”  These platforms offer bold visions not just for glueing everything together, but also for establishing a canonical axle around which the entire stack spins. 

But, could the solution be…smaller? The stack isn’t missing another hub or aspirational standard; it’s missing plumbing. 

More specifically, speaking as a representative of a vendor, I’d love to surface information about what’s happening across the data stack directly in Mode.[^4] To people writing a query or looking at a dashboard, the statuses of Stitch pipelines, dbt jobs, Redshift queues, and Monte Carlo tests all provide important context. That context, however, is hidden in other tools, out of reach and out of mind. While Mode could integrate directly with these products, it’s impractical for us to build and maintain integrations with even a fraction of the tools in a landscape as expansive and dynamic as the data industry. Open standards could solve this problem, *if *everyone consolidated around that standard. A snowball’s chance, as they say. 

Two of the modern data stack’s biggest successes pioneered a better way forward. Segment, staring down the barrels of hundreds of third-party APIs, made no attempt to standardize them; they simply did the hard work for us, giving its customers a single API to write to all of them. Fivetran did the same in reverse, creating one tool to read from scores of disparate APIs.

The data stack could use a similar switchboard. Connect to all of the APIs of various data tools; present all of that information back through a single set of endpoints; let people turn connections on and off just as they can in Segment. To get information from every tool in the rest of the stack, vendors would only need to integrate with a single API, no new standards necessary. Just cold, hard product, sold for cold, hard cash. 

This sort of tool may not make for the most compelling investor pitch or community launch. There wouldn’t be many slides painting grand visions of virtuous growth cycles; the company’s addressable market, limited to “modern data stack vendors,” wouldn’t be the sort of number you put in 200-point font. (Although, if the data market is worth $1 trillion, scraping off one percent of one percent of that still gets you your path to $100 million.) 

But, slides are just stories. It’s what happens after you raise the money [when the real work starts](https://twitter.com/Ray_Sturm/status/1063885093859553280). And in the case of wiring together the modern data stack—and, I believe, in building the next generation of data tools—more than standards to be built, there’s [money to be made](https://www.youtube.com/watch?v=B_hyT7_Bx9o).

# Are entities standards?

Regular readers may be wondering…what? Didn’t you just propose some nonsense about [entity standardization](https://benn.substack.com/p/entity-layer) last week? Isn’t that a standard?

In a way, it is. Which, if nothing else, shows how strong the pull of standards is. Even for a hater like me, it’s hard not to be drawn to their potential. 

But, the entity layer proposal is a proposal for a standard architecture, not a standard library. For better or for worse, we’ve had more luck agreeing to the former—ELT over ETL, centralized data warehouses, in-database transformations, and so on—than to the latter. I think that’s because architectural standards rely much less on (always elusive) network effects. They can work in a vacuum: My life is made better by choosing the “right” data architecture regardless of what you choose, and defaults emerge because what’s good for you is probably good for me. By contrast, for libraries for logging events or data structures for modeling metadata to be valuable as standards, we both have to choose the same thing. And data people aren’t exactly known for being [agreeable](https://twitter.com/search?q=from%3A%40bennstancil%20fight&src=typed_query&f=live). 


---


[^1]: Some people claim that Zoom, for example, [grew because of its network effects](https://digital.hbs.edu/platform-digit/submission/zoom-prepared-for-a-pandemic/). Zoom is viral—my Zoom link exposes you to the product—but there are no meaningful network effects. My usage of Zoom doesn’t improve your experience on Zoom, nor does it degrade your experience on Google Meet.

[^2]: In recent years, the network effect moat fad has been replaced by the data moat fad. This claim says that growth is a virtuous cycle because more usage creates more data, which creates more accurate models and better in-product features (e.g., improved recommendations on Netflix), which creates more growth. This, too, is mostly fiction.

[^3]: And the blogging peanut gallery gets a show worthy of NBA Twitter. Databricks claims to set a [world record](https://databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html) in database performance; [Snowflake says](https://www.snowflake.com/blog/industry-benchmarks-and-competing-with-integrity/) it’s a “marketing stunt lacking integrity;” [Databricks calls](https://databricks.com/blog/2021/11/15/snowflake-claims-similar-price-performance-to-databricks-but-not-so-fast.html) the response “sour grapes.” Census launched under the banner of [operational analytics](https://blog.getcensus.com/what-is-operational-analytics/), and [now embraces](https://blog.getcensus.com/what-is-reverse-etl/) Reverse ETL (and started their own [performance wars](https://blog.getcensus.com/reverse-etl-benchmark-series-pt-2-census-89x-faster-than-hightouch-for-marketing-syncs/)); Hightouch [introduces themselves](https://hightouch.com/blog/hightouch-vs-census) as “the very first company to coin the term ‘Reverse ETL’” on a blog called the [Operational Analytics Blog](https://hightouch.com/blog); Census says the term was [popularized in 2019](https://blog.getcensus.com/announcing-our-series-a-from-sequoia/); Hightouch claims they [came up with it](https://www.forbes.com/sites/frederickdaso/2021/11/17/hightouch-raises-40m-at-a-450m-valuation-to-democratize-reverse-etl-for-all-business-teams/?sh=7dbec592560a) in December of 2020. Inject it all straight into my veins.

[^4]: Making Mode, I suppose, a [data app](https://benn.substack.com/p/entity-layer#footnote-4) for data people.

================================================================================

# How to play Wordle

*Using data from a quarter-million tweets to turn Wordle into a numbers game, and what it teaches us about the truth. *

---

![](https://substackcdn.com/image/fetch/$s_!XSBC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7aaa70e8-a27c-48ca-b896-c8c3d64916f7_806x334.png)

It all started [with a fumble](https://twitter.com/whstancil/status/1472249410583150592). On February 7, 2016, eight minutes into Super Bowl 50, Cam Newton, the face of the Panthers’ franchise and the dynamic future of a reinvented NFL, fumbled the ball on drop-back and [inexplicably balked at recovering it](https://www.youtube.com/watch?v=ytwAalH7vec). The Broncos, a team led by [aging Nationwide salesman Peyton Manning](https://www.youtube.com/watch?v=CYc0upTJg3M), picked it up, ran it back for a touchdown, and ran away with the Super Bowl title. 

Cam’s career fell apart with that fumble, and the rest of the world went with him. [Colin Kaepernick](https://www.washingtonpost.com/sports/2020/09/13/colin-kaepernick-eric-reid/) was blackballed; [Tom Brady](https://www.youtube.com/watch?v=4cxiaqAW0SA) kept winning. A [tyrant](https://www.theatlantic.com/ideas/archive/2018/10/the-cruelty-is-the-point/572104/) took charge. The [virus](https://www.nytimes.com/interactive/2021/us/covid-cases.html) got out; the [villains](https://www.npr.org/2021/11/19/1057288807/kyle-rittenhouse-acquitted-all-charges-verdict) got acquitted. The [deserts](https://en.wikipedia.org/wiki/2021_Texas_power_crisis) froze; the [oceans](https://www.nytimes.com/2021/07/03/world/americas/eye-fire-gulf-mexico.html) caught on fire; the [sky](https://www.newyorker.com/culture/video-dept/the-day-the-san-francisco-sky-turned-orange) turned orange. The [train](https://benn.substack.com/p/runaway-train) came off the tracks, and the [citizenry](https://en.wikipedia.org/wiki/2021_United_States_Capitol_attack) went to war. Even [Hank Aaron](https://www.espn.com/mlb/story/_/id/30759123/long-home-run-king-hank-aaron-dies-86) died. For six years, we’ve been marching through the valley of the shadow of death, and find nothing but plunging cliffs; we’re the unhappiest we’ve ever been, and the happiest we’ll ever be. 

But, every once in a while, a light breaks through the clouds and tows us out of our despair. It suspends us above the storm, reminds us of simpler days and better times, and holds us all in a warm rhapsody of relief, joy, and, usually, video games.  

Pokémon Go was one such levitation. I was living in San Francisco at the time, and for a few days following the game’s release, it consumed us. We all played obsessively. We celebrated our rare catches. The evening streets were full of roving packs of wide-eyed millennials, chasing virtual lightning bugs. Every bus rider was collecting Zubats. 

Though that euphoria didn’t last, a new version is back: Wordle.

True to form, our rapture from 2022 is isolated and online. We play Wordle alone, and—by the hundreds of thousands—post our scores to Twitter, in a simple array of green and yellow emojis. 

Are tweets like this annoying? Maybe, though no less annoying than the usual barrage of regurgitated memes that pass for jokes in Twitter’s [refinement culture](https://www.nytimes.com/2021/06/17/style/lindy.html) chop shop.[^1] Do some people hate it? Sure—and [some people hated](https://www.independent.co.uk/arts-entertainment/music/reviews/billie-eilish-album-review-when-we-all-fall-asleep-stream-tracklist-tour-a8843596.html) *When We All Fall Asleep, Where Do We Go?; *there will always be haters. But are these tweets *useful*? Absolutely. For those of us who are into Wordle,[^2] they provide a huge dataset for understanding how we play the game—and how we should play. 

As someone who’s more comfortable with numbers than words, off I went.

# When to panic

I collected about 250,000 games from the last ten days of tweets.[^3] The overwhelming conclusion that they tell us is this: You shouldn’t lose. More than 96 percent of games are wins, and more than half of them are solved in four or fewer guesses. This distribution is consistent with [other analyses](https://twitter.com/WordleStats) (though not with my 88 percent win rate, so I’m already skeptical of everything). 

![](https://substackcdn.com/image/fetch/$s_!zOE4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F98b85f10-0978-45a2-abde-51366234aa6e_1558x642.png)

This should calm our nerves when we first start a puzzle—[the odds are ever in our favor](https://www.youtube.com/watch?v=_s7qgNMqDJI). But how should we feel halfway through a puzzle? How should we feel, four guesses in, staring at a grid of grayed-out vowels, with only a handful of yellow squares to our credit, and nothing left but ten-point Scrabble letters to choose from? Should we still be calm *then*?

Given Wordle’s high solve rates, we’re likely still ok. As the chart below shows, after four guesses, people only lose 8 percent of the time. Even when a puzzle comes down to its last guess, loss rates are only 17 percent, nearly identical to your odds of hitting a six on a die in a single roll (or, if you prefer, of rolling a seven in craps).

![](https://substackcdn.com/image/fetch/$s_!3L0U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F62587054-a609-483b-81fd-6fbe34b8292c_1286x702.png)

Of course, this omits a very important detail: How much do we know? If we’re down to one guess, there’s a big difference between knowing four green letters, in which case people lose on their last guess only 7 percent of the time, and knowing one green letter, in which case people lose a third of the time. And if we haven’t gotten any greens and only have one guess left, we’re pretty much sunk. Ninety-three percent of those puzzles end in a loss—although, shoutout to [some](https://twitter.com/hibikisan/status/1481394871252361217) [very](https://twitter.com/huluwafr13/status/1481299305247948808) [impressive](https://twitter.com/_jeyraof/status/1481667254500880386) [saves](https://twitter.com/_yukirock/status/1481050336937013250).

![](https://substackcdn.com/image/fetch/$s_!E1Bq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F247063c1-659b-471d-80d5-a6aae6e0ab3c_1446x944.png)

The grid below when when exactly you should panic. First, tally up the number of known green letters (this is the distinct number of positions, out of the five, for which you know the letter). Then, count the total number of yellow squares (in this case, it’s not distinct positions or letters; it’s every yellow square). That entry in the table shows the average number of additional guesses that people typically take to solve the puzzle. 

![](https://substackcdn.com/image/fetch/$s_!0WH0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F335af086-0161-4b4a-835a-f9d4a2f65e10_1286x394.png)

For example, this was how I started yesterday’s puzzle. With one known green and two total yellows, I should expect to solve the puzzle in 2.5 guesses. With three remaining, no need to panic yet. 

![](https://substackcdn.com/image/fetch/$s_!RZ7v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5f8f77c3-b4de-4f80-8f31-bfb4d31d2355_674x406.png)

But a lot was riding on my next guess. If my fourth try scored me two yellows and no new greens—giving me one green and four total yellows—I’d be in trouble. I’d typically need 2.1 guesses to solve the puzzle at that point, and would only have two guesses left. Fortunately, my next word had two yellows and two *additional* greens, giving me a board with three known greens and four total yellows. Saved, for another day. 

# Hard mode or strategic mode—or both?

Yesterday’s game highlighted a long-standing[^4] question I’ve had about Wordle strategy: Was “ghoul” a good second guess? After the first guess, the only information I had was one green letter. I could chase the winning word by only guessing words that start with R, or I could play strategically as I did, banking that green letter and using the first position as a way to hunt for more letters. By guessing “ghoul,” I put five new letters in play; if I guessed something that started with an R, I would’ve only been able to test four new letters. 

This exact decision comes up frequently. The table below shows the distribution of results for opening guesses. 17.3 percent of the time—in 44,000 of the 250,000 puzzles sampled—people started their game with one green and no yellows. (This is the third most common opening, behind one yellow and no greens, and one of each.) 

![](https://substackcdn.com/image/fetch/$s_!o4sg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F836dae23-242a-4004-b65f-f5c048e4762c_1014x448.png)
*[one person](https://twitter.com/sansiniestro/status/1482762211307175941)*

These people all faced the same choice I did: Guess a word with the same green again, or not? 

Wordle, for its part, suggests that the former strategy of guessing the same green letter is more difficult. The game has a setting that requires you to play using that approach, and it’s called “hard mode.” As evidenced by my guess of “ghoul” yesterday, I agree with the game: Sticking with the green letter you know makes you less likely to win. 

The masses, however, disagree. Of the 44,000 puzzles that start with one green and no yellows, 86 percent kept using that same green letter—in effect, voluntarily playing on hard mode.[^5] 

![](https://substackcdn.com/image/fetch/$s_!DfSk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb830246c-c90a-46da-8038-42ef0088c880_1064x352.png)

More importantly—and disturbingly, for my Wordle history—this is the better strategy. People who play the same green lose less frequently, and take fewer guesses to win. Evidently, playing strategically is foolish, and playing on hard mode is easier.

For even more damning proof that the strategic approach is the wrong one, consider three opening scenarios. In all three cases, the player has the same amount of information after two guesses. They know one green letter, and up nine letters that they can exclude. All three scenarios *should* play out the same. If anything, Scenario 2 should slightly underperform the others, because this scenario can only exclude up to eight letters. 

![](https://substackcdn.com/image/fetch/$s_!qCbq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc15cfbbb-caf1-4e5d-8901-bc84a01ea40b_1600x278.png)

However, all three scenarios actually perform very differently. Games that follow Scenario 1 are more than twice as likely to end in a loss than those that follow Scenario 3. And the only difference between the two scenarios—the only reason the results would be different—is the revealed strategy of the player in Scenario 1. 

![](https://substackcdn.com/image/fetch/$s_!KSnr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F497562ba-4dde-4d77-b7a8-2e3f43bd1713_1584x834.png)

But…why? Why is the Scenario 1 strategy worse? Why is it worse than even Scenario 2, in which players necessarily have less information? Why is hard mode easier?

There are two answers, I think. First, while the hard mode approach makes it more difficult to come up with words to guess, it restricts you to a smaller corpus of possible words, which likely makes the game easier to solve. Moreover, I suspect that playing this way also nudges you toward words that are “structurally” similar. For example, if you know the second letter is a B, it’s probably more likely that the first letter is a vowel, and always putting a B second improves your chances of finding that vowel.

Second, the biggest benefit of playing strategically and following the approach in Scenario 1 is potentially finding an extra yellow square. But yellow squares just aren’t that valuable. As the Panic Grid shows, adding an extra yellow square to what you know reduces the number of expected guesses remaining by about a tenth of a guess, compared to a half-guess reduction for an extra green. Sacrificing the benefits of playing on hard mode are a steep price to pay for such a small improvement.

In other words, playing strategically might make sense for [Masterminds](https://en.wikipedia.org/wiki/Mastermind_(board_game)), in which each code is a random sequence of four values. But Wordle is Wordle, not Five-random-letters-dle. The letters aren’t independent. Knowing where one letter goes—and the implications that has on what possible letters could go in other places—is information that shouldn’t be ignored. 

The rest of the world, on the other hand—our spiraling catastrophe of isolation and insurrection, of forest fires and fourth waves, of Tom Brady *[still](https://www.espn.com/nfl/recap/_/gameId/401326630)*[ winning](https://www.espn.com/nfl/recap/_/gameId/401326630)—is a different story. For at least a few minutes a day, beam me out of here, Wordle. Ensconce me in your playful squares, and remind me that “happy” is a five-letter word.

# Nobody goes digging looking for dirt

![](https://substackcdn.com/image/fetch/$s_!8JO8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60bfd561-43ba-4a38-8e92-023e8f864e47_700x296.png)

As I was making my way through this data, there are hundreds of different paths I could’ve taken. Some of them were interesting; some boring. Some fit into neat narratives; some were convoluted and confusing. Some were easy to uncover; others required regex. 

As is the case with most analyses, I didn’t follow a predetermined route through these decisions. When one particular question proved too hard to answer, I asked a different one. If another result was dull, I discarded it and went looking for something better. And when something *was *compelling, I stopped and took note. 

The final product was a child of these choices. This doesn’t make the analysis wrong, but it makes it something other than objective. It doesn’t mean the conclusions aren’t true, but it means there are a lot of other true things that have been left unsaid. 

Andrew Gelman refers to this phenomenon of analysts looking for interesting results as the “[garden of forking paths](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf).” The forks are conscious and unconscious decisions we make that draw us towards conclusions of a particular flavor: Controversial, counter-intuitive, statistically significant, *worth mentioning.* They are the biases in the pegs of our analytical Plinko board, nudging our chip towards the most compelling bins at the bottom. And when we land somewhere mundane, we often don’t bother telling anyone we played. 

As analysts, we all do this, all the time. It’s not because we’re lying, cheating, or p-hacking, nor is it because we don’t want to tell the truth. We do it because we want to tell an *interesting and meaningful* truth. 

This, I think, is the hardest part about, as Tristan Handy asks us to consider, [saying true things](https://roundup.getdbt.com/p/saying-true-things-is-hard)—truth itself isn’t enough. We don’t do analysis to confirm the status quo. We don’t get promoted for figuring out what everyone already knew. Nobody goes digging looking for dirt. 

These are the subtle and unavoidable biases that I think Jillian Corkin is correct to highlight [in her response](https://roundup.getdbt.com/p/adopting-a-truth-seeking-stance) to Tristan’s question. No matter how truth-seeking we try to be, the ground will never be [truly level](https://www.youtube.com/watch?v=1M6C0f4L1d8). Even if our data is a perfect reflection of reality (which it isn’t) and even if we can act as fully disinterested parties ([which we can’t](https://benn.substack.com/p/tilt-and-tilted)), we still chase—or as readers, remember—the interesting stuff. 

One potential solution is to celebrate boring results. Just as academia has popularized [replication studies](https://en.wikipedia.org/wiki/Replication_crisis), data teams could encourage analysts to confirm things that we all already assumed. But that’s expensive, and no analyst wants to spend their days retracing someone else’s steps. 

The other solution is what Jillian suggests: Talk about it. Put your biases in the open. Acknowledge, if even only briefly, the paths untaken. Don’t let the data “speak for itself;” put the datas into words.


---


[^1]: Please take a moment to appreciate the irony that Paul Skallas, the internet’s foremost opponent to refinement culture, a theory that claims today’s society is derivative and unoriginal, was, in the end, [a blatant plagiarist](https://twitter.com/antoniogm/status/1463261889874464771).

[^2]: Into in, casually, not obsessed with it, no, it’s just fun, I’m fine, it’s fine, it’s normal, totally normal, totally fine, I’m definitely fine, what time is it, when is midnight?

[^3]: This is, it must be noted, both a biased and messy sample. Not every game gets posted on Twitter, and [not every post](https://twitter.com/TubaMasterTom/status/1481705573313830920) on Twitter is a real game. The first problem is unfortunately unavoidable; the second problem doesn’t appear to be common enough to distort the results.

[^4]: Yes, it’s been two weeks, but in 2020 2021 2022 time, so two weeks feels like eight months.

[^5]: Are they actually playing on hard mode? Probably not—only about [3 percent of people](https://twitter.com/WordleStats/status/1484211175030149127) typically play that way.

================================================================================

# Service pressure

*Data teams aren't service organizations, but we can learn a lot from those that are.*

---

![](https://substackcdn.com/image/fetch/$s_!Beym!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0900057-7e21-4dd7-be80-4e5d7d7bb2db_1280x720.jpeg)
*[A pressure like a drip, drip, drip, that’ll never stop](https://www.youtube.com/watch?v=tQwVKr8rCYw)*

Some of our opinions make sense: They’re well-reasoned and deeply considered, and built on firm foundations of logic and experience. Others, however, come from hiccups in our timeline, developed from quirks of coincidence, as inexplicable as they are strong. For me, this latter group includes my feelings about fall-themed salads, the Bank of International Settlements, Steph Curry—and [bullets](https://benn.substack.com/p/the-more-the-merrier). 

I have a moral aversion to writing with bullets. They strip the humanity out of ideas; they turn thoughts tactical, into “action items;” they make every message a *Politico* *Playbook* Morning Brief, meant for a Hill staffer who wants a daily news email that confirms to them they don’t have time to read a daily news email. Bullets are devoid of context and connective tissue, snappy takeaways without supporting evidence, all chorus and no verse.

But, as I was reminded by a friend I recently exhausted with a four-page wall of prose, bullets have their place. When people are upset, irritated, or just want to [read the recipe](https://slate.com/technology/2017/12/why-does-every-online-recipe-begin-with-the-preface-to-a-personal-memoir.html) without slogging through an indulgent preamble about childhood memories of honeysuckle, they need bullets, not a shoehorned [bank robbery analogy](https://benn.substack.com/p/data-is-for-dashboards).

Reluctantly, I have to admit that my friend is right. She’s right today, and she was right eight years ago, when I learned the same lesson the hard way.

In the months immediately following Mode’s initial launch, we had too few users to justify spending much time analyzing how they were using the product—but we had enough that we needed to provide them with a lot of technical support. As Mode’s analyst at the time, I found myself reassigned to be Mode’s support agent. 

Though I eventually rotated off the support queue and into other roles, I stayed involved long enough to work with the early team that replaced me. In doing so, I had a chance to learn from a handful of far more talented and compassionate[^1] support experts than I ever was. 

Sitting in the trenches together with confused and frustrated customers, they didn’t just teach me how to close out a ticket. They showed me how to handle the [constant pressure](https://www.youtube.com/watch?v=tQwVKr8rCYw)[^2] of an infinitely scrolling to-do list. They showed me how to diagnose a problem, how to listen, how to talk to someone who’s frustrated, and how to meditate without taking sides. They taught me that support, like sales, is a job everyone should do at least once. And they taught me how all of this can make me a better analyst. 

They also taught me that, sometimes, you need to use bullets. No customer wants an essay from their support agent, nor do they want short text messages, abbreviated to the point of illegibility. Instead, they want instructions, with the important points highlighted, and the details included if they need them. Often, that means they want lists, section headers, and bolded callouts. 

So, in honor of that lesson, and in honor of my friend who reminded me of it again this week, these are the other things that the support team taught me—in all their formatted, bulleted, and bolded glory.

### Be a detective, not a consultant

When a customer reports a bug, production is whatever is on their screen. No matter what *should* be happening, no matter what’s happening when you try to reproduce it, no matter what the code says, no matter how little information they give you,[^3] their problem is the problem to fix. 

The same applies to analytics. As [Randy Au eloquently put it](https://counting.substack.com/p/the-many-faces-of-production), **production is what’s in people’s heads.** It doesn’t matter how it got there or if we believe it to be wrong. They think it; therefore, it is. Our job, first and foremost, is to figure out more about it:

We can’t do either of these things, however, if we act as consultants, charged with imposing our expertise on the unenlightened masses. Instead, analysts should model themselves after detectives who don’t seek to correct others’ experiences, but to understand them. And just as a detective—and a support agent—would never tell a witness they’re wrong, neither should we. We should instead say, “that’s interesting; tell me more.”

### People want to be seen

I was once told by a senior solutions and support leader that the biggest complaint technical users have about support teams is that they don't listen, and they don't understand users’ businesses. For these users, being told no was fine; they knew their questions were sometimes unreasonable, and their feature requests were unrealistic. But being rejected offhand, before they had a chance to explain what they wanted and why it was important to them—that was unforgivable. 

Good support teams understand this. They understand that, sometimes, **customers just want to be heard.** They’re frustrated by a bug, and they want someone to validate that emotion. They’re excited about an idea, and they don’t want to be demoralized by a quick dismissal.

As an analyst, I struggle to remember that this also applies to the questions people ask of data teams. When people come to data teams with bold ideas that we know won’t work, it’s tempting to immediately dash their dreams with an avalanche of well ackshuallys—”we tried this before and it didn’t work, we can’t get that data, and even if we could, it couldn’t be interpreted that way.”

This misses the point. People usually don’t want a model that precisely predicts which customers are going to miss their invoice deadlines; instead, they want the data team to understand that late payments are causing trouble. **They want us to see their problems as much as they want us to solve them.**

### We don’t have to be smartest person in the room

Next time you’re sitting in a meeting, take a moment to ask yourself what you want people to think about the next thing you say. Do you want them to be inspired? To laugh? To feel supported? To see you as being organized? To see you as being in charge?

For a lot of analysts, if we’re honest with ourselves, our answer is that we want people to think that we’re smart. We’ve been conditioned to believe that, above all else, our [cultural capital comes from our intellect](https://twitter.com/hspter/status/1440407412377198601). Better to be a prickly truth teller than a hyperbolic leader or a passionate manager. 

On a support queue, however, **nobody cares how smart you are**. On the contrary, the best support agents deflect their own cleverness back onto the customer. 

For example, prior to joining Mode, our first support lead worked at a hardware company. Their customers tended to put the batteries in their products incorrectly, but people reacted poorly when support agents asked them to check if they’d messed up something so simple. So the support team devised a strategy: Tell the customer the problem can sometimes be resolved by taking the batteries out and putting them back in. When customers did this, they’d notice the batteries were wrong, but could do so so without having to admit to the mistake they’d made. 

When these sorts of things happen—when people report a bug but were actually clicking on the wrong button, or when they thought they couldn’t log in but were using the wrong password—you want to take credit for solving the problem. You want the customer to know that you knew what was happening all along. You want them to know that you were smart enough to figure it out. 

Our support team taught me to let it go. Give the customer the win; take the glory to the grave. 

For data teams, our job is the same: Help people solve problems. We can—and should—be satisfied by good results and by happy coworkers; we don’t need measure our worth by how smart they think we are smart too.[^4] 

### Have faith in the loose science of CSAT scores

When I first started researching how to measure the performance of Mode’s support team, I was excited to find how many [quantitative metrics](https://blog.happyfox.com/support-performance-metrics/) we had to choose from. That same initial team lead, however, dismissed most of these numbers. They’re useful, she said, but argued that we should focus most of our energy on customer satisfaction, or CSAT, scores.

As a stubborn analyst, I hated this idea. Unlike metrics like first response time or average time to resolution, a CSAT score is built on a subjective foundation. It’s often just a user survey, in which customers are asked to rate how they *feel* on an arbitrary scales of numbers (and, worse still, emojis[^5]). 

I was wrong. The best support interactions weren’t the ones that got the fastest responses, or resolved in the fewest messages. Moreover, people can’t realistically “vote with their feet” by choosing other alternatives for technical support. Our help line was their only option. **CSAT scores, imperfect as they are, measured what we cared about most—if people were actually happy.** Sometimes, the best way to figure out what people think is to just ask them.

I’ve come to believe the same thing about data teams. Yes, CSAT captures feelings and not facts, but feelings are [often rooted in facts](https://benn.substack.com/p/method-for-measuring-analytical-work#:~:text=We%20develop%20gut,with%20a%20chart.%C2%A0) we can’t quite describe. Rather than looking for indirect proxies for assessing if a data team is serving their internal customers well, just ask those customers. If they say yes—if business leaders would rather have an extra cook in their kitchen when they’re making a decision, with all the friction that introduces—would any metric really convince us that they’re wrong? 

### It’s not about you

When someone reports a bug, you can react in a lot of different ways:

Away from the support queue, when you’re talking to internal teammates about the recent conversations you’ve had with customers, you can also position yourself in several ways:

This happens because support agents are caught in the middle between company and customer. They’re the voice of both to the other, professionally invested in the success of each, with little agency over either. Speaking from experience, it’s confusing to know where to stand. 

Data teams sit in a similar breach. They operate in the spaces between teams and competing opinions, and are often called upon to indirectly mediate internal disagreements. But data teams [aren’t disinterested observers](https://roundup.getdbt.com/p/saying-true-things-is-hard): They’re invested in the business, and likely have personal relationships with people on both sides.

Support teams offer a potential solution: **Be everyone’s stern therapist.** Listen, give people emotional space, tell the facts straight—and, most importantly, recognize that none of this is about you. The customer isn’t mad at you; they’re just mad. The marketing leader who’s holiday campaign is flatlining didn’t want you to bury that information; they just need time to process it. And neither of them need a cheerleader to amplify their emotions; they just need someone to help them figure out what to do next.

### Finally, take pride in maintenance 

During my first few months on the support queue, I often went home frustrated by how little I got done every day. I'd arrive in the morning, talk to customers, close out tickets, and return the next day to a fresh batch. Aside from gradually expanding our help docs, I built nothing. At a startup where progress is paramount—and in an industry that [tried to manifest](https://future.a16z.com/its-time-to-build/) “IT’S TIME TO BUILD” into the zeitgeist—I was running on a treadmill. 

I should’ve taken more pride in that work. We appreciate airplane pilots who fly back and forth along the same routes every day; we celebrate chefs who cook food that needs to be made again tomorrow night. These jobs are no less important than those of the idolized builders; they’re just shaped differently. 

Data teams should remember that as well. We often chase big projects, like launching a new testing platform, building a new pricing forecast model, or finally refactoring the core financial metrics. **But the value of this work doesn’t discount the value of analytical maintenance**—of keeping the key dashboards up, of making sure operational teams have quick access to information, and [of reliably publishing the news](https://medium.com/p/265f13a37fa5#:~:text=Getting%20information%20in%20front%20of%20people%2C%20even%20if%20no%20single%20conversation%20explains%20everything%2C%20can%20keep%20people%20well%20informed.).

By rejecting the identity of a service organization, we turn our queues of requests into to-do lists, a looming prerequisite to burn down before we can get to the bigger, more meaningful stuff we’re supposed to be doing. But there’s something to be said for embracing parts of the service role. Our backlogs are inevitable, and a service mindset can turn those requests into routines; they can become weekly work that isn’t an ante to do something better, but a healthy habit to be celebrated.

Just as a support team will never finish their work on the queue, we’ll never be done with maintenance. We can’t change that. But we change how we see it, and how we value ourselves on the days that we do it. 


---


[^1]: Low bar.

[^2]: I finally [got that login](https://twitter.com/bennstancil/status/1486064476474548228), Jillian.

[^3]: The second most memorable ticket I ever worked on started with a simple message: “Mode borked, you fix.” Though most issues didn’t start quite so starkly, this was life in support in a nutshell: Short, vague, and more emotion than detail. (The most memorable was the first ticket we got after our third major update to Mode’s SQL editor (codename: [Return of the Jeditor](https://mode.com/blog/multiple-queries/)). The user wrote in to tell us that something looked different, but “it didn’t look like a bug?”)

[^4]: There is, it must be pointed out, a caveat to this. Men are more likely to issue the pedantic corrections that are, first and foremost, meant to signal how smart we are, and we’re more likely to get credit for being smart without needing to remind people of it. To be part of the solution, men, I think, need to not only stop trying to be the smartest little boy in the room, but also recognize the more subtle ways that people demonstrate their intelligence.

[^5]: Say what you will about 1-to-10 scales, but they’re better than emoji scales. Intercom, for example, asked people to rate their support experience using a five-emoji scale where the highest rating was the heart eyes face ​​😍. On multiple occasions, customers told us they wanted to give us the top score, but didn’t, because it felt creepy sending their support agent a heart eyes face.

================================================================================

# Business in the back, party in the front

*Sorting through the chaos in the consumption layer.*

---

![](https://substackcdn.com/image/fetch/$s_!FL7K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F012c4464-c068-444c-b2e2-794aecd829de_1268x573.jpeg)

[The front is falling off.](https://www.youtube.com/watch?v=3m5qxZm_JqM)

Or, more accurately, the front is splitting into a thousand tiny pieces, dumping 20,000 tons of [crude oil](https://stkbailey.substack.com/p/data-team-roi) into our corporate environments. 

In this case, our enormous faceless frigate is the front of the modern data stack.[^1] Over the last decade, the data industry has been building a giant ship, now worth hundreds of billions of dollars, for ingesting, storing, transforming, and shipping data to every corner of every company in the world. Three-fourths of our boat has taken a clear shape around emerging architectural principles. We scrapped legacy ETL processes and replaced them with ELT; we’ve agreed to centralize our data in cloud warehouses that speak ordinary SQL (and have come to terms with Snowflake taking a tithe on everything we do); we do transformations in the warehouse, in SQL, and are [starting to debate](https://www.thoughtspot.com/blog/the-metrics-layer-has-growing-up-to-do) if we should define metrics in similar ways. 

Though far from universal, these approaches are at least normal.[^2] If a data leader pitches this design to their CEO, they can find hundreds of analyst reports, blog posts, and customer testimonials to back them up. Nobody gets fired for buying I(ngestion), B(ig cloud data warehouses), and M(odels in SQL).

But the front quarter of the ship—analytics and BI tools for data consumption—the front is a very different story. There are no defaults. There are no generally accepted standards. There’s [barely a shared understanding](https://blog.getcensus.com/what-does-data-as-a-product-really-mean) of what companies should be doing with the data they have, much less *how* they should do it.

Instead, the front of the data stack is represented by an explosion of tools, all tacking in slightly different directions. There’s [traditional BI](https://powerbi.microsoft.com/en-us/); there’s [modern BI](https://looker.com/); there’s [headless BI](https://medium.com/gooddata-developers/the-future-of-bi-is-headless-e3949bb0bf2); there’s [open-source BI](https://www.lightdash.com/); there’s [Bitcoin-based BI](https://markets.businessinsider.com/news/currencies/microstrategy-bitcoin-never-sell-5-bln-crypto-stash-michael-saylor-2022-1). There are [notebooks for analysis](https://deepnote.com/), [notebooks for SQL](https://count.co/), [notebooks for collaboration](https://noteable.io/), [notebooks for apps](https://hex.tech/), and [apps for notebooks](https://towardsdatascience.com/introduction-to-papermill-2c61f66bea30). There are [data visualization tools](https://www.tableau.com/), [data visualizations for notebooks](https://medium.com/@jonmmease/announcing-vegafusion-570f62207ba7), and [notebooks for data visualizations](https://observablehq.com/). There are [SQL editors for teams](https://popsql.com/), [SQL editors for people who don’t want to write SQL](https://www.metabase.com/product/), and [SQL editors for Snowflake customers](https://docs.snowflake.com/en/user-guide/ui-snowsight-gs.html). There are [collaborative workspaces](https://www.hyperquery.ai/), and [tools that combine lots of things together](https://mode.com/). There are [spreadsheets we can’t get rid of](https://www.tiktok.com/@miss.excel/video/7013792677523164421) and [spreadsheets replacing the spreadsheets we can’t get rid of](https://www.fastcompany.com/90716633/how-i-recreated-wordle-in-google-sheets); there are [rebuilt spreadsheets](https://equals.app/); there are [spreadsheets, but BI](https://www.sigmacomputing.com/product/sigma/). And more of everything [is coming](https://www.ycombinator.com/companies/?batch=W22&batch=S21&industry=Analytics).

All of these tools do ostensibly the same thing; they help people analyze data, and help companies make sense of that analysis.[^3] Which raises two obvious questions: First, do we need so many nuanced options, or will the shape of the front of the ship, much like the rest of the boat behind it, settle on a more narrow consensus? And second, if it does, what will it look like?

# The case for choice

A few years ago, I was talking with a handful of people at a data meetup in San Francisco. One person was sharing their frustrations about their current team. "We hired a couple of data scientists to solve hard problems like building ML models,” he said, “but they've mostly only had time to answer business questions like analysts."

As someone else was quick to point out, his premise was wrong: Helping people use data effectively *is* the hard problem. Unlike purely technical work, or the work of moving data around the lower levels of the data stack, solving business problems requires that data [cross the chasm from computer to person.](https://counting.substack.com/p/the-many-faces-of-production) The fragmentation of the analytics space may simply be a reflection of the boring truth that people are different, we understand things in our own ways, and we’ll never have a standard API into people’s heads.

Moreover, different companies also use data in lots of different ways. The consumption layer is the interface to those use cases, and may need to be as varied as they are. 

To continue our long history of [food analogies](https://benn.substack.com/p/the-future-of-operational-analytics#footnote-1), every business needs to cook different things with their data. Our kitchens are only so big, and don’t have the space or budget for every appliance and piece of cookware from Sur La Table. We’ve got to choose if we want a juicer or an Instapot; if we want a tortilla press or an immersion blender; if we want a conical burr grinder, a digital gram scale, a gooseneck pour-over kettle, and a Chemex carafe, or if we’re happy making coffee with a can of instant Folgers and the hot water from a garden hose that’s been sitting out in the sun. 

The best choices aren't universal. We all have different problems, and different aptitudes and preferences for how to solve them. What architecture is best? How should we compose our kitchens? It, as the classic line goes, depends. 

Still, kitchens have *some *standards. No matter what we’re cooking, we all need a sharp chef’s knife, a stock pot, and a couple sauce pans. Even if the consumption layers’s details remain stubbornly variable, surely, surely, we’ll eventually agree on a few essentials.

# The choices we face

In fairness, people have been trying to make sense of how we consume data for decades, and these aren’t exactly novel questions. But in fairness to being fair, a lot of the more recent conversations about the consumption layer have been dominated by voices who have a very big stake in which perspective prevails.[^4]

People who are incentivized to say that we shouldn’t consume data through dashboards say [we shouldn’t use dashboards](https://blog.count.co/dashboards-are-dead/). People who are incentivized to say that analytical applications are different from self-serve BI tools say that [analytical applications should be different from self-serve BI tools](https://hex.tech/blog/bi-tools-hex). People who are incentivized to say that collaborative, document-inspired experiences are best say that [collaborative, document-inspired experiences are best](https://twitter.com/josephmoon_ai/status/1482054010198654977). People who are incentivized to say that legacy BI is dead [declare legacy BI dead](https://benn.substack.com/p/bi-is-dead).

It’s not that these arguments are wrong (though they can’t all be right). Nor is there anything wrong with people making them—presumably, we created these incentives for ourselves because we believed them first. But, this makes the conversation about the consumption layer a campaign, and proxies opinions about architectures through specific products. That muddies feature sets with more fundamental questions that haven’t yet been sorted out. 

In the spirit of [trying to figure out](https://twitter.com/jthandy/status/1488136756205600772) what exactly we’re all doing here—of stepping back from talking about which kitchen appliances we need and not the brands behind them—these questions, however, are still worth asking.

### Specialized, or a suite?

Few people would disagree that different jobs call for different interfaces. Spreadsheets, notebooks, dashboards, exploratory visualizations—they all have their place, just as [docs and slides](https://twitter.com/josephmoon_ai/status/1460468073522106369) have their place in office productivity apps. 

The interesting question is about how they fit together. Should analytics tools exist as completely separate products, like the old desktop Office suite? Should they all be under one integrated roof, though remain generally distinct, like [Google Apps](https://en.wikipedia.org/wiki/Google_Workspace#History) (I mean Google Apps for Work, I mean G Suite, I mean Google Workspace)? Or should the lines between them be fully blurred, as they are in Notion and Coda?

### For analysts, or for everyone?

No modern analytics tool would dare not be collaborative. But collaborative among which groups of people? Specifically, should analysts and data scientists primarily live in an advanced tool, and the rest of the business live in a BI and reporting tool, with people occasionally interloping between the two? Or should everyone always gather together in one spot, whether they’re there to look at a dashboard of ad spend or to do a strategic investigation of why search ads are suddenly outperforming social ads?[^5]

This question is complicated by domain-specific apps, from traditional tools like Google Analytics to whatever operational tools [the future cooks up](https://benn.substack.com/p/the-future-of-operational-analytics). In a potential world where everyone lives in their functional apps, do we even need a tool for generic dashboards?

### Who’s an analyst, anyway?

Analysts’ and data scientists’ roles are getting compressed from both sides. Analytics engineers are eating into the upstream edge of their work, designing data models and configuring business logic that analysts used to be responsible for. And quantitatively savvy business experts are squeezing the downstream boundary, self-serving ([in theory, at least](https://benn.substack.com/p/self-serve-still-a-problem)) answers without analysts needing to intervene.

The latter case raises foundational questions about how non-analysts should consume data. Should they work in environments with high walls and protected paths, limited but [precisely governed](https://twitter.com/emilieschario/status/1393000594298834946)? Or should people be encouraged to gradually venture off the trail? 

Looker and Tableau provide useful examples of this dimension’s two poles. Though both sell to a general, “code-free” business audience, they clearly see that audience differently. Looker emphasizes governance and control; it’s BI with enough padding that nobody can hurt themselves. Tableau has a steeper learning curve and can be more easily misused, but, for the folks who invest in learning it, can stretch much further. 

### Separate, interoperable, or embedded?

We can also imagine more extreme reconfigurations of the consumption layer. Rather than every tool building their own cut of a notebook, or visualization engine, or SQL client—not to mention content management systems, admin tools, and application cruft that’s necessary in every modern SaaS product—vendors could simply provide composable pieces that get glued together elsewhere. This consumption layer could look like Wordpress: An open platform where everyone chooses their favorite plugins. Just as it has for other parts of the stack, will there be a day that modularity and interoperability—true interoperability, not just APIs shouting at each other—comes for consumption too?

Technology trends are often cyclical, swinging back and forth from [centralized and decentralized](https://moxie.org/2022/01/07/web3-first-impressions.html), from [bundled and unbundled](https://a16z.com/2019/09/11/platforms-verticals-unbundling/). In the analytics space, however, the [pendulum has lost its period](https://www.youtube.com/watch?v=dDU2JsgLpm4).[^6] That’s not necessarily a bad thing; innovation emerges from disorder. But it seems inevitable that the industry will eventually pull itself back in line, potentially corralled by community consensus, or, more harshly, yanked together by an [inescapable economic gravity](https://benn.substack.com/p/data-and-the-almighty-dollar). But either way, someday, our boat [will get its face](https://www.nytimes.com/2016/03/22/world/europe/boaty-mcboatface-what-you-get-when-you-let-the-internet-decide.html). 


---


[^1]: omg you must be soooo surprised

[^2]: And actually, it turns out, really do have a [minimum crew requirement of one](https://www.youtube.com/watch?v=3m5qxZm_JqM&t=64s).

[^3]: I forced a bot to read the marketing sites of a thousand data companies and write its own. Here is the first page: Collaboration and team collaboration help data teams collaborate! Announcing Notebook, the democracy for data teams. Stack modern data with AI-powered analytics at the speed of time. We’re humbly backed by $800 million in seed funding to remake the Snowflake data lake. Join our growing Slack community; no credit card required.

[^4]: Disclaimer: I have a [very big stake](http://benn.company) in which perspective prevails.

[^5]: Because, hoo boy, [are they ever](https://www.google.com/finance/quote/FB:NASDAQ?sa=X&ved=2ahUKEwigv476ieX1AhUpiOAKHQy5AmkQ3ecFegQILBAc&comparison=NASDAQ%3AGOOGL&window=1M).

[^6]: That music, though.

================================================================================

# The next billion programmers

*The next product I’d build? Excel, for everything.*

---

![](https://substackcdn.com/image/fetch/$s_!YOV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3aa536b4-3f18-4a84-893c-c7a16deb2b6c_2400x1219.jpeg)
*Where millennials learned Excel. *

If you ask data folks what they think of Excel, there are two acceptable answers. The first is rank contempt. It’s a bloated bundle of bad practices and antipatterns, the disease that won’t go away, the interminable termites in the foundation, the [crumbling hooks](https://www.youtube.com/watch?v=qBpiXcyB7wU&t=542s) on which our entire society hangs. The second answer is that Excel is software’s greatest achievement. It’s the ultimate Swiss Army knife; a perfectly designed cockroach; antifragile, manifest. 

There is no third answer. In some warped version of the midwit meme, savvy observers live on the extremes, and only fools—”I like Excel, but I don’t think there’s anything special about it”—occupy the seemingly reasonable middle ground.

In my view, Excel is an unqualified triumph, for one very particular reason: It’s given more than a billion people direct access to the profound power of computers.

Of course, it’s hardly controversial to say that computers are powerful tools. But to most people, myself included, that power is abstract and confusing. It’s the incomprehensible reach of Google, [finding 4.9 billion records](https://www.google.com/search?q=how+does+google+work&oq=how+does+google+work&aqs=chrome..69i57.2479j0j7&sourceid=chrome&ie=UTF-8) in less than a second. It’s the impossible precision of rocket science, blasting barn silos to the edge of space, and landing them, upright, on a tiny target in the desert. It’s the magic of hardware itself, which can somehow turn bits of metal and silicon scattered across my house into words on my screen, and then words on another screen in Thailand, milliseconds later.

In contrast, Excel shows us something much more relatable—the power of automating simple arithmetic.

Consider a simple everyday application of Excel: Planning a party for the Super Bowl [big game](https://www.vox.com/the-goods/2019/1/31/18202037/super-bowl-53-ads-trademark-the-big-game-2019). You might start by listing everyone who’s coming. Excel counts how many people are on your list, so you can immediately decide how many bags of chips to buy. If some people want to invite their friends, you can easily add them, and Excel updates the tally. You add what food people are bringing, and input how much money each person spent; Excel can now tell you how much money each person should chip in. If a few friends don’t drink, you can tweak the math to discount the cost of beer from their contributions. Last minute cancellations are no problem either; just remove people from the list, and all the numbers immediately get recomputed. 

None of this math is hard to do; it could be done just as easily by a middle schooler as a sophisticated piece of software. But it’s *irritating* to do. Excel provides a solution: Write a few formulas—a simple program—and have a computer do the arithmetic for us. In that, Excel isn’t valuable because it does something that’s incomprehensible; it’s valuable because it does something that’s easily understandable, over and over, at incomprehensible speed. 

While we could write programs to do the same thing in any number of languages, the genius of Excel is in its interface. Excel doesn’t force people to learn an entire language, but it doesn’t discourage them from writing code either;* *instead, *it makes the code they write remarkably efficient. *By embedding all of the computational logic in spreadsheet cells, Excel’s users don’t have to worry about variable assignment or object definition, much less much knottier things like development environments or package dependencies. They just enter their numbers, and write simple formulas to manipulate them.

This efficiency disguises what Excel really is: An interface for writing programs. In doing so, hundreds of millions of people who wouldn’t call themselves programmers became programmers, and Excel formulas became the [most widely used code in the world.](https://www.microsoft.com/en-us/research/blog/lambda-the-ultimatae-excel-worksheet-function/#:~:text=Excel%20formulas%20are%20written%20by%20an%20order%20of%20magnitude%20more%20users%20than%20all%20the%20C%2C%20C%2B%2B%2C%20C%23%2C%20Java%2C%20and%20Python%20programmers%20in%20the%20world%20combined.) And Excel, which at first glance is little more than a calculator running on a loop,[^1] turned into a limitless platform for automation, capable of planning parties, creating personal budgets, and modeling entire businesses and industries.

# Automate anything

If Excel is for automating arithmetic, more traditional programming languages are for automating anything on the internet. You can collect [data from Twitter](https://benn.substack.com/p/twitter-profile-pictures). You can clean up a [century of baseball box scores](https://benn.substack.com/p/a-season-without-bats). You can send emails, reformat messy files, start your [vacuum cleaner](https://www.npmjs.com/package/roomba-sdk), feed your [cat](https://petlibro.com/collections/automatic-pet-feeder), order a [Range Rover](https://www.ebay.com/itm/384685967273?hash=item59911207a9:g:V7AAAOSwXdlh5tJw), check your [flight status](https://aviationstack.com/), and digitally stamp your exclusive ownership of a reference to a link to a jpeg of a [single green pixel](https://opensea.io/assets/0x495f947276749ce646f68ac8c248420045cb7b5e/23438849716378104815337549082155974522842134269864614386264898953977958236161).[^2]

There is, as a matter of both convenience and genuine influence, enormous power in doing these things automatically. By figuring out how to search for tweets programmatically, I was able to fix my strategy for playing Wordle.[^3] By writing a script that would automatically find promising candidates and send them messages on LinkedIn, a friend of mine became many times more productive as a recruiter and changed the course of his career.[^4]

These examples, unfortunately, are the exception. Both my friend and I work in the tech industry, and were surrounded by others who could inspire us to search for these sorts of automations. Most people aren't so lucky. They don't have the examples to emulate that we did, or teammates who they could learn from.

But what they do have, as Excel shows us, is the ability. Programming custom logic into an interface isn't some rare talent; it just has too many prerequisites. Setting up an environment to write, manage, and run code has too many complicated steps, and too much can go wrong during each one. In Excel, you can model the entire global economy without parsing a single stack trace; in general programming experiences, you can't get to "Hello, world" without doing so.

Which raises an obvious question: Could there be an Excel for generic scripting? Just as Excel made it possible for billions of people to write code to automate arithmetic, could there be a tool that makes it possible for billions of people to automate anything?

# Excel for Everything

This isn’t an entirely novel concept, and lots of popular tools solve adjacent problems. Nothing, however, quite hits the nail on the head. 

Zapier is great for connecting different apps together, but it encourages people to follow prebuilt recipes and use code as a ([still complicated](https://zapier.com/help/create/code-webhooks/use-javascript-code-in-zaps)) escape hatch. Excel, by contrast, simplifies the code necessary to do certain things, but embraces coding as part of its interface. 

Airtable and Retool help people create lightweight applications with interactive widgets. Though these can be used for a wide range of problems, they aren’t really meant to automate generic tasks. Their success, however, shows how imaginative people can be once they’re given tools that make hard things easy.

Notebooks and hosted Python runtimes get closer, but tools like these are *too* open-ended. They solve the development environment problem, and create an interface that, if you really squint, looks like a series of linked cells like Excel. But they require people to write and debug raw Python, from start to finish. For example, when planning a party, you can simply type in attendees’ names into Excel, and then use Excel’s formula language to gradually add more complexity. In a notebook, that list has to be created in Python directly. It’s a pool with only a deep end.

What, then, would be better? How do we give people a shallow end, and let them swim into deeper waters?

In my view, we could build a more generalized Excel as a collection of interconnected blocks, where each block contains a spreadsheet (or other data structure) of its own. For each block, you could either enter data directly into it, as you would in Excel, or read from other blocks, just as formulas in Excel do. In the latter case, the formulas could be simple functions for performing common actions, or they could be generic Python. Each cell is, in effect, a tiny lambda, with easy-to-access inputs.

The rest of the interface should make it easy to control how data moves around the blocks, and to see the intermediate outputs that each block produces. Each bit of Python, then, could be focused on exactly the job it needed to do—parse some text, reformat a data structure, call an API. The goal, in other words, should be to make each block small enough that it could be [copied from Stack Overflow](https://stackoverflow.blog/2021/12/30/how-often-do-people-actually-copy-and-paste-from-stack-overflow-now-we-know/). 

By making each block run independently (i.e., they can read from other blocks but aren’t defined by other blocks, just as *logic* in one Excel cell is independent of the logic in other cells), people can work piecemeal, without having to think about the entire system in their head. This interface would let people construct programs iteratively, with the UI guiding them to add more complexity just as Excel’s does. 

Admittedly, I haven’t considered all of the details here. And it’s possible someone else has tried this, and it didn’t work.[^5] But given the success of Excel, the power in simple programs, and the accessibility of languages like Python once you remove all of the preconditions that are typically required to use it, it’s hard to dismiss the possibilities of what a combination of Python and Excel could do.[^6]

If I were living on another timeline, it’d be an idea that I’d want to explore. As someone who’s scattered dozens of Python scripts around my laptop and Dropbox account, I want a tool that would help make them easier to build and manage. And as someone who’s tried to deploy a number of AWS Lambdas internally at Mode, but has always been stymied by some API gateway permission, incomprehensible IAM configurations, or the unholy nightmare that is AWS CloudWatch, I’d love a tool that makes Lambdas serverless *and* painless.[^7] 

But more importantly, I think it’d be cool if a lot more people could do this too.

# “You can do anything”

Back in Mode’s early days, I was talking to a couple engineers about an idea for a new feature. At some point during the conversation, I asked them if it was possible to build it. In response, one engineer said, “It’s a computer. You can build anything.”

Initially, I thought it was a hyperbolic throwaway line, like a parent telling a kid that they can eat whatever they want at a mall food court. I’ve got a few choices, but I can’t actually build *anything*…can I?

No, I was told; he meant it literally. Within the bounds of a few physical laws about the speed of light, we could build anything. Some things will be harder or take longer, but eventually, if we can think of it, we could make a computer do it. 

It took me a while to truly internalize this idea, but there’s something magical about finally understanding what he meant. There’s something magical about seeing a problem and knowing—*knowing*—that, somehow, some way, it can be solved. There’s something magical about running a program and watching it slowly tick through its operations, each printout simultaneously affirming that you figured out the puzzle, and deepening your sense of amazement about how any of this actually works. And there’s something magical about hitting enter—always a little harder than normal—to run these scripts, and still feeling a shooting pride and reflexive smile as I watch them go.[^8]

Sadly, most efforts to share this feeling with people focus on turning people into professional programmers. Programming, we’re implicitly told, is a career, not a skill. Or worse still, programmers are who you are, and some people [just aren’t math people](https://www.theatlantic.com/education/archive/2013/10/the-myth-of-im-bad-at-math/280914/).

Someone—some *thing*—should change that narrative. Programming is simply a tool, applicable to everything from planning a party to blogging, and to every career from recruiter to lawyer. With a bit of help to make it more accessible, anyone can do it—and with it, anyone can do anything.


---


[^1]: I mean a literal calculator, with buttons and a screen and C and CE keys that allegedly do different things.

[^2]: For sale, 300 ETH, OBO.

[^3]: Prior to [this post](https://benn.substack.com/p/how-to-play-wordle), I was 16 for 18. After changing my strategy, 20 for 20, with only one game going six turns.

[^4]: This was years ago; since then, people have built a number of products that do the same thing.

[^5]: In which case, my stance is [no one has truly tried it](https://en.wikipedia.org/wiki/No_true_Scotsman).

[^6]: And yes, while you can kinda sorta [run Python inside of actual Excel](https://towardsdatascience.com/how-to-supercharge-excel-with-python-726b0f8e22c2), that largely misses the point. The interface should be designed around using Python; Python can’t be tacked on to today’s spreadsheets.

[^7]: Given how simple the substantive parts of the these Lambdas are, and how complex the cruft around them is, I’ve always felt that they represent a huge missed opportunity. Two things I’d immediately want to invest in: “Lambdas, but better,” and “Slack, but more like email.”

[^8]: I have a pet theory that this is one of the reasons dbt is popular. It introduced a lot of people to the command line—and, by extension, to the feeling that comes with executing a program on it. Moreover, most of the interface of the dbt’s command line product is a slow drip of green success indicators. To the cynical software engineer, this is just a day on the job. But to the rest of us, it’s a satisfying mix of power and wonder, like seeing under the hood a car for the first time.

================================================================================

# Disclose your angel investments

*The community has given us a lot. We should be transparent about it.*

---

![](https://substackcdn.com/image/fetch/$s_!FQ-Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6a1b2eb7-433c-4c2f-bc87-374df382c8af_1440x810.jpeg)
*When the lights go out, we turn on each other.*

The modern data industry is full of awkward alliances. Boundaries between categories are still being defined; startups are looking for open spaces in the stack to wedge themselves between big companies; big companies are trying to figure which parts of the stack they can annex next. The result is a lot of uneasy partnerships among companies with partially overlapping edges, and aggressively overlapping roadmaps.

But there’s another, less visible network of elite entanglements that crisscross the industry: shared investors.  

Most companies raise the majority of their capital from institutional investors. These investors, especially those with a particular focus on data products, can create loose fraternities among their portfolio companies. Though this undoubtedly creates bias and subtle distortions in the market—a fund may favor one company over another because of a prior investment—venture firms weren’t built to allocate capital in perfectly efficient ways. More importantly, all of this is, for the most part, above board. Corporate investors typically disclose their investments, and board relationships are often made public. 

But institutional investors aren’t the only entities on most companies’ [cap tables](https://carta.com/blog/what-is-a-cap-table/). It’s also common for startups to take small amounts of money from various operators and influencers in their industry. In most cases, these investors aren’t brought on board as part of classic angel or “friends and family” rounds that are meant to get a company off the ground before proper venture capitalists are willing to invest. Instead, they’re optional add-ons that are more about building relationships than raising money. 

It’s a mutually beneficial arrangement for both parties. As a founder, you want people who are well-connected to be in your corner. You want the support of people who have large audiences that will trust their endorsement. You want advice from seasoned operators. And you want to share the potential rewards of your success with those who helped you in your career. Starting a company—especially for the first time—is an unsettling thing, and it’s easy to question if it's a good idea to try. Few things are as reassuring as a handful of experts putting their own money on the line to back you. 

Angel investors also stand to benefit from the deal. For the PR-minded investor, it provides an opportunity to paint yourself as paying your good fortune forward, and fueling the engine of American innovation. It keeps you connected to the market, engaged with interesting problems, and introduces you to the most revered of Silicon Valley characters, the Founder. For the cynic, angel investing is a way for the rich to get richer.[^1] It lets you stroke your ego, act like you know more than you do, flip through fundraising decks, and ask founders who’ve spent two years researching a problem and building a product if they thought of something that occurred to you three slides into their pitch. Quietly, it’s also a personal hedge—if the next wave of entrepreneurs passes you by, you can ride their coattails to continued wealth and relevance.

However you cut it, though, it’s definitely *normal. *Ever since people started building semiconductors in Palo Alto garages, the revolving door between founders and angel investors has been part of the lifeblood in Silicon Valley. People start companies; people make money; people invest that money in the next generation of companies.

It’s the startup circle of life—but it’s not all hakuna matata. Angel networks are often built on circular upvoting and implicit kickbacks—you scratch my back, I scratch yours—that creates an old boys’ club of insiders, with privileges for some and not others. Often, the people who need money and votes of confidence the most can’t get them, while those in need of neither are oversubscribed. Moreover, because people typically don’t invest in competitive companies, this dynamic can simultaneously open doors for those who are well-connected, and actively shut doors for those who aren’t. By recruiting the right angels, founders can garner the support of industry influencers and pacesetters.[^2] Those in the clique get made by the family, and in Silicon Valley, [you don’t speak against the family](https://techcrunch.com/2022/01/31/ryanbreslow-bolt/).

These same dynamics underlie much of the data ecosystem. Some firms invest aggressively in the space and actively circle the wagons around their portfolio companies. Though this corporate posture is probably inevitable—VCs have LPs and fiduciary responsibilities, too—the same pattern exists among angels and influencers. Without more transparency into who’s financially aligned with whom, this risks something that should be bigger than any individual’s portfolio: the health of the community.

# The vendor code of conduct

The analytics community [is a vibrant place](https://benn.substack.com/p/delirium), full of lots of real people, looking for a good careers and a supportive network. To this point, those in the community, from its creators to its newest members, have done a remarkable job in providing that, while avoiding the toxic undercurrents that infect a lot of adjacent groups.[^3] This extraordinary streak is powered by the underlying decency of the community’s members, and by the [assertive](https://docs.getdbt.com/docs/contributing/slack-rules-of-the-road) [norm-setting](https://locallyoptimistic.com/community/) of the community’s guardians. Chief among these norms: Vendors should clearly identify themselves, and not solicit members to buy their wares. 

I’ve never met anyone who doesn’t emphatically support this rule. Ulterior motives undermine meaningful discussion. Just as [members of Congress won’t write fair legislation](https://www.nytimes.com/2022/02/09/us/politics/pelosi-stock-trading-congress.html) if they’re worried about how a law affects their stock portfolios, data professionals can’t have fair conversations if they’re subversively shilling for a particular product (or, more commonly, subtly undercutting a competitive product). Having talked with plenty of people about data warehouses, about ETL, and about BI, my relationship with Mode always makes talking about BI far harder. I have a blatant bias, and I’m corrupted by it, every time. I can’t change that, nor can any other founder or employee with a stake in their company. All we can do is disclose it.

The same applies to investors, other shareholders, and anyone else with a vested interest in a company's success. They may not be on the payroll, or own as much equity as a founder, but they still have a commercial incentive to promote one company over another. Their views on that company—*my* views on the companies I’ve invested in—are not neutral. Our testimonials aren’t unbiased; our hype is not organic. We’re invisible thumbs on scale, gently tiling honest conversations in our financial favor. 

These distortions are complicated and messy. Data companies are built by data people, sold to data people, and partner with data people. As soon as these same groups start to invest in one another, it becomes impossible to untangle partnerships from friendships, customers from investors, and spectators from shareholders. In Silicon Valley, where momentum and market perception can be worth more than real revenue, this stuff matters. 

And in a community that already [has an exclusive streak](https://benn.substack.com/p/who-is-the-community?utm_source=url), this stuff *really* matters. As the data ecosystem grows, kind words and thoughtful guidelines alone won’t keep it healthy or make it inclusive. Community leaders also have to be honest about what we get out of that community, which products we favor and why, and which outcomes and exits we stand to benefit from. People turn to this community for advice on everything from small decisions to career-defining ones; the least we can do is reward that confidence with an equal measure of transparency. 

Unfortunately, this doesn’t always happen. I *know* people who write about companies while burying their affiliation with that company or one of its competitors. I *know* of customer quotes on websites that come from people who were investors before they were customers. I *know* companies that have built speaker lineups with undisclosed names from their cap tables. 

To be clear, none of this is illegal ([usually](https://www.justice.gov/usao-ndca/pr/former-netflix-executive-sentenced-30-months-bribes-and-kickbacks-netflix-vendors)). Nor do I begrudge anyone for being an angel investor, or any company for taking angel investments. I have no issue with people who become experts in their industry getting rewarded for that expertise. And as a founder, customer, ecosystem partner, person with a small audience on the internet, and, over the last year, periodic investor, I contribute to this dynamic as much as anyone.

Speaking from experience, it can be genuinely hard to know what to do. Is it wrong to invest in a company because of personal relationships? What happens if two companies I’m associated with become competitive? “[How can I be helpful](https://twitter.com/vcstarterkit/status/1245545300762038272)” to companies I’ve invested in if Mode partners with a rival? How do you draw lines between what you hear privately and what you say publicly? How do I keep friendships and financial allegiances from infecting this blog?

Honestly, I don’t know. I’m still figuring this out. But, I think one rule is obvious, for me and any other angel investor who participates in “the discourse:” Disclose your investments. Just as there are [journalistic standards](https://www.nytimes.com/editorial-standards/ethical-journalism.html#investmentsAndFinancialTies) for reporters, there are ethical ways to invest. If you are a shareholder in a company and are discussing something related to what that company does, be upfront about your relationship.

Despite ranting about the data industry for the better part of a year, I’ve only done this twice, both in [offhand](https://benn.substack.com/p/semantic-observability?utm_source=url#footnote-5) [footnotes](https://benn.substack.com/p/metrics-layer?utm_source=url#footnote-7). 

**To correct that, [this is the complete list](https://www.benn.ventures/).**

I’ll add future investments to that site. If you’ve also made investments in the space, I hope that you do the same. 

# Beer and money

My dad, who spent his entire career in family law, recently told me that *Anna Karenina *[got it wrong](https://en.wikipedia.org/wiki/Anna_Karenina_principle). Nearly every failed marriage fails for one of two reasons: Beer, or money. 

Thus far, the data community has been an uncommonly happy union. While the people in the community deserve most of the credit for that, we can’t ignore the other major factor that underpins its success: Money. In the last eighteen months—if not the previous decade—there’s been no shortage of cash to go around. It’s easy to be friends when everyone’s making money, and we can all ride high-flying corporate brands to personal success. 

Though the community probably won’t turn on itself over cases of Bud Light, bull markets and [1,500x revenue multiples](https://www.forbes.com/sites/kenrickcai/2021/12/17/data-startup-airbyte-is-set-to-become-a-unicorn-intensifying-showdown-with-rival-company-fivetran/?sh=1ef8a73e1f38) don’t last forever. Sooner or later, the money will begin to dry up. Sooner or later, the stakes of who wins and who loses will get very real, and congenial coopetition will turn into [cutthroat competition](https://benn.substack.com/p/data-and-the-almighty-dollar). Sooner or later, Frank Slootman [comes for all of us](https://www.aol.com/news/snowflake-ceo-why-you-must-declare-war-on-your-competitors-183109540.html). 

Nothing would tear the community apart faster than fissures at the top. And nothing drives people apart faster than conflicts of interest and suspicions about backroom alliances. Given the unavoidable things that imperil the culture of the community—competition, philosophical disagreements, and inevitable bad blood between people in a group of thousands—we shouldn’t add unspoken investments to the list. Those of us who’ve been sustained by this community at least own it that. 


---


[^1]: Unless, of course, you lose all your money. But if you do, who knows, you might [get played by William H. Macy in a Hulu miniseries](https://en.wikipedia.org/wiki/The_Dropout_(miniseries)).

[^2]: I remember watching this play out in real time in 2013. When we founded Mode, we needed a place to host our blog. The two options at the time were Medium, founded by Twitter founder Ev Williams, and Svbtle, founded by minor internet celebrity Dustin Curtis. The two products were effectively identical. I chose to use Svbtle because it looked a little better, and because it had a like button, called “kudos,” with a cool animation. Silicon Valley’s elite chose Medium (though, more accurately, chose Ev). Premier tech writers and influencers quickly coalesced around it. Within a couple years, Svbtle was [effectively dead](https://www.designernews.co/stories/44300-is-svbtle-dead). Though their fates could’ve diverged for a number of reasons, I struggle to believe that Ev Williams’ connections didn’t play as big of a role as anything.

[^3]: To be fair, I don’t use Reddit. Last time I did, I bought three shares of Gamestop stock and [started a blog](https://benn.substack.com/p/runaway-train). It’s probably best for all of us that I don’t go back.

================================================================================

# Nothing to add

*Sometimes, there’s no math to do. *

---

![](https://substackcdn.com/image/fetch/$s_!gFq1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F448a7587-abc5-4ab3-9f00-857c42b49494_2109x1406.webp)
*A free country.*

As inapt as it would be to talk about the bundling of the modern data stack while a brutal dictator is launching a murderous invasion of free and democratic neighbor, it would be even more inapt for me to talk about that invasion. I know next to nothing about Ukraine, other than what’s happening there is a horrible crime, and that maps of troop movements advancing on major cities belong in high school textbooks, not in live updates on the *New York Times*. 

In moments like these, from those of global calamity to individual tragedy, it's hard to know what to do with a microphone. For leaders, it's tempting to tell strained stories about some tenuously related personal journey that demonstrates your solidarity with those who are suffering. For people with audiences on the internet (or with weekly blogs and a vain pride about keeping a publishing cadence), it's tempting to ignore the whole thing, and carry on in the comfort of the familiar. For data people, it's tempting to treat numbers like a universal cipher that grants you the privilege to weigh in on any issue, if only you show your cute little proof. No matter your role, though, the important thing, it feels, is to have something to say.

The lesson I’m trying to learn, while watching a confusing war on a distant shore, is that it’s ok to be at a loss for clever words (a lesson that, given the paradoxical existence of this post, I’ve yet to fully embrace). It's ok to simply reaffirm the obvious—that, while, most conflicts are complex and nuanced, this one is not; there is nothing to add up; there are good guys and bad guys, and this is a war of unhinged aggression. It’s ok to drop the facade of cool detachment and jaded cynicism, and share how you feel—anxious, stunned, helpless, and indiscriminately angry. It’s ok if the best words of support you can muster ring in your ears as hollow and vague—they may sound differently to those who need them.

And beyond that, it’s ok to watch, to listen, and to have no other numbers or opinions to add.

================================================================================

# Work like an analyst

*We’re not so different, business stakeholder and I.*

---

![](https://substackcdn.com/image/fetch/$s_!H7Al!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fadb05705-75ef-4b5e-b38a-a83632f157cf_1200x800.jpeg)
*Guts. Glory. A mini donut drive-thru. Ram.*

A couple weeks ago, the data community was overtaken by a giant debate, the likes of which not seen since the data mesh entered the chat in 2021, about whether or not the stack is being [unbundled](https://blog.fal.ai/the-unbundling-of-airflow-2/) or [rebundled](https://dagster.io/blog/rebundling-the-data-platform), [and](https://www.dataengineeringweekly.com/p/bundling-vs-unbundling-the-tale-of?s=r) [what](https://roundup.getdbt.com/p/disjointed-lineage?s=r) [it](https://metadataweekly.substack.com/p/metadata-weekly-17) [all](https://roundup.getdbt.com/p/bundled-or-unbundled-data-stack) [means](https://twitter.com/nick_handel/status/1496147736223227910). It was a [periodic flare-up](https://twitter.com/sarahcat21/status/1493680206434897920) of the only thing [we can ever talk about](https://twitter.com/bennstancil/status/1426276715383468033), and, if nothing else, proof that, no matter how much we fight with [Gartner and their ilk](https://benn.substack.com/p/the-missing-analytics-executive?utm_source=url#footnote-4), we also can't help but [put things in their pedantic little boxes](https://benn.substack.com/p/gerrymandering?utm_source=url#footnote-4). 

I’m not here to choose a side in that discussion. In my view, the middle layer of the data stack—say, everything after ingestion and before consumption—is neither being bundled or unbundled; it’s simply being reconstituted. Some components are being pulled apart, others are getting mixed together, and the final product will look nothing like its former parts. We aren’t building a Lego from discrete blocks, or disassembling a set into individual pieces;[^1] we’re making a stew. You could argue, I suppose, that chefs unbundle potatoes from the peel and meat from the bone, and then [rebundle it with a pot and some broth](https://www.youtube.com/watch?v=Sr2PlqXw03Y). I’d argue that that’s a weird choice of verb. 

But that’s not to say there’s no bundling happening in the data ecosystem. It’s just happening in a different place. 

# How companies do analysis

When we first started Mode, we expected our users to fall into two clear categories: Analysts, and everyone else. Analysts, we thought, would live in Mode’s SQL editor, write lots of queries, and distribute their work to their colleagues. Everyone else—the non-analysts[^2]—would look at dashboards and poke at reports, but wouldn’t write queries. Very few people, we assumed, would exist in the middle as occasional query authors. Just as people who aren’t designers create very few assets in Photoshop, we thought people who weren’t analysts would write very few queries in Mode.

![](https://substackcdn.com/image/fetch/$s_!cH8V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6e4ccda-2a7e-441f-b5f7-0a4490ceadeb_1306x762.png)
*What we thought our usage distribution graphs would look like*

We were wrong. Analysis, even SQL-based analysis, isn’t like design, where a handful of people create stuff and everyone else is a consumer, with clear lines between them. It’s much, *much* fuzzier. Though analysts were always our first adopters, lots of people—PMs, engineers, marketing managers, executives, support agents, operations leads, and all job titles in between—periodically wrote queries. These people occupied the middle part of the distribution between analysts and non-analysts that we thought would be vacant. Users weren’t bimodal like we expected, but continuous. 

![](https://substackcdn.com/image/fetch/$s_!RRXr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8f85bd2c-0942-46f8-ba0f-147b20b40285_1298x770.png)
*What our usage distribution graphs actually looked like.*

Moreover, non-analysts gradually deepened their technical usage (i.e., query writing) over time. After being exposed to their company’s data, people who we didn’t classify as analysts—and, based on their initial behavior, didn’t classify themselves as analysts either—started to display the same behavioral patterns as analysts. 

It’s not surprising that most people want to be part of the analytical process. In many ways, we built Mode for that reason—just as [Figma created](https://kwokchain.com/2020/06/19/why-figma-wins/) a “way for non-designers to be involved in the design process,” we wanted to do the same for analysis.

What *is* surprising is the lengths people went to be involved. When we first identified these behaviors, Mode didn’t offer much to non-analysts. The product was heavy on SQL and light on code-free, drag-and-drop interfaces. Mode wasn’t drawing people into the analytical process by developing an accessible way for them to participate; instead, people were participating in the same way that analysts did, just at a less frequent cadence. 

Though this is a complex phenomenon to interpret, one explanation stands out above others: Analysis is becoming a team sport. Business stakeholders aren’t kicking questions over to their data help desks and waiting to get reports back to review; everyone is contributing together. A few months ago, [Tristan predicted](https://roundup.getdbt.com/p/data-expertise-everywhere) that analytical talent wouldn’t be concentrated in a “select few specialists,” but would “live inside of operational areas of the business.” If Mode’s customers are any indication, this started happening years ago. 

As far as paradigm altering shifts go, this one—analysis becoming multi-functional; the democratization of analytical reasoning and technical talent, not just data—is far more foundational than whatever reshuffling happens among the data stack’s middleware. After all, what ultimately matters [is the experience](https://benn.substack.com/p/the-modern-data-experience?s=w) that middleware provides. While it’s right and fair to talk about how our [fragmented SaaS landscape](https://sarahsnewsletter.substack.com/p/what-is-saas-debt) undermines that experience, that’s only half the equation. Applications need to be complementary to one another *and* to the people they’re trying to serve. 

For decades, the industry has thought of those people the way we did at Mode in the early days: different, divided, and discrete. This belief is embedded across the entire data ecosystem. Products have technical and non-technical faces; vendors distinguish between analyst and non-analyst seats; brands are built around which side of the spectrum you favor; Gartner tells you you need to buy [two](https://www.gartner.com/reviews/market/analytics-business-intelligence-platforms) [reports](https://www.gartner.com/reviews/market/data-science-machine-learning-platforms) and not one. 

What do we do if this changes? What happens if, ten years from now, we can’t separate between analysts from their opposite—not because we don’t know what to call the latter group, but because they no longer fill clearly distinct roles? 

# Where we’re going, we still need roads

It’s awards season, and let me propose a perennial Oscar snub: The American Truck Commercial. Seemingly thousands of these spots get churned out every year, and all of them follow the same spectacular arc: They show a truck, perfectly polished, bounding through the American West. Over a [montage of farmland jingoism](https://www.youtube.com/watch?v=AMpZ0TGjbWE), a man—he’s never shown, but we can see him, in his 50s, a smoker with calloused hands, squinting in the late Montana sun, rough but not unkind, beaten down but not broken—talks about [freedom](https://www.youtube.com/watch?v=8gwNS9QkFes), [hard work](https://www.youtube.com/watch?v=CLIqYbeoEss), and how, [out here](https://www.youtube.com/watch?v=jj1VxPVEEoE), we don’t [drink oat milk](https://www.youtube.com/watch?v=dhEkVakVWFE) or [use the internet](https://www.youtube.com/watch?v=QJxc7ZaLL8A). We see trucks [plowing snow](https://www.youtube.com/watch?v=r0ArLFDT_tM); we see trucks [drifting through deserts](https://www.youtube.com/watch?v=mMq5zuSqA8o); we see trucks towing [trees](https://www.youtube.com/watch?v=Lq5C6hz7uK0), [boats](https://www.youtube.com/watch?v=rjxIjcEhS-M), and [heavy machinery](https://www.youtube.com/watch?v=2Kft22BHIak); we see trucks [forging rivers](https://www.youtube.com/watch?v=wbx38XzzSQk);[^3] we see trucks offroading [up mountains](https://www.youtube.com/watch?v=JBCb3UUWrxk), [through snow](https://www.youtube.com/watch?v=AtQIz5pkwwg), and [over rocks](https://www.youtube.com/watch?v=qUYiguby-w4) in the protected areas of the Badlands National Park. We do not see pavement, women,[^4] or people of color.

Having grown up in a place where people buy a lot of trucks, I can assure you, this is not what people do with them. They don’t haul [space shuttles](https://www.youtube.com/watch?v=TLdA8tEpO0k); they drive their trucks on the interstate and idle them in Wendys’ drive-thrus. 

But this paradox is why they’re popular. While trucks can hypothetically “[conquer Denali](https://www.youtube.com/watch?v=hyNshwyrtGY),” roads are pretty useful too. Even if you want to go [mudding](https://en.wikipedia.org/wiki/Mud_bogging) at the dirt spot, you can’t drive a [swamp buggy](https://en.wikipedia.org/wiki/Swamp_buggy) down I-85 to get there.[^5] 

Trucks, by contrast, can be luxury vehicles while simultaneously embodying America’s pioneer ideals: Offroading on parking lot medians at high school football games.[^6] They’re spacious and comfortable enough to take the family on a road trip to the beach, and rugged enough to (hypothetically) [drive in the ocean](https://www.youtube.com/watch?v=osAtAgxOMag) once you’re there.

Compare this to the alternative modes of transportation, like traveling by plane, then train, then car, then dirt bike. These vehicles live on their own thoroughfares, each disconnected from the other. And each requires a new set of skills to operate. You need a pilot to fly you from Des Moises to Houston, a conductor to run the train to Galveston, a driver to taxi you to the beach, and a fourth expert to teach you to ride a dirt bike on the beach. With a truck—and an overlapping system of highways and roads—one person, in one machine, can do it all.

For decades, the data industry hasn’t had a truck. We've had power tools for analysts, meant to be driven by elite experts across treacherous and unexplored terrain. And we’ve had simple reporting applications for everyone else, designed to be put on cruise control across well-trafficked highways. Because neither was created for the other’s domain—you can’t drive a four-wheeler across the country, or a Greyhound through the [Lost Coast](https://en.wikipedia.org/wiki/Lost_Coast)—these two sets of roads remained disconnected, as did the people who drive on them. 

In practice, this means that people often operate in different tools—and live in [different realities](https://benn.substack.com/p/data-is-for-dashboards?utm_source=url#:~:text=In%20this%20way%2C%20companies%20don%E2%80%99t%20live%20in%20a%20physical%20world%2C%20but%20in%20a%20virtual%20one%2C%20like%20those%20in%20Pixar%20movies.%20The%20landscape%20around%20a%20company%20is%20artificial%2C%20and%20exists%20only%20as%20a%20rendered%20representation%20of%20the%20calculations%20that%20define%20it.%20And%20to%20make%20decisions%20in%20that%20world%2C%20you%20have%20to%20create%20it%20first.). People who spend their time looking at dashboards see one view of the business; people who are investigating problems see another. Even if these worlds aren’t contradictory, they’re not often shared. 

In the past, when thinking about this technical divide, [I’ve assumed it was a product problem](https://benn.substack.com/p/gerrymandering). Over the last decade, we’ve created a lot of specialized tools, and segregated our “jobs to be done” as a result. If only we could better connect these tools—if only our criss-crossing BI highways had off-ramps to the detailed city streets, and the ad hoc trails in the countryside beyond—we’d solve this problem. 

I’m now realizing that misses the real point: The divisions of labor between analysts and everyone else are fading. Analysis is getting bundled with other functions; the behaviors of analysts and non-analysts are overlapping; analysts are becoming [positionless](https://wraptext.equals.app/every-analyst-is-a-finance-analyst/). The reason to build a “[modern data experience](https://benn.substack.com/p/the-modern-data-experience)” isn’t to unify the disjointed products of a bunch of startups; it’s to serve a world in which far more people want to work like analysts. 

That’s a much bigger—and harder—ambition. But fortunately for us, if we need a recommendation on how to bring red and blue people together into a [purple center](https://blog.getdbt.com/we-the-purple-people/), there’s a[ truck commercial for that too](https://www.youtube.com/watch?v=-gPOPLrUfyw).


---


[^1]: But if we *were *taking apart a Lego, [Joel](https://twitter.com/joelcarron) can tell you what [color your pile of pieces](https://mode.com/blog/lego-data-analysis/) would be.

[^2]: Business users? Domain experts? Stakeholders? Non-technical users? People you want to hang out with? `SELECT * FROM people p LEFT JOIN analysts a ON a.person_id = p.person_id WHERE a.person_id IS NULL`?

[^3]: Your F-150 broke an axle, Josephine got dysentery, and everyone died before you made it out of Nebraska. But on the bright side, you did collect four tons of buffalo meat from hunting nine times in two days.

[^4]: The truck commercial [Bechdel test](https://en.wikipedia.org/wiki/Bechdel_test): Do women exist?

[^5]: This sentence is, in fact, real.

[^6]: Those who drove, say, their grandmother’s [1998 Pontiac Grand Am](https://en.wikipedia.org/wiki/Pontiac_Grand_Am#/media/File:1999-2002_Pontiac_Grand_Am_SE_sedan_--_12-23-2011.jpg) were confined to concrete roads of communism.

================================================================================

# A very big deal

*Snowflake goes shopping, and buys the store.*

---

![](https://substackcdn.com/image/fetch/$s_!8fN9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F53c344b3-e1e0-43e6-b79d-483efd7658e7_1280x720.jpeg)

Tech acquisitions are like marriages: They mean a lot to the people involved, but the rest of us would be forgiven for not noticing that much is different. After blustery announcements about synergy, shared visions, and obligatory corporate excitement for the next step in the journey, most acquisitions fade from relevance. The rough edges of innovation get sanded down. A few fonts and brand colors get updated. Product roadmaps are gutted to make space for backend infrastructure integrations and strategic repricing initiatives. Acquisitions don’t create revolutions; they capture those that have already happened, and repackage them into a limited enterprise offering.  

Last week was an exception. We may not have felt it then, we may not feel it tomorrow, and we may not even feel it in twelve months, but five years from now, we’ll look back and see it—[Snowflake’s acquisition of Streamlit](https://blog.streamlit.io/snowflake-to-acquire-streamlit/) started an avalanche, and the data landscape will be forever changed because of it.[^1] 

On the surface, the acquisition of Streamlit, an open-source framework for creating data apps, is both underwhelming and a bit confusing. In the fundraising fever dream between a fading pandemic and an impending nuclear winter, a purchase price of $800 million wasn’t an uncommon sum for a data company. Industry observers, accustomed to seeing [much](https://techcrunch.com/2021/08/31/databricks-raises-1-6b-at-38b-valuation-as-it-blasts-past-600m-arr/) [bigger](https://fortune.com/2021/04/13/scale-ai-valuation-new-funding-fundraising-data-labeling-company-startups-vc/) [totals](https://techcrunch.com/2021/09/20/fivetran-hauls-in-565m-on-5-6b-valuation-acquires-competitor-hvr-for-700m/), reacted to the announcement with a collective shrug: Tech publications [quoted](https://www.datanami.com/2022/03/04/snowflake-nabs-streamlit-for-data-science-apps/) from press releases, people [quipped about the name](https://www.linkedin.com/feed/update/urn:li:activity:6904957953007349760/), and Wall Street [took an ax](https://www.cnbc.com/2022/03/02/snowflake-snow-earnings-q4-2022.html) to Snowflake’s stock price, either for lowering their growth forecasts or for existing a global economy that could, at any moment, be annihilated by a humiliated madman who misses his stolen yacht. 

Assuming we aren’t all nuked into oblivion, the acquisition is still somewhat perplexing. Financially, it definitely doesn't make sense. While Snowflake, which is still worth $60 billion after its stock market swan dive, can easily afford it, it’s paying a huge premium for a company that [reportedly](https://www.linkedin.com/feed/update/urn:li:activity:6904957953007349760/) makes less than $100,000 in revenue. Even by last year’s fundraising standards, an 8,000x revenue multiple is steep.

Streamlit is also a peculiar strategic fit. The company hosts a platform that helps data scientists build data apps with a few lines of Python. These apps aren’t exactly traditional BI tools, but they exist squarely in the consumption layer of the data stack. 

Compare this acquisition with the other notable announcement in the industry last week: [Google laid off](https://community.looker.com/looker-adoption-community-1016/can-someone-confirm-looker-laying-off-us-dcl-29178) Looker’s entire U.S. support staff. Though layoffs happen for all sorts of reasons, the tea leaves of this one are easy enough to read. BigQuery, as many have long suspected, is the gravitational center of Google’s data stack. While Looker [makes real money](https://twitter.com/drinkzima/status/1482407107177562112), Google's balance sheet only registers line items that make preposterous money. [High-volume computing services](https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html)—i.e, BigQuery—can do that in the way that SaaS applications can't. For Google, Looker is too big to fail, but too expensive to scale. The future of the cloud data warehouse business is, in other words, the cloud data warehouse.

Why, then, would Snowflake, BigQuery’s most direct competitor, invest more in the top of the stack, just as Google is backing away from it? Why would Snowflake look at the same market and move in the opposite direction? 

Because that's not what they are doing.[^2] Streamlit will never step foot in Gartner's data apps magic quadrant, or, I suspect, ever even attempt to bring a traditional product to market. It will instead become something more fundamental: the platform—in a true sense, the [capital-P platform](https://stratechery.com/2018/the-bill-gates-line/#:~:text=A%20platform%20is%20when%20the%20economic%20value%20of%20everybody%20that%20uses%20it%2C%20exceeds%20the%20value%20of%20the%20company%20that%20creates%20it.)—on which every other commercial data app is built and sold. 

# The promise and peril of data apps

Last fall, a16z's Martin Casado predicted that we’re going to [remake all of today’s SaaS apps on top of the data layer](https://www.youtube.com/watch?v=q1nERFM9brA&t=3458s). Soon, according to Martin [and others](https://pchase.substack.com/p/thenewbackend), we’ll all be running our businesses through “data apps,” which, not yet having an agreed-upon definition, I’m defining as a product that solves a non-data problem where removing data ruins the product.[^3] It’s a compelling future, where data is more than a "Insights" tab glued onto a recruiting tool or a task management app. It's a future in which data is [embedded in operational experiences directly](https://benn.substack.com/p/the-future-of-operational-analytics), just as reviews are inextricably interwoven into Yelp. 

But, as I’ve argued before, there are a few obstacles that make this vision impractical today. First, it’s technically incomplete. The “data layer” isn’t yet [a coherent platform](https://benn.substack.com/p/entity-layer); it’s just a database. Early data apps like [Eppo](https://www.geteppo.com/), [Supergrain](https://www.supergrain.com/),[^4] [Vero](https://www.getvero.com/), and [Narrator](https://www.narrator.ai/) simply connect to analytical warehouses directly, and “integrate” with the data layer through a JDBC driver and some SQL queries. 

Though that architecture is potentially powerful, it’s not an easy sandbox to develop in. SQL is a clumsy language for application development, and most databases don’t provide true APIs, much less software development kits, or SDKs. Without these tools, building data apps today is akin to building web applications before frameworks like [Rails](https://en.wikipedia.org/wiki/Ruby_on_Rails)—every application has to solve low-level problems like, “How do I interact with a database?” and "How do I translate this user action into performant DDL?" It's possible, but it's not easy.

This creates the second challenge to Martin’s vision: Data apps are expensive to build. Dealing with these technical complexities requires engineers, and hiring engineers requires money. 

Moreover, data apps are also expensive to sell. Enterprise IT teams generally prefer that people don’t install random apps from the internet on corporate computers, and they’ll hunt you down if you connect those apps to tightly governed data warehouses. As a result, in addition to engineering teams, data app vendors also need go-to-market teams to pitch, sell, and support their products.

This [ratchets up the economics](https://benn.substack.com/p/data-and-the-almighty-dollar) of the data app market. The cost of selling a data app requires vendors to charge a high price for that app; to command high prices, app providers are incentivized to expand the surface areas of their products; this makes hard-to-create products even mores expensive to build and maintain, amplifying the pressure to sell them at a high price. 

For all the talk about the value of modularity in the data stack, these dynamics push the data app market towards monolithic consolidation. Just as there are no boutique car manufacturers, so long as building and distributing data apps requires significant capital, we won't have a thriving ecosystem of specialized data apps either. Instead, we'll have a bunch of startups chasing the only things that make them economically viable businesses: [Significant venture funding](https://twitter.com/jthandy/status/1442840153345695744), big visions, and the soft monopoly of category creation.

Streamlit and Snowflake could change all this. 

# The framework

Today, Streamlit is primarily focused on manipulating and presenting data within the Streamlit application. There are [utilities for connecting to databases](https://docs.streamlit.io/knowledge-base/tutorials/databases), but they’re mostly light wrappers around Python SQL clients like psycopg2. Streamlit’s [methods](https://docs.streamlit.io/library/api-reference) and [components](https://streamlit.io/components) libraries, which provide ways to “visualize, mutate, and share data,” are far more expansive. 

Over time, I expect this balance to flip. A package for wrapping yet another abstraction layer around Python visualization libraries is cool; a framework for interacting with Snowflake as though it’s a transactional database—as a kind of [active record](https://en.wikipedia.org/wiki/Active_record_pattern) for Snowflake, if you will—is transformative. 

For instance, a Streamlit app’s configuration files could define which tables in an underlying Snowflake database are [entities](https://benn.substack.com/p/entity-layer). From within the app, developers could then interact with those tables, via Pythonic syntax, with Streamlit facilitating the handshake between the app and the database. Rather than translating application logic into strings of SQL queries and back again, developers could simply call something more native:

Not only does this shortcut otherwise difficult operations, but it could also solve [much bigger problems](https://twitter.com/sarahcat21/status/1486723639365947406) that most data apps punt on today. Take access controls—as anyone who’s ever built a data app knows, trying to translate *application-level* permissions into *database-level* permissions is an immensely complicated problem. Streamlit could handle this directly: By being aware of both the application user and the underlying database grants, the query generated by the code snippet above could automatically change depending on who that user is. It’s this sort of magic, much more so than filter widgets and gauge charts, that transform data apps from neat toys into markable products.

# The marketplace

Notably, a framework like the one above would make Streamlit more complicated. Rather than helping data scientists build simple apps in minutes, it would help developers build complicated apps in days. 

But that’s not a step backwards; it’s a huge step forward. The impact of Rails and Django, which are used by engineers to create rich web applications that can be fully commercialized, has been unfathomably greater than the impact of low-code website builders like Squarespace. Streamlit could offer the same promise. To get there, though, it has to be willing to sacrifice the lower end of the market for the top end, and invest in frameworks over components.

But if they do that, Streamlit could also catalyze the second tectonic shift in the data landscape: The introduction of a Snowflake app store. 

With support for more complex apps, Streamlit and Snowflake could host a marketplace of commercial apps, similar to those like Eppo, Supergrain, Vero, and Narrator. Before listing the apps, Snowflake would review them, and pre-approve each as as meeting (or not meeting) various security and compliance standards like HIPAA and SOC 2. In return for this, and for making vendors’ products available in a single click, Snowflake would take a cut of each sale. To jumpstart the ecosystem and give vendors an incentive to create and list their apps, Snowflake could also pass a portion of the compute costs generated by the apps back to them as a form of royalties. 

The combination of these two things—a framework for data app development, and a marketplace for apps that Snowflake stands behind—closes nearly all the gaps in Martin’s vision. It provides the technical layer on which apps can be built. It makes developing apps cheaper, and makes selling apps easier. And if apps are built on top of an entity-like framework, it makes them fast to deploy as well. To get started, all customers would need to do is point the apps to the tables required to back them, like an event stream formatted like Narrator’s [Customer 360](https://www.narrator.ai/customer-360/).

Imagine, for example, what it would take to bring a customer success app to market in this new world. Rather than integrating with Salesforce and other CRMs, the product could simply require a customer entity with a predefined schema. Customer usage data, which is notoriously hard to reliably load into marketing and sales tools, could be ingested though a few user and event entities.[^5] With direct access to this data, the app could then create renewal forecasts, automatically suggest customer interventions, and write customer health scores back to the database. And all of this could be built by a small team, distributed directly through an app store, and sold for a fraction of the cost of a comparable SaaS app today. 

# The store next door

This isn’t, of course, a magic bullet. Most obviously, it's restricted to Snowflake. For most apps, that’s probably not as limiting as it seems at first glance. App developers can still make plenty of money selling only to iPhone users; as long as development costs are reasonably low, data app providers could still make good money only selling to Snowflake’s 6,000 customers.[^6] 

A Snowflake app store also starts to collide with the other nascent “app platforms” in the space, like [dbt’s](https://blog.getdbt.com/next-layer-of-the-modern-data-stack/) and [Looker’s](https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model) unified semantic layers. In Google’s case, their offering is likely to become directly competitive at some point, the Android Play Store to Snowflake’s iOS App Store. That’s ok though; the two would likely share structural similarities, and would expand the market to Google customers as well. 

dbt is in a more interesting position. Down one path, they could embed themselves *behind *app development frameworks, providing additional logical governance for entities and metrics. Or they could expand upwards, and offer an open, database-agnostic development environment, where `Users.filter(...)` gets translated into the SQL dialect of your choosing.[^7]

Regardless, Snowflake has set the avalanche in motion. The release has been triggered, a data app development framework is coming, and an app store is close behind. It won’t completely reshape the data landscape—avalanches level trees, not mountains—because we’ll still need [core categories](https://benn.substack.com/p/the-intergalatic-data-stack?s=w) like centralized warehouses, ETL pipelines, and robust analytics tools. But it will blanket thousands of enterprise experiences, from how we hire to how we send prospects’ swag, with a cascade of small data apps that do very little, but do it very well. 

That’s how Apple and Android devices injected software into every corner of our personal lives. And it’s how the modern data stack will inject data into every operational nook of professional lives.


---


[^1]: [Sooner or later, Frank Slootman comes for us all](https://benn.substack.com/p/disclose-your-angel-investments#:~:text=Sooner%20or%20later%2C%20Frank%20Slootman%20comes%20for%20all%20of%20us).

[^2]: One assumes. I have no idea what they’re actually doing. [Nobody tells me nothing](https://www.youtube.com/watch?v=b_G5EPNEHDo); I’m just out here taking swings.

[^3]: Mint? Data app, because you use it to manage your personal finances, but if you remove the data about your spending patterns, it no longer works. Salesforce? Not a data app, because you can take the reports out of Salesforce and still have a reasonably functional CRM. The *New York Times*’ [COVID dashboard](https://www.nytimes.com/interactive/2021/us/covid-cases.html)? Not a data app, because it’s not a product meant to solve a problem; it’s just an interface for exposing data.

[^4]: I’m a [personal investor](https://benn.substack.com/p/disclose-your-angel-investments) in Supergrain.

[^5]: While reverse ETL tools could also do this, that means the ETL tool has to build the integration *and* the customer has to buy the reverse ETL tool. If the goal is make the app marketplace more dynamic, adding in a bunch of extra costs and product dependencies probably doesn’t get us very far.

[^6]: This doesn’t apply to some types of products. For example, analytics and BI tools, almost by definition, need access to a wide range of data sources.

[^7]: “I’ll take door number 3, Monty: This is all nonsense, these are all terrible ideas, and we’ll be ignoring the whole thing.”

================================================================================

# Startups shouldn’t care about revenue

*And data teams should make sure they don’t.*

---

![](https://substackcdn.com/image/fetch/$s_!hRAs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F58a856df-c0a7-4873-bddb-ee5927418a93_1268x998.jpeg)
*[Selling your soul for thirty pieces of silver.](https://en.wikipedia.org/wiki/Judas_Repentant,_Returning_the_Pieces_of_Silver)*

The stock market is down again, and that can only mean two things: First, woo woo technical analysts are posting candlestick charts of stock prices, randomly drawing lines between points, circling the days when Mercury was in retrograde, calling it “[the death cross](https://twitter.com/Jason/status/1503915388425306119),” and announcing that it’s time to buy again because the [13-session countdown pattern](https://www.cnbc.com/2022/03/16/charts-suggest-the-nasdaq-100-and-sp-500-could-be-days-away-from-bottoming-jim-cramer-says.html) has broken a resistance band; and second, venture capitalists are falling over themselves to tweet out apocalyptic [R.I.P Good Times](https://articles.sequoiacap.com/rip-good-times) predictions. 

In the latter case, the story will play out as it always has. VCs will recycle euphemisms about “opportunities to trim the fat;”[^1] they’ll trot out lazy stories about how the most enduring companies are forged in the hottest fires; and they’ll scold the unchecked enthusiasm and unrealistic valuations of the last eighteen months, as if they didn’t bankroll the whole party.  

Substantively, their actual advice will be simple: [Protect your cash](https://twitter.com/DavidSacks/status/1503812105895522306). Raise it if you can, spend it cautiously, and do whatever you can to make more of it. 

On one hand, this prescription is obvious, and obviously correct. Revenue is an inescapable gravity that pulls every company—no matter how high-flying they once were—back to earth at some point. Businesses can’t outlast their bank account.[^2]

On the other hand, taken too directly, this can be a dangerous suggestion for a lot of startups, especially young ones. [Like raising venture capital](https://m.signalvnoise.com/reconsider/),[^3] revenue is a one-way door—once you use it as the scale with which you weigh yourself, you never step off. For early stage companies that are still looking for their way, they should focus on the long-term value they can provide, not the short-term value they can extract. And a stock market panic doesn’t change that.

# “Pre-revenue”

I’ve always heard that doctors struggle to watch shows like *Gray’s Anatomy *because the medical technobabble is too ridiculous to overlook. I struggled to watch HBO’s *Silicon Valley *for the exact opposite reason—it was too real. 

In one particularly [scathing scene](https://www.youtube.com/watch?v=BzAdXyPYKQo), several of the characters are talking about how their company can make money. Russ Hanneman, the belligerent embodiment of Silicon Valley’s id, cuts them off: “If you show revenue, people will ask how much, and it’ll never be enough…It’s not about how much you earn, but what you’re worth.” Though the bit goes a little too far—Hanneman suggests that you actually want to lose money—his main point doesn’t miss: In Silicon Valley, perception is all that matters.

How do people, and specifically venture capitalists, perceive an early-stage company that’s still taking shape? The dirty secret is that it’s through whatever lens the company presents itself. If a company leads with how much money it makes, VCs will score it on revenue growth. If a company leads with daily active users, VCs will score it on product adoption. If a company leads with community enthusiasm, VCs will score it on the chatter they hear in the ecosystem. Investors may prod at the other benchmarks, but any chart that’s sufficiently up and to the right can tell a compelling narrative, and fund a business to keep building. 

There is astonishing possibility in this dynamic. It’s the manifestation of Silicon Valley’s almost foolish optimism, and the magic that lets companies focus on solving problems without worrying too much about mining money out of user excitement. It is what lets us start with an idea, not an accountant, and what lets us dream in years, not months and quarters. 

This isn’t to say it’s all fair and good; [it is not](https://benn.substack.com/p/disclose-your-angel-investments?s=w#:~:text=Often%2C%20the%20people%20who%20need%20money%20and%20votes%20of%20confidence%20the%20most%20can%E2%80%99t%20get%20them%2C%20while%20those%20in%20need%20of%20neither%20are%20oversubscribed.). But it’s what we have. It’s the sling that gives upstart Davids a fighting chance against corporate Goliaths; the rule that we can play by that many of our opponents can’t. We should hold onto it dearly.[^4]

This holds true in bear markets and recessions. In public markets, a “flight to safety” drives investors away from risky assets and equities into safer, slower growth securities like bonds and large cap stocks. For VCs, such a shift doesn’t make sense. The entire investment strategy in venture capital is built around [hitting a few home runs](https://altos.vc/blog/paradox-of-the-power-law-in-venture-capital) on high-risk, long-term bets. Though downturns put pressure on startup valuations, this rule still applies—it’s either go big, or get profitable. Companies shouldn’t get caught in the middle by half-heartedly gesturing towards the latter before their revenue growth proves they’re on a path to the former.

# Why *not* revenue?

This raises the question though—why not focus on revenue? It will eventually matter, either to VCs, to public markets, or to a bank account that’s running dry. Is it not better to preempt all this and build a healthy financial business as soon as possible?

I can’t speak to how institutional investors would answer this question, though Silicon Valley clearly has no problem backstopping yawning financial losses [for a very long time](https://techcrunch.com/2022/02/03/snap-finally-did-it-yall/). But, on the other side of the table, inside the startup and, particularly, on a data team, I can say one thing definitely: Revenue is a bad KPI.

We like to think of “north star metrics” as the scientific underpinnings of our businesses. They separate the truth from our opinions, and help us make objective, reasoned decisions. In practice, however, they’re [often lousy](https://benn.substack.com/p/tilt-and-tilted?s=w) [at both](https://benn.substack.com/p/does-data-make-us-cowards?s=w). Instead, the real value of a key metric is something much squishier: alignment and inspiration. It translates a vague mission statement—connect the world, make commerce better, empower healthier lives—that could be interpreted in dozens of different ways into something concrete. It provides a tangible means to a vision’s nebulous ends.

This only works, however, under three conditions. First, key metrics have to be singular. Companies can’t chase dozens of performance indicators, with each team having their own preferred set. If they do, everyone ends up confused; at worst, people proxy their opinions through weaponized KPIs.[^5] This doesn’t end arguments; it escalates them.

Second, key metrics have to feel within reach. If they’re at the end of a long chain of dominos—this happens, and then this happens, and then this happens, and then the metric improves—people drift from them.[^6] Or, if decisions are tied to metrics via a tenuous series of causes and effects, any decision can be justified, just as a sufficiently complex conspiracy theory can find a path to any conclusion it needs to.

Finally, key metrics have to endure. They can’t encourage halting starts and stops, or favor penny-wise improvements over pound-foolish sacrifices. They need to point us to the spot we want to be on the horizon, not compel us to look at our feet. 

Revenue fails on all three fronts. Revenue goals are often set month to month, and quarter to quarter. Moreover, for most employees inside a startup, it’s a distant and delayed metric that offers no guidance on how to make decisions, no feedback on the quality of those decisions, and no reward if those decisions are correct. In need of other ways to measure progress, or in search of a way to ascribe more meaning to what they do, people create intermediate metrics, splintering the organization’s focus across different—and often misaligned—goals. 

For alternative metrics, [smarter](https://www.slideshare.net/03133938319/numbers-that-actually-matter-finding-your-north-star) [people](https://future.a16z.com/north-star-metrics/) than me have lots of ideas. The only point I’d add—for the pedantic data people like me—is that no metric is perfect. It can always be gamed. There can always be scenarios in which it goes up, and something bad still happens. There will always be measurement errors. Pointing these things out is neither clever nor helpful; to the contrary, it undermines the exact thing the metric is meant to accomplish, which is to rally people around a single direction. A [simple metric presented with conviction](https://eppo.substack.com/p/stop-micromanaging-product-strategy?s=r) is far better than a detailed one presented with precision.

# For as long as you can

For any company wondering which metric matters most to them, there’s an easy test to figure it out. Imagine it’s the end of the quarter, and you missed every target except one. Which target would you choose to hit?

For as long as you can, make your answer something other than revenue. For as long as you can, don’t start pitch decks and board slides with revenue numbers. For as long as you can, start the story of your company’s progress around another metric. For as long as you can, measure your success using a better scale. For as long as you can, celebrate hitting revenue goals, but treat it as the byproduct of bolder ambitions.

Simple as it sounds, this isn’t an easy fight. Executives will always want financial dashboards; the hard work of generating revenue should be applauded and rewarded; P&L statements will always be lurking somewhere in board presentations. In conversations about company performance, if there is no loud and persistent alternative, revenue metrics will always fill the void. 

This is true in good times, and it’s especially true now, during moments of market uncertainty. But no matter how top of mind revenue is for a leadership team, to most people inside a startup, it’s not a useful fixation. And as data teams, it’s our job to make sure there’s something better to pay attention to. 


---


[^1]: Kudos to Scale for calling this out in the early days of the pandemic. [As Rory O'Driscoll said](https://www.scalevp.com/blog/admitting-ignorance-planning-in-the-time-of-covid), “Phrases like ‘cut the burn’, thrown out in a cavalier pseudo-tough fashion, do not capture the impact of this on the people involved.”

[^2]: That said, it recently occurred to me that, for as much as Silicon Valley types like to brand themselves as business savants and ridicule government institutions for not having the discipline to manage their balance sheets, we also celebrate a lot of people who, in all likelihood, have never worked for a company that actually made money. Which made me curious—in how many years of your professional career did your employer turn a profit? I’ll start: One year, out of thirteen. [Take the survey!](https://docs.google.com/forms/d/e/1FAIpQLSeDj7yv8NFRQpmLup7qVTJ5_ahHo_QFmlwpXhB_rHsTCOQdzw/viewform) *[ An update: [The results!](https://benn.substack.com/i/66244589/reader-mailbag-are-companies-profitable)}*

[^3]: Linked to with great reluctance and self-loathing.

[^4]: Lest this be interpreted as cynical advice to [fly too close to the sun](https://www.apple.com/tv-pr/originals/wecrashed/) (or to run an outright Ponzi scheme), the point is not to build nothing of value. The point is that companies have more leeway in how they define that value than they often think.

[^5]: “New customers is a key metric for us this year, so I think we should lower our prices.” “Well, revenue growth is also a key metric, so I think we should raise our prices.”

[^6]: My first job out of school was at a think tank in Washington, D.C., where our goal was to influence policy decisions that [safeguard global peace and security](https://carnegieendowment.org/about). The line between my work and global peace was roughly as follows: I write a paper that my boss likes; my boss publishes it; some low-level Fed staffer finds it; that low-level staffer shares it with a high-level staffer; the high-level staffer makes of note of it in their report to Fed Board; someone on the board reads the report; Fed policy changes because of the report; the change helps stabilize the global economy; international peace (e.g., collect underpants → ? → ? → ? → ? → ? → ? → world peace). Needless to say, I didn’t feel attached to the mission.

================================================================================

# The ghosts in the data stack

*An OLAP cube exorcism.*

---

![](https://substackcdn.com/image/fetch/$s_!3pP6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb875e474-00b1-4b41-9ce2-fb2ba4214782_1400x1050.jpeg)
*Do not make a SQL pun, do not make a SQL pun, do not make a seque-*

Data, [Vicki Boykis tells us](https://vickiboykis.com/2021/03/26/the-ghosts-in-the-data/), is full of ghosts. Teams, organizations, and the analytics industry at large are haunted by implicit knowledge—knowledge that “exists within expert communities but is never written down." What it actually means to clean data, how to navigate the politics of influencing decisions, how to evaluate and purchase software—these are the things data professionals need to know but are never taught. And they’re the things that, once you discover, you forget how you learned. 

OLAP cubes are another such ghost. Despite being a foundational piece of data technology, they’re mostly ignored by today’s data community. Despite being frequently referenced in marketing white papers, technical blog posts, and Gartner’s numbing cocktails of buzzwords and acronyms,[^1] OLAP cubes are uncomfortably difficult to define. Wikipedia offers a brief, self-referential definition—an OLAP cube is a multi-dimensional array of data—and refers us to an [academic paper](https://arxiv.org/abs/cs/0701155) on databases for more information. [OLAP.com](https://olap.com/learn-bi-olap/olap-bi-definitions/olap-cube/), a website presumably dedicated to the promotion of OLAP cubes, tells us what they do (“a data structure that allows for fast analysis”) and what they’re not (OLAP is *not* OLTP). 

Last year, Claire Carroll, motivated by exactly this ghost—she quotes a [Twitter user](https://twitter.com/Mjirv/status/1425223072743923716) who says “the ratio of ‘number of articles I have read about OLAP cubes’ to ‘amount of understanding I have of them’ is truly outrageous”—finally [gave us a worthwhile definition](https://analyticsengineers.club/whats-an-olap-cube/). OLAP cubes are just tables, but tables structured in a very particular way. 

When we think of a table of data, we instinctively think of it as a list of objects. Each row represents some discrete concept: a person, a purchase, a campaign donation, an action taken on a website, a pitch thrown in a baseball game. Not only is it easy to imagine how someone might collect this data—when a person buys something, add that purchase and various details about it to the ledger—but it’s also intuitive to manipulate. To count the number of individuals in a table of people, count the rows; to find purchases of a particular product in a transactions log, search for the entries for that item; to calculate the average donation from a list of campaign contributions, average the “donation amount” column.

![](https://substackcdn.com/image/fetch/$s_!KrXL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9dc0a02d-a88e-4ce6-b8a4-520fc6342414_1600x594.png)
*This is a purchases table, and we all know what it means.*

As sensible as tables like these are, they can also get quite large—large enough that, twenty years ago, performing these simple operations on them was impractically expensive. Just counting the number of items sold in a table of purchases, for example, could take minutes or hours; more complex calculations, like computing median sales prices by state, could take far longer. 

OLAP cubes—i.e., tables of a particular structure—were created to solve this problem. Rather than a list of objects, OLAP cubes are a table of metrics, or “measures,” pre-aggregated across nested layers of groupings, or “dimensions.” In the example below, the table of raw purchases is aggregated by month and state. There’s a row for each possible combination (say, January and California, or February and Ohio); each row includes metrics on that pairing, like the number of items sold and the total amount they sold for.

![](https://substackcdn.com/image/fetch/$s_!wVfm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7c82cec5-c107-48bc-a12c-75110fd1eec0_812x558.png)
*An OLAP cube.*

While this table itself isn’t directly useful, people can aggregate it again to produce more traditional reports. If you want to count the total number of purchases, sum the column of items sold. If you want to tally sales in California, filter the table to just the rows where state equals California. And for more complex operations, you can aggregate the data across multiple steps. To find the average number of items sold in a month, first sum the table by month to create a twelve row table of total items sold in each month; then average those totals into a single number. 

This is more complicated than working directly on top of the original table, but it has one enormous benefit: It’s fast. No matter how many orders you process, the OLAP cube in the example above can never be more than 600 rows long—fifty states times twelve months. Computers, even those from decades ago, can easily work with tables of this size.[^2] As a result, most legacy BI tools were centered around OLAP cubes. BI administrators would define the cube in the tool, it would get precomputed on a regular cadence, and everyone else would create reports by manipulating and pivoting the data in the cube. 

OLAP cubes were also limiting, though. Details get lost when raw data is grouped into pre-calculated aggregates. If you wanted to cut your data by a dimension that wasn’t precomputed, or if you wanted to see a list of the six purchases made in Ohio in January, you couldn’t do it. Compressing data, it turns out, isn’t [lossless](https://www.youtube.com/watch?v=NH2lwGzBruM). 

But there’s another, much less discussed downside to OLAP cubes: *They’re very difficult to understand. *Unlike a standard table, an OLAP cube doesn’t describe a tangible concept. It isn’t a list of objects or reportable metrics; each row, rather than representing a straightforward noun, is a combinatorial abstraction. It’s a middleman, an assemblage of computational scaffolding that can only be understood through its relationships to the raw data underneath it and to the reporting needs above it.

This structural weirdness is obvious in any BI tool that’s built on top of an OLAP cube. The screenshot below shows how [Microstrategy presents](https://images.app.goo.gl/xMiQJv2abCQvLqAL7) their cube to its users. It *feels* like building a pivot table in Excel. But look at the fields: Customer, day, item, metrics, % change units sold, cost per unit, last year’s profit. What underlying table would have all of these fields? 

![](https://substackcdn.com/image/fetch/$s_!Jr5T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4a189f21-91c2-4598-8026-6ac3d26f1689_1600x995.png)

No single table, as we typically conceive of one, would. And that’s the problem—OLAP cubes aren’t a normal table. They’re “multi-dimensional arrays of data,” and we can’t intuitively make sense of that. 

This, I believe, is what makes OLAP cubes one of Vicki’s ghosts. You can’t look at one and understand it on its own. It has to be defined in context—in the context of the data it describes, and in the context of the technical limitations that necessitate it.

# The OLAP cube is dead, long live the OLAP cube

None of this was supposed to matter anymore. As Claire mentioned in her post, OLAP cubes are no longer popular. Modern databases like Redshift, Snowflake, and BigQuery are large enough to count (and do much more complex operations) on top of very large datasets. People can now compute metrics on top of raw tables nearly as quickly as they can against OLAP cubes. 

But, like any good ghost, though they may not exist in the physical form, OLAP cubes are spiritually very much alive. 

Consider Looker, for example. Looker was one of the first major BI tools to fully discard the OLAP cube, and run its queries directly against the underlying database. In doing so, however, Looker changed how* *BI tools interact with data, but they didn’t change how people interact with BI tools.

LookML, the configuration language underneath Looker, is, in effect, a recipe for running a query. It defines how raw tables are related to each other, how they should be aggregated to compute various metrics, and the dimensions by which those metrics can be grouped. In an OLAP infrastructure, this configuration would be used to build an OLAP cube; metrics and reports would then be computed on top of that cube. When people use Looker to create a report, it does both of these steps at once. If you ask Looker for sales by month, it creates a query that extracts this metric directly, with LookML providing the instructions for how to do this.

Despite changing the engine, [Looker’s UI](https://docs.looker.com/exploring-data/exploring-data) is the similar to Microstrategy’s. Just as Microstrategy presents fields as collections of dimensions and measures to be explored, so too does Looker. And just as this presentation makes it hard to understand exactly what underlying object is being manipulated by these fields—it feels like a table, but definitely isn’t *just *a table—so too does it in Looker. 

![](https://substackcdn.com/image/fetch/$s_!CB4u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6f8705a2-da50-4020-8219-ed77398a5dc4_1600x1056.png)

The BI landscape is full of interfaces like this. [Tableau presents data](https://www.tableau.com/products/desktop) in this way, [as does Power BI](https://powerbi.microsoft.com/fr-fr/blog/create-a-power-bi-desktop-report-in-minutes/).[^3] Even metrics layers like Transform function similarly: [Configure how your data is structured](https://blog.transform.co/product-focus/metrics-framework/), and present those results [as dimensions and measures](https://blog.transform.co/product-focus/metrics-api/) to be explored.[^4]  

On one hand, this makes sense. A LookML data model, like an OLAP cube or a Tableau extract, isn’t a simple table of users or customers. It’s a more complex data structure, and the UI reflects that. On the other hand, these data structures are confusing, and BI tools don’t have to create interfaces that mirror the architectures that sit underneath them. Rather than displaying data as we model it, we should display data as we use it.[^5]

# What we think of when we think of data

Early in my career, I worked for a think tank in Washington, D.C., where I helped write a weekly newsletter about the global economy.[^6] The posts were quantitative, and required a fair amount of data analysis. We relied on government data sources, and I, as someone who had never even heard of SQL at the time, relied on those sources’ drag-and-drop web portals to get the data I needed. I was, in other words, the business user to the government’s BI tools. 

Two websites easily stood out at the best. The first was [Fred](https://fred.stlouisfed.org/), the data platform of the St. Louis Fed. What makes Fred great is its simplicity: It’s just a [giant list](https://fred.stlouisfed.org/tags/series) of economic indicators. I didn’t know what the data sources behind Fred looked like, nor did I ever care. All I had to do was search for a metric.

The second website I liked was the [IMF’s World Economic Outlook database](https://www.europeandatajournalism.eu/var/ezdemo_site/storage/images/news/useful-data/world-economic-outlook-database-imf/150623-1-fre-FR/World-Economic-Outlook-Database-IMF.png).[^7] Unlike Fred, the IMF site didn’t ask you to choose a metric; it helped you build a dataset. You chose the category of economic statistics you wanted to see—you had choices like trade balances, financial indicators, and GDP and output statistics—and picked the countries and regions you wanted to see them for. The website would then generate a big table for you, with one row per country, and one column per indicator. 

![](https://substackcdn.com/image/fetch/$s_!POMZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F014e4d97-84f0-4d8b-bee2-3f945913860c_1600x1512.png)

![](https://substackcdn.com/image/fetch/$s_!UW-e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1248678-1b1b-4b54-b49c-2a90b1dab62d_801x450.png)
*Where I spent my early twenties.*

These websites were great because they presented data in the two forms that I understood it: As a metric, or as a table. While I’m sure the data underneath my requests was complicated and intertwined—there may have even been an OLAP cube or two in there—these websites hid all of that from me. Their interfaces matched the way I asked questions: Show me this metric, and help me create that list of countries.

Contrast this with the [Census’s website](https://data.census.gov/cedsci/table?q=United%20States), the worst government data portal. The Census offers a bunch of topics for you to “explore.” Because data is organized by source (i.e., the survey from which it was collected) and is presented in nested pivot tables, to use the site, you have to have some understanding of how the Census collects its data, and how different topics are related to one another. Even basic numbers, like population statistics by state, are difficult to find.[^8]

![](https://substackcdn.com/image/fetch/$s_!e_Jb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9db69509-9ee8-4c77-b747-bce4d739a592_1600x886.png)
*Nonoperational analytics.*

Most BI tools look like the Census site. They expose data as it’s defined, through a thin GUI around a complex OLAP cube. A better solution would hide the implicit structure of the data behind the tool, and instead speak the language of those who use it. It would replace wide-ranging explorations with simple methods for finding metrics, and for creating flat, intuitive datasets.

This might seem like a step backwards.[^9] It constrains how much you can explore, and splits unified self-serve interfaces like Looker’s into two separate flows. But, our goal as tool builders—either as those who build products, or those who are creating reports and dashboards for our coworkers—should be to rid what we create of Vicki’s demons, and make things that can be understood with as little implicit knowledge as possible. After three decades of trying with OLAP cubes, it’s time to give up the ghost. 


---


[^1]: The description of this [Gartner report](https://www.gartner.com/en/documents/1739826) is 208 words long. Thirty-eight are acronyms: SQL, SSAS, OLAP, OLAP, SQL, DBMS, BISM, SSAS, SSAS, SSAS, SSAS, OLAP, OLAP, BI, SSAS, OLAP, SSAS, GUI, SSAS, SSAS, BI, SSAS, OLAP, MOLAP, ROLAP, SSAS, MOLAP, ROLAP, UDM, SSAS, SSAS, SSAS, SSAS, SSAS, UDM, SSAS.

[^2]: In practice, of course, most OLAP cubes are considerably larger than this. They contain more dimensions, like day of purchase and item purchased, and more metrics, like average sales price and total tax paid. But, contrary to my long-standing assumption that OLAP cubes were a special computational engine or some incomprehensibly complex matrix of numbers and Greek letters, larger OLAP cubes are structurally identical to the one above.

[^3]: Mode [uses a similar language](https://mode.com/help/articles/visual-explorer) of dimensions and measures. For better or for worse, however, the data underneath Mode’s Visual Explorer is typically a flat table returned by a query rather than a table structured like an OLAP cube.

[^4]: We [beheaded](https://basecase.vc/blog/headless-bi) the OLAP cube, and are now haunted by its ghost.

[^5]: [Conway’s corollary](https://en.wikipedia.org/wiki/Conway%27s_law): The experience of using any product will be a copy of the product’s data structure.

[^6]: It’s Tuesday, let’s talk about [the regional distribution of foreign exchange reserves](https://carnegieendowment.org/2011/05/12/why-are-reserves-so-big-pub-43888).

[^7]: Sadly, they’ve since replaced it with a [much worse version](https://www.imf.org/en/Publications/WEO/weo-database/2021/October).

[^8]: More precisely, with the Census site, it doesn’t feel like you’re finding data as much as you’re *creating* data. And that’s exactly the problem.

[^9]: Not least of all because I’m saying that the most advanced tech companies in the world should take their design cues from a ten-year old “data portal” built by a financial NGO.
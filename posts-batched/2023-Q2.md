# Posts from 2023-Q2

This file contains 13 posts from 2023-Q2.

================================================================================

# The new philosophers

*How the modern data stack falls out of fashion.*

---

![](https://substackcdn.com/image/fetch/$s_!MJaj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec0e828a-a8f9-457f-81f5-4da5dfb655e7_840x360.png)
*Plato, aka the modern data stack, left of center. Aristotle, aka the next new thing, right of center. Socrates, aka Hadoop, not pictured.*

> *What’s the Hadoop of today’s data ecosystem?*

Shortly after I started working in tech in 2012, I attended my first data conference: *[Strata + Hadoop World](https://conferences.oreilly.com/strata/strata-ca-2019/public/content/about.html)*. Held at the San Jose Conference Center, the same venue that hosted Apple’s and Facebook’s developer conventions, it was a confident coronation of the next big thing. Data vendors launched new products to packed auditoriums; customers, in scripted fireside chats, told us how digitally transformed they were; venture capitalists assured us this was just the beginning. The event’s premier sponsors—Cloudera, Hortonworks, and MapR—had raised hundreds of millions of dollars, pitched their software out of huge pavilions in the vendor hall, and were on the fast track to IPOs. Everything was ascendent: The startups, the ecosystem, and, most of all, the [revolutionary promise of Big Data](https://news.microsoft.com/2013/02/11/the-big-bang-how-the-big-data-explosion-is-changing-the-world/).

It didn’t go so well after that. 

Hortonworks’ shares [fell by more than seventy percent](https://finance.yahoo.com/chart/HDP/#eyJpbnRlcnZhbCI6IndlZWsiLCJwZXJpb2RpY2l0eSI6MSwiY2FuZGxlV2lkdGgiOjcuMDkzODk2NzEzNjE1MDIzLCJ2b2x1bWVVbmRlcmxheSI6dHJ1ZSwiYWRqIjp0cnVlLCJjcm9zc2hhaXIiOnRydWUsImNoYXJ0VHlwZSI6ImxpbmUiLCJleHRlbmRlZCI6ZmFsc2UsIm1hcmtldFNlc3Npb25zIjp7fSwiYWdncmVnYXRpb25UeXBlIjoib2hsYyIsImNoYXJ0U2NhbGUiOiJsaW5lYXIiLCJwYW5lbHMiOnsiY2hhcnQiOnsicGVyY2VudCI6MSwiZGlzcGxheSI6IkhEUCIsImNoYXJ0TmFtZSI6ImNoYXJ0IiwiaW5kZXgiOjAsInlBeGlzIjp7Im5hbWUiOiJjaGFydCIsInBvc2l0aW9uIjpudWxsfSwieWF4aXNMSFMiOltdLCJ5YXhpc1JIUyI6WyJjaGFydCIsInZvbCB1bmRyIl19fSwic2V0U3BhbiI6eyJiYXNlIjoiYWxsIiwibXVsdGlwbGllciI6MX0sImxpbmVXaWR0aCI6Miwic3RyaXBlZEJhY2tncm91bmQiOnRydWUsImV2ZW50cyI6ZmFsc2UsImNvbG9yIjoiIzAwODFmMiIsInN0cmlwZWRCYWNrZ3JvdWQiOnRydWUsImV2ZW50TWFwIjp7ImNvcnBvcmF0ZSI6eyJkaXZzIjpmYWxzZSwic3BsaXRzIjpmYWxzZX0sInNpZ0RldiI6e319LCJjdXN0b21SYW5nZSI6bnVsbCwic3ltYm9scyI6W3sic3ltYm9sIjoiSERQIiwic3ltYm9sT2JqZWN0Ijp7InN5bWJvbCI6IkhEUCIsInF1b3RlVHlwZSI6Ik1VVFVBTEZVTkQiLCJleGNoYW5nZVRpbWVab25lIjoiQW1lcmljYS9OZXdfWW9yayJ9LCJwZXJpb2RpY2l0eSI6MSwiaW50ZXJ2YWwiOiJ3ZWVrIiwic2V0U3BhbiI6eyJiYXNlIjoiYWxsIiwibXVsdGlwbGllciI6MX19XSwic3R1ZGllcyI6eyJ2b2wgdW5kciI6eyJ0eXBlIjoidm9sIHVuZHIiLCJpbnB1dHMiOnsiaWQiOiJ2b2wgdW5kciIsImRpc3BsYXkiOiJ2b2wgdW5kciJ9LCJvdXRwdXRzIjp7IlVwIFZvbHVtZSI6IiMwMGIwNjEiLCJEb3duIFZvbHVtZSI6IiNGRjMzM0EifSwicGFuZWwiOiJjaGFydCIsInBhcmFtZXRlcnMiOnsid2lkdGhGYWN0b3IiOjAuNDUsImNoYXJ0TmFtZSI6ImNoYXJ0IiwicGFuZWxOYW1lIjoiY2hhcnQifX19fQ--) in the two years after its IPO. When Cloudera [went public in 2017](https://www.cnbc.com/2017/04/28/cloudera-valued-at-three-times-rival-hortonworks-after-ipo.html), it was worth half its private market valuation. MapR [blew up](https://siliconangle.com/2019/05/30/mapr-may-shut-investor-pulls-following-extremely-poor-results/), and Hewlett Packard Enterprise [bought its “business assets”](https://www.forbes.com/sites/davidteich/2019/08/05/hpe-acquires-mapr-assets-in-an-attempt-to-strengthen-its-artificial-intelligence--machine-learning-portfolio/?sh=18d269e426b4) for an undisclosed sum.[^1] Strata rebranded to *Strata Data & AI*, and then [got canceled](https://www.oreilly.com/conferences/strata-data-ai.html). Hadoop eventually became a one-liner: At [Gartner’s data conference](https://www.gartner.com/en/conferences/na/data-analytics-us) this year, our fling with Hadoop was the punchline of a lot of self-deprecating jokes, like an [embarrassing high school ex that was never good for us](https://www.youtube.com/watch?v=VuNIsY6JdUw). 

Of course, we didn’t discard data entirely; instead, we moved on to a new thing. Hadoop-based or -inspired data systems like [Hive](https://en.wikipedia.org/wiki/Apache_Hive), [Pig](https://en.wikipedia.org/wiki/Apache_Pig), and [Impala](https://en.wikipedia.org/wiki/Apache_Impala) got replaced by cloud data warehouses, which looked like ordinary relational databases, but very big, very fast, and relatively cheap. These products became platforms for [dozens of new categories](https://www.moderndatastack.xyz/categories) of data tools and hundreds of new companies. 

Colloquially, we’ve come to call this collection of products the modern data stack. But that term represents more than just a set of tools; it represents the epochal sequel[^2] to the era of Big Data. MapReduce was hard to write; Hadoop was hard to maintain; the data science initiatives that these tools were supposed to unlock were plagued by unusable data and brittle pipelines. The modern data stack was the reactionary counter-movement to these problems.[^3] That movement includes tools *and* philosophical beliefs—SQL-first, cloud-first, decision support over fancy data science, modular over monolithic—that emerged organically, and were eventually [canonized by dbt Labs](https://docs.getdbt.com/community/resources/viewpoint). Their viewpoint became the industry viewpoint, something something [ZIRP](https://en.wikipedia.org/wiki/Zero_interest-rate_policy), and the “modern data stack”—as an ecosystem of tools and as The Way make data valuable—went vertical.

But every movement, especially one as hyped and frothy as this one, will inevitably get some ideas wrong. Surely, something we’re excited about won’t pan out; surely, something will be our Hadoop.

Over the last eighteen months, I’ve asked a number of people what they think that might be. I’ve gotten a range of answers: ELT, streaming, the centralized warehouse, data catalogs, observability, the [ever-impressive, long-contained, often-imitated, but never duplicated](https://youtu.be/GI27mX1BHvk?t=105) data mesh. Worthy candidates, all, but I can’t help but wonder if the answer is the entirety of the modern data stack. Just as the era of Big Data gave way to the modern data stack, it’s starting to feel like the era of the modern data stack is on the verge of being overtaken by the next counter-movement.

# 99 problems, 15 standards, 1 landscape

[Like every startup pitch deck](https://benn.substack.com/p/metadata-money-corporation#:~:text=There%20are%20a%20few%20mandatory%20slides%20in%20every%20data%20startup%E2%80%99s%20fundraising%20deck.), every data talk has a few mandatory slides in its preamble. For years, one of the mainstays was a chart showing how [data volumes double every two years](https://benn.substack.com/p/why-do-people-want-to-be-analytics#:~:text=Publications%20like%20the,two%20years.). The talk track was always the same: “Businesses are [drowning in digital papers](https://www.youtube.com/watch?v=4_Gf0mGJfP8&t=257s), and we’re building MetaQuery.io to help.” 

No longer. We’ve stopped talking about how data volumes are doubling, and started talking about data *tools *are* *doubling.[^4] Every presentation now opens with a screenshot of [Matt Turck’s MAD landscape from 2012](https://www.slideshare.net/mjft01/a-chart-of-the-big-data-ecosystem)—”This is what customers used to have to choose from.” Then, with a dramatic slide change—“Today, it’s *this*”—they show [Matt’s 2023 landscape](https://mattturck.com/landscape/mad2023.pdf). “And we’re building [MetaQuery.ai](https://xkcd.com/927/) to help.”

The punchline is the entire modern data stack. It’s the now-widespread acknowledgement that there are too many tools, that we’ve created too many thin categories, and that what was meant to be a usable rewrite to Hadoop is now an unnavigable labyrinthine of tools and intertwined costs. If a Big Data platform was too hard to set up, deploying the modern data stack has been, if anything, too *easy*. 

These frustrations have shown up in a number of ways. People complain about tools being disconnected and hard to manage. [Meta-vendors](https://5x.co/) [launch products](https://techcrunch.com/2023/04/05/mozart-data-announces-free-tier-to-encourage-smaller-businesses-to-get-on-platform/) that manage other vendors for you.[^5] There are constant rumblings about how much metered data warehouses cost. Some consulting firms market themselves around helping companies clean up and organize their dbt projects. Data quality is an ever-present problem. And new concepts like [data contracts](https://www.datacouncil.ai/talks/data-contracts-accountable-data-quality?hsLang=en), [active metadata management](https://towardsdatascience.com/what-is-active-metadata-and-why-does-it-matter-add3408c228), and [data control planes](https://www.datacouncil.ai/talks/building-a-control-plane-for-data?hsLang=en) are direct efforts to control—and make a market from—the chaos that the modern data stack can often create. We had 99 problems, so we used the modern data stack—and [now we have 100 problems](https://xkcd.com/1171/).

In fairness, none of these issues are necessarily fatal, or even unexpected. [Progress isn’t linear](https://www.principles.com/principles/689e0214-1e50-4ca0-a5d2-5e223599badf/). We experimented; we found some things that work and some that don’t; we’re experimenting again. The best problem any new technology can have is that [people want to use it too much](https://benn.substack.com/p/how-snowflake-fails#:~:text=More%20succinctly%2C%20demand%20for%20Snowflake%20won%E2%80%99t%20go%20down%20because%20demand%20for%20Snowflake%20went%20up.%C2%A0And%20there%E2%80%99s%20a%20different%20between%20something%20being%20%E2%80%9Cexpensive%E2%80%9D%20and%20people%20buying%20a%20lot%20of%20it.). 

Moreover, most of these new ideas aren’t rejections of the modern data stack’s foundational tenets, but iterations on top of them. Though [attention](https://benn.substack.com/p/internal-tools-make-lousy-startups)-[seeking](https://benn.substack.com/p/should-we-be-grateful) [hecklers](https://benn.substack.com/p/day-of-reckoning) criticize the modern data stack’s edges because there are likes and subscribers to be had by starting fights,[^6] most of us still agree with the modern data stack’s [gospel](https://docs.getdbt.com/community/resources/viewpoint). We complain, but rarely offer an alternative philosophy.[^7] 

Six months ago, I thought this was a steady equilibrium—two steps forward, one cynical blog post back, and messy progress for years to come. But in the last few months, I’ve changed my mind. I now believe we’re in the liminal[^8] space between two eras. In a few years, we’ll see this time as when we faded away from the modern data stack, and moved towards intelligent infrastructures. 

# The next discontinuity

I have a theory that technological cycles are like the stages of *Squid Game*: Each one is almost entirely disconnected from the last, and you never know what the next game is going to be until you’re in the arena. 

For example, some new technology, like the automobile, the internet, or mobile computing, gets introduced. We first try to fit it into the world as it currently exists: The car is a mechanical horse; the mobile internet is the desktop internet on a smaller screen. But we very quickly figure out that this new technology enables some completely new way of living. The geography of lives can be completely different; we can design an internet that is exclusively built for our phones. Before the technology arrived, we wanted improvements on what we had, like the proverbial faster horse. After, we invent things that were unimaginable before—how would you explain everything about TikTok to someone from the eighties? Each new breakthrough is a discontinuity, and [teleports us to a new world](https://twitter.com/bennstancil/status/1631790710381748225)—and, for companies, [into a new competitive game](https://commoncog.com/to-get-good-go-after-the-metagame/)—that would’ve been nearly impossible to anticipate from our current world. 

Artificial intelligence, it seems, will be the next discontinuity. That means it won’t tack itself onto our lives as they are today, and tweak them around the edges; it will yank us towards something that is entirely different and unfamiliar.

AI will have the same effect on the data ecosystem. We'll initially try to insert LLMs into the game we're currently playing, by using them to help us write SQL, create documentation, find old dashboards, or summarize queries. 

But these changes will be short-lived. Over time, we'll find novel things to do with AI, just as we did with the cloud and cloud data warehouses. Our data models won’t be augmented by LLMs; they’ll be [built for LLMs](https://benn.substack.com/p/the-rapture-and-the-reckoning). We won't glue natural language inputs on top of our existing interfaces; natural language will become the [default way we interact with computers](https://www.linkedin.com/posts/benn-stancil_some-personal-news-i-just-wrote-this-blog-activity-7045092475899629569-XFng/). If a bot can write data documentation on demand for us, [what’s the point of writing it down at all](https://rachsmith.com/i-want-good-search/#:~:text=If%20a%20bot%20can%20write%20for%20me%2C%20what%20is%20the%20point%20of%20me%20writing%20in%20the%20first%20place%3F)? And we're finally going to deliver on the promise of self-serve BI in ways that are profoundly different than what we've tried in the past.[^9] 

As these changes—and dozens of others that we can’t anticipate—start to come into focus, a new set of philosophical beliefs will likely coalesce around them. We’ll figure out [new ways to structure](https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c) AI-powered technology stacks and AI-enabled data teams; at some point, as the good ideas separate from the bad, someone will [pin a new set of theses](https://en.wikipedia.org/wiki/Ninety-five_Theses) to the modern data stack’s door. 

If this happens, the tenets of the intelligent infrastructure could quickly outpace those of the modern data stack as the next new thing. AI technologies are advancing at a breakneck pace, and they've already captured the imagination of the enterprise data ecosystem in ways the modern data stack still hasn't. For the Fortune 500 buyers at the recent Gartner conference, the modern data stack is an up-and-coming flirtation. LLMs, by contrast, were already everywhere, with vendors and CIOs scrambling to adopt it.

# Evolve, or die

The week after Gartner's conference, I attended [Data Council](https://www.datacouncil.ai/) in Austin. At one point, I found myself in a conversation with a newly-minted founder who told me they were excited about their company's potential because it could ride the momentum of generative AI and of the modern data stack.

On AI, yes, of course. ChatGPT proved to be the only thing that could replace Elon Musk as the permanent main character on Twitter;[^10] weeknight AI meetups in San Francisco are now better attended than major data conferences. For better or for worse, there’s no shortage of momentum in AI. 

But the founder’s comment about the modern data stack struck me as something between anachronistic and contradictory. Though I don't believe the tools that make up the modern data stack will fail, the modern data stack *as a movement* seems incompatible with the rise of AI. It's a philosophy that was designed for a world in which reasoning through a data problem with a robot was a fantasy. That philosophy may be no more suitable for the world run on LLMs than MapReduce is for a world run on Snowflake.[^11]

That’s is the reality that data teams and data vendors both have to reckon with. Our world is changing; the future we’ve been fighting to create may soon become the past that we have to fight to escape. For the modern data stack, [this is it](https://twitter.com/hspter/status/1640883350603980801).


---


[^1]: And for a $69.99 negotiation fee, [GoDaddy will help you negotiate](https://www.godaddy.com/domainsearch/find?checkAvail=1&domainToCheck=mapr.com) to buy the domain mapr.com.

[^2]: Get it???

[^3]: I’ve [joked before](https://benn.substack.com/p/how-fivetran-fails#footnote-2-73615268) that the best definition of the modern data stack is data tools that launched on Product Hunt. It’s not an entirely accurate definition—Snowflake was never on Product Hunt, for example—but it gets at the right idea for me: That the modern data stack isn’t an actual stack, but the era when data tools become cloud-first, bottoms-up, and community-oriented. This definition also excludes tools like Oracle’s Autonomous Data Warehouse and PowerBI, which I suspect most people would say *aren’t* clear members of the modern data stack.

[^4]: YC’s Law: The number of data companies in YC [doubles every two years](https://benn.substack.com/p/the-data-os#:~:text=In%202017%2C%20Y%20Combinator%E2%80%94an%20incubator%20of%20both%20startups%20and%20the%20Silicon%20Valley%20zeitgeist%E2%80%94funded%2015%20analytics%2C%20data%20engineering%2C%20and%20AI%20and%20ML%20companies.%20In%202021%2C%20they%20funded%20100.).

[^5]: I’m a [personal investor ](https://benn.substack.com/p/disclose-your-angel-investments)in both 5x and Mozart Data.

[^6]: [Get rekt](https://astralcodexten.substack.com/p/a-cyclic-theory-of-subcultures#:~:text=go%20homesteading%20in.-,The%20only%20source%20of%20status%20is%20to%20seize%20someone%20else%E2%80%99s%20%2D%20ie%20to%20start%20a%20fight,-.), [@bennstancil](https://twitter.com/search?q=from%3A%40bennstancil%20friday%20fight&src=typed_query&f=live).

[^7]: You could potentially argue that the data mesh is [an alternative ideology](https://martinfowler.com/articles/data-monolith-to-mesh.html). I don’t think I’d quite agree with that; to me, it feels more like a recommendation on how to make the modern data stack scale within very large companies. But some people may see that differently.

[^8]: I swear, I went thirty-some odd years without knowing that this word existed, and now it’s [everywhere](https://trends.google.com/trends/explore?date=all&geo=US&q=liminal&hl=en).

[^9]: Given that my day job is to build a [BI tool](https://mode.com/), I’ve thought way too much about this one. A longer post for a different day.

[^10]: It’s unclear who will destroy humanity first.

[^11]: Of course, it’s also possible that AI is a flash in the pan; that the modern data stack is the tortoise that’s still going to win the race; and that now is the perfect time to build boring analog products when all the competition is distracted by something else. During a gold rush, sell pickaxes—or stay in Ohio, and be the only carpenter in town.

================================================================================

# How dbt succeeds

*Peacetime dbt, wartime dbt, and an alternative third thing.*

---

![](https://substackcdn.com/image/fetch/$s_!QsNL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ecad335-a8ba-4e04-9a77-8eb677c87e2d_960x430.png)
*[Wartime dbt comin’.](https://www.youtube.com/watch?v=iMm1Wih0kug)*

Of all the characters I’ve met in Silicon Valley’s data scene, one person will always have my undying loyalty: [Peter Fishman](https://www.linkedin.com/in/petefishman/). 

Fish ran the data team at Yammer, and was my first boss in the tech industry. He was uninterested in glittery technology, didn’t care about the corporate charade, and, despite always being the smartest person in the room, was never once taken by self-aggrandizing puffery. Instead, he led via adage, analogy, and a relentless push to do practical things. Like a great basketball coach,[^1] he gave us a philosophy to follow—which came to be known, colloquially at first and eventually as a published document, as the Fishman Playbook—and a license to freestyle once we were on the court. Working with Fish was like working with Norm McDonald: A subtle master of his craft who took you up long climbs and down short descents [that were always worth the wait](https://www.youtube.com/watch?v=jJN9mBRX3uo). 

But my loyalty to Fish runs deeper than owing him credit for a bunch of stolen ideas; I owe him credit for the entirety of my career. In early 2012, I was stuck in a dead-end job as junior researcher at a think tank in Washington, D.C. I was six months and 58 applications into looking for a new job, and had yet to land a single offer. On application number 59—which was made possible [by a referral](https://benn.substack.com/p/analytics-is-at-a-crossroads#:~:text=I%20was%20nearly%20one%20such%20casualty.)[^2]—I lucked into an interview to work on Fish’s team. 

Despite my dubious resume, my unproven skill set (not only did I not know SQL, I didn’t know what it was), and what was surely an alarming series of conversations during my onsite, Fish took a chance on me. He pulled from the path I was on—slumping towards a job as a bitter Hill staffer with no career prospects, destined to take my entitled frustrations out on the American people by gumming up the legislative process with petty acts of bureaucratic sabotage—and bent me towards the life I now have. If you’re a subscriber to this blog, a customer of Mode, or simply an American citizen who’s grateful that there isn’t someone trying, purely out of misanthropic spite, to require that all tax forms get submitted via fax, you too owe a debt to Fish. 

Though Fish is singular, everyone has some version of Fish in their lives. We all have the coach who gave us a shot, the school that said yes, or the professor who turned an aimless elective into a lifelong passion. In the long arcing curves of our lives, we remember the sharp upward turns.

Fish put such a turn in my life. For people all over the data industry, dbt Labs put a turn in theirs. 

No matter how you feel about dbt Labs, its impact on people's lives is undeniable. dbt, and the analytics engineering profession that followed it, plucked thousands of people out of jobs they didn't like and [into ones they did](https://twitter.com/chetchavat/status/1525188296908865540). It got people [paid](https://benn.substack.com/p/the-technical-pay-gap#footnote-4-56820702). It encouraged them to [operate at the top of their license](https://roundup.getdbt.com/p/saying-true-things-is-hard#:~:text=operating%20at%20the%20very%20top%20of%20my%20license), in [their flow states](https://www.aranke.org/dbt-jquery/). It created a community [that celebrated them](https://tayloramurphy.substack.com/p/alone-together-in-the-data) for who they were and what they did.[^3] 

Some people may disagree with the corporate benefits of all of this—perhaps dbt isn’t the right architecture; perhaps analytics engineers aren’t operating at the top of their license, but above it, overpaid and out of their depth; perhaps workers are unwise to be loyal to a company that’s legally obligated to love its shareholders more than its fans. Maybe; time will tell. But to the people who've found better jobs, higher salaries, and a professional home, that’s irrelevant. I'm grateful to have been rescued by Fish, even if the data industry would be better off without [yet another BI tool](https://mode.com/) and yet another white guy with Ideas and a Blog;[^4] a small army of analytics engineers is rightfully grateful to dbt Labs, even if modern data stack goes bust. Though dbt Labs' success is impressive, this bubbling appreciation from its users is what makes it truly unique. 

This isn't to say that dbt Labs’ job has been easy. Nothing about hiring hundreds of people, supporting [16,000 customers](https://www.getdbt.com/dbt-labs/about-us/), and chasing extraordinarily ambitious revenue targets, all while trying to be the careful custodians of a popular community, is easy. Building a company at this pace is a [marathon of sprints](https://twitter.com/josh_wills/status/1595895824713596928). That’s really hard, no matter how enthusiastic your cheering section.

Still, for most of their seven-year run, dbt Labs has been running through supportive crowds, over roads that tilts downhill. The coming miles, however, are full of hills and deserted streets. For dbt Labs to win the next stretch of the race, its tactics may need to change.

# An uneasy peace

It's always been a bit of a mystery to me why more companies haven't challenged dbt. Despite solving a widespread problem, [occupying strategically important technological territory](https://benn.substack.com/p/powder-keg), and having a [relatively thin technical moat](https://benn.substack.com/p/how-dbt-fails#footnote-anchor-10-77057156:~:text=At%20its%20core%2C%20dbt%20is%20a%20relatively%20thin%20piece%20of%20technology%20(and%20an%20open%2Dsource%20one%20at%20that)), dbt Labs has historically had relatively few direct competitors. The explanation, I'm coming to realize, is obvious: Why bother competing with it? The explosion of interest in data tooling created lots of opportunities in adjacent spaces—observability, discovery, orchestration, semantics, activation, contracts, mesh—that don’t have dominant incumbents. In a world where companies could raise money on narrow solutions, don't rip at the prom king's coattails. Ride them.

dbt Labs has been a clear beneficiary of this implicit truce. Every data tool wanted to surf on dbt’s success. BI tools [launched](https://mode.com/integrations/get-dbt/) [integrations](https://www.thoughtspot.com/partners/dbt) with it; orchestration vendors [ran](https://docs.dagster.io/integrations/dbt) [it](https://www.prefect.io/partner/dbt/); metrics layers [augmented it](https://cube.dev/blog/dbt-metrics-meet-cube); databases [remarketed](https://quickstarts.snowflake.com/guide/data_teams_with_dbt_cloud/#0) [it](https://docs.databricks.com/partners/prep/dbt.html). This didn’t just keep dbt Labs protected from competition; it also made everyone else a tacit reseller of dbt.

Since the market fell apart last year, however, these dynamics are compounding in reverse.

Across the entire economy, data teams are taking a more serious look at how they’re spending money. This puts a lot of pressure on the narrow data tools that happily integrated with dbt: They have inflated valuations, a deflating addressable market, and a ticking clock counting down how long they have to make up the difference. In that context, the budget spent on dbt isn’t a line item to piggyback on; it’s a target—and especially so if startups sense that there are [cracks in dbt’s armor](https://twitter.com/thebmbennett/status/1646194750817550346). Everyone’s friends [until we’re hungry](https://benn.substack.com/p/disclose-your-angel-investments#:~:text=money%20will%20begin%20to%20dry%20up.%20Sooner%20or%20later%2C%20the%20stakes%20of%20who%20wins%20and%20who%20loses%20will%20get%20very%20real%2C%20and%20congenial%20coopetition%20will%20turn%20into%20cutthroat%20competition.), desperate, and there’s [blood in the water](https://benn.substack.com/i/77057156/blood-in-the-water). 

Though there haven’t been major collisions yet, troops are amassing on the borders. There’s now a [dbt for the enterprise](https://coalesce.io/), and a [dbt with better developer ergonomics](https://sqlmesh.com/).[^5] There are startups bringing dbt’s design principles to data assets that aren’t in your warehouse.[^6] There are a handful of DAG-based orchestrators that claim to be better at running dbt jobs than dbt Cloud. And just two weeks ago, Looker announced that it was ([predictably](https://benn.substack.com/p/bi-is-dead#:~:text=When%20Looker%20got%20acquired%2C%20I%20thought%20Google%20would%20eventually%20acknowledge%20this.)) spinning out LookML into an [independent semantic layer](https://cloud.google.com/blog/products/data-analytics/introducing-looker-modeler), putting the final nail in the once-congenital partnership between Looker and dbt. 

So what should dbt Labs do? As I see it, they have three options.

## Peacetime dbt

I’ve [consistently been skeptical](https://benn.substack.com/p/how-dbt-fails#:~:text=dbt%20Labs%E2%80%99%20original%20business%20model%20was%20built%20around%20selling%20seats%20on%20dbt%20Cloud%2C%20which%20clearly%20understated%20the%20value%20that%20dbt%20provides.) that dbt would be able to meaningfully monetize open-source data infrastructure technology by selling seats to a SaaS product—and I’ve consistently been proven wrong. By all accounts, dbt Labs is still growing at a remarkable clip on what can no longer be dismissed as a small revenue base. In a time when [Slack communities have become passé](https://clrcrl.com/2022/05/06/mds-company-slack.html), dbt Labs’ continue to grow; in a year when travel budgets are getting ruthlessly gutted, Coalesce, dbt Labs’ conference, has, [thus far](https://benn.substack.com/p/all-in-one-place), kept its character as a must-attend [celebration of the community and its common interests](https://benn.substack.com/p/delirium).

In other words, it ain’t broke; don’t try to fix it; [as someone other than Gandhi once said](https://apnews.com/article/archive-fact-checking-2315880316), people fighting you is the penultimate step to you winning. 

Still, even if these trends hold, I’m not sure that dbt Labs can keep walking the exact same path it’s on. Thus far, I think dbt owes a lot of its success to being unapologetically human. Rather than trying to design some perfect semantic abstraction, it built data models by and for people—they’re written in simple SQL, and create simple tables. There’s no complex configuration language, no [virtualized OLAP cube](https://benn.substack.com/p/ghosts-in-the-data-stack), no nested or abstract data structure. 

I’m increasingly becoming convinced that the entire modern data stack is on the cusp of getting rebuilt [for our AI overlords](https://benn.substack.com/p/the-new-philosophers). If that happens, data models won’t get built for people, [but for LLMs](https://benn.substack.com/p/the-rapture-and-the-reckoning). LLMs, however, probably won’t write pure SQL, which is riddled with complexity; instead, I’d expect them to run through semantic models like LookML. People ask the LLM a question; it creates a “semantic query;” a semantic layer complies that request into a SQL query. 

This structure would favor a semantic layer like [Malloy](https://github.com/malloydata/malloy)—tricky to understand, unnatural to query, but extremely flexible—over one built on top of human-readable tables. Fortunately for dbt Labs, [Transform](https://www.getdbt.com/blog/dbt-acquisition-transform/) gives them a path to building a similar product. So who knows, maybe Transform is dbt Labs’ Instagram: A sensible pairing today, and a tactical stroke of genius tomorrow.  

## Wartime dbt

If Transform turns out to be a brilliant bet, it won’t be dbt Labs’ first such move. Say what you will about the nascent dbt Semantic Layer, but its rollout was one of the shrewdest moves in the data industry in the last decade.[^7]

Just as interest in universal semantic layers (aka [metrics layers](https://benn.substack.com/p/metrics-layer) aka [headless BI](https://basecase.vc/blog/headless-bi)) was starting to heat up—[five companies](https://docs.google.com/presentation/d/1D4j-xPxcB-fjGcY7aTu7F5pCDOJBMTpXcZ6v56pXd0A/edit#slide=id.g7f33e18031cdf7e4_1637) had recently entered the space, with at least a couple more in stealth—dbt Labs [announced their plans](https://github.com/dbt-labs/dbt-core/issues/4071) to build a competing product. In doing so, they froze the market. Data teams, many of whom were already using dbt and liked it, decided that it didn’t make sense to bring on an independent metrics store if dbt would be offering one soon. Other startups couldn’t compete with dbt’s offering, because there was no concrete offering to compete against. And venture capitalists, concerned about dbt steamrolling the market, soured on the whole scene. This cleared the road for dbt—today, only one of the five companies is still working on the same product. If semantic layers end up being prized real estate—and especially if Transform keeps dbt relevant in our AI-overrun future[^8]—put this in a business school case study. 

dbt Labs could run this play again, in every adjacency. Plant a flag in the orchestration space, in data discovery, in observability, in contracts, in the mesh, and try to annex them all. Buy Dagster.[^9] Subsume Malloy. Buy Preql to give business users a means for creating their own metrics in dbt.[^10] Take on the entire transformation layer; bury every drag-and-drop orchestration tool with a visual designer embedded in dbt Cloud. Go to the mat with Informatica, and [its $1.5 ](https://www.informatica.com/about-us/news/news-releases/2023/02/20230208-informatica-reports-fourth-quarter-2022-financial-results.html)*[billion](https://www.informatica.com/about-us/news/news-releases/2023/02/20230208-informatica-reports-fourth-quarter-2022-financial-results.html)*[ book of business](https://www.informatica.com/about-us/news/news-releases/2023/02/20230208-informatica-reports-fourth-quarter-2022-financial-results.html). Just as Snowflake [declared war](https://news.yahoo.com/snowflake-ceo-why-you-must-declare-war-on-your-competitors-183109540.html) on everyone who wants to store and process data, dbt Labs could do the same to every company that wants to prepare and transform data.

To be sure, they don’t need to do all of this at once, [head down and guns blazing](https://youtu.be/usYvefDzOqQ?t=65). But as their entry into the semantic layer illustrates, there are gambits here that, if played right, could keep dbt Labs firmly balanced on two feet, and put everyone else on their heels. 

## Take the money and (dbt) run

dbt Labs’ third option is avoid the question entirely, and sell the company.

This isn’t exactly a novel suggestion. The possibility of one of the big database vendors buying dbt Labs gets [floated periodically](https://win.hyperquery.ai/p/snowflake-acquires-dbt). It’s sensible enough: Databases benefit from transformation tools; buying one as popular as dbt would let them siphon more workloads into their platform. 

It’s also not surprising that a deal hasn’t gotten done. dbt Labs is currently valued at a premium not because of its technology, but because of its loyal community following and the future revenues that VCs assume that community can generate. For prominent vendors like Snowflake or GCP that already have relationships in every corner of the data industry, that community probably isn’t a monetizable asset. Rather than spending billions to own dbt Labs, they’re likely better off spending a few million dollars to be a supportive partner. And if they ultimately decide they want to offer their own native transformation layer, there are [much cheaper options out there](https://dataform.co/blog/dataform-is-joining-google-cloud).

For dbt Labs, the math is inverted. So long as the trajectories of the product and community are healthy, stay the course, and work with everyone. It’s a gamble, though: If the plan is to IPO, dbt has to convert its community into cashflow at some point. Wall Street doesn't care about how much goodwill dbt Labs has; it doesn’t care about Slack members, or Github stars, or meetups hosted. It cares about how much cash dbt Labs can crank out. 

Which brings me to Databricks.

As best I can tell, Databricks is running a clear second to Snowflake in the war to replace Oracle as the leader (not named Microsoft) in the enterprise database market. My naive guess is that Databricks’ problem is its branding—it’s historically been seen as an [overly-complicated data science platform](https://benn.substack.com/p/the-end-of-big-data), and not a meat-and-potatoes cloud data warehouse. To overtake Snowflake, it doesn’t need technology; it needs mindshare. It needs sales opportunities. It needs to feel like a safe default. 

Nobody has the mindshare of dbt Labs. In dbt Labs, Databricks would be buying tens of thousands of warm introductions to prospects that may otherwise never give them a look. They’d also be buying legions of enthusiastic users who are, I suspect, more loyal to dbt than they are their warehouse. 

For dbt Labs, Databricks offers a few things in return. First and foremost, Databricks would value dbt Labs *for what it is today*. There’s a long road ahead for dbt Labs to extract revenue from its community; unlike other buyers, Databricks would likely want that community, and not just what it might become.

Second, Databricks solves the riddle of dbt Labs’ business model. Databricks can offer dbt as a free, unmetered service. It wouldn’t care if you use the open-source version or dbt Cloud, nor would it worry about how many seat licenses you buy. This frees up dbt Labs to focus on what it does best—driving adoption of dbt’s core services.

Finally, if the future of data runs through semantic LLMs, Databricks and dbt Labs are [better positioned](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) than anyone to own that revolution. Done right, merging the two products doesn’t just derisk dbt Labs and put Databricks on the database map; it puts the two companies on a map of their own, in the middle of [one the biggest revolutions in technology in decades](https://www.wsj.com/articles/bill-gates-says-artificial-intelligence-is-the-most-revolutionary-technology-in-decades-75fb8562).

I have no idea which path is dbt Labs’ best bet—such is the luxury of being a talking head on the sidelines. But to admit my biases, I hope that they continue to succeed, just as I hope that Fish continues to succeed. However right or wrong their philosophies may be, both have transformed a lot of lives for the better. And that’s a thing worth rooting for. 


---


[^1]: Like, say, Dean Smith or Roy Williams.

[^2]: By, it turns out, San Francisco’s most lynch-curious resident.

[^3]: Of course, the community around dbt Labs [isn’t perfect](https://benn.substack.com/p/who-is-the-community); nothing of its size is. But there are much darker paths it could’ve gone down. To paraphrase Winston Churchill, the data community is the worst collection of people in the tech industry, except for all the others.

[^4]: Just wait until I [start a Podcast.](https://www.nytimes.com/2023/03/06/style/dating-men-with-podcasts.html)

[^5]: They aren’t marketed exactly as this, but the implications seem clear.

[^6]: I.e., [dbt-excel](https://dbt-excel.com/), but for real.

[^7]: I should also say I have no idea if this was intentional or not.

[^8]: Funnily enough, dbt Labs’ acquisition of Transform also* *froze the market. Though the initial reviews of dbt Labs’ semantic layer [were tepid](https://pedram.substack.com/i/71846283/dbt-metrics), Transform injects new promise into the product. For customers, the choice today is the same as it was a year ago: Wait for an upcoming release from a trusted vendor, or invest in a new product that may be redundant and inferior in a few months or quarters?

[^9]: [I’m a personal investor in Dagster](https://benn.substack.com/p/disclose-your-angel-investments), and am obviously rooting for this outcome because if dbt Labs buys Dagster for a cool one billion dollars, I will make just enough money to be able to buy [a brand new Mazda 3](https://www.mazdausa.com/vehicles/mazda3-sedan). Start a company to start a blog to invest in a company to engineer an acquisition to buy a mid-tier sedan without spending a dime? *That’s* orchestration, baby.

[^10]: [I’m also a personal investor in Preql](https://benn.substack.com/p/disclose-your-angel-investments), though I haven’t yet made any plans for those winnings. Upgrade my Mazda 3 to a 2.5 Turbo Premium Plus, I guess.

================================================================================

# “Am I the Jared Kushner?”

*Why analysts should do less analysis. Also, content warning: A picture of Jared Kushner.*

---

![People Could Not Stop Making Jokes During Jared Kushner's Rare Public  Address](https://substackcdn.com/image/fetch/$s_!6-om!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F332cb182-0c95-4cf0-9b1f-7ab3d53f9ad2_1200x797.jpeg)
*I warned you.*

I don't use Reddit. But every once in a while, something leaks out of its fermenting grounds into one of the [clearance bins of cheap content](https://www.youtube.com/watch?v=us52N76XA28) that I frequent. Usually, it’s an [“NBA” “highlight”](https://www.reddit.com/r/nba/comments/12msvx0/tnt_crew_talks_about_the_mavs_tanking_for_lottery/), some meme that [Elon Musk stole](https://twitter.com/therecount/status/1536820115228049409), or—relatedly—the question, "Am I the Asshole?"

[r/AmITheAsshole](https://en.wikipedia.org/wiki/R/AmItheAsshole), or AITA, is a subreddit where people explain some interpersonal conflict that they’re currently dealing with, and ask if they’re the person who’s being a jerk. As best I can tell, the canonical form of this is [a man asking the internet](https://www.reddit.com/r/AmItheAsshole/comments/uma6fa/aita_for_making_my_girlfriend_leave_the_country/) to tell him that the terrible thing he’s doing to his girlfriend isn’t terrible.

To the best of my knowledge, Jared Kushner has never posted on AITA. There is no need for him to; he is, rather obviously, [an asshole](https://www.nytimes.com/2020/10/26/us/politics/kushner-black-racist-stereotype.html). He is, however, one of a particularly refined form: Superficially educated, supremely confident of his own middling abilities, and willing to [inject himself into any problem](https://www.businessinsider.com/what-does-jared-kushner-do-in-trump-administration-2017-4) because, he assumes, his intellect and level head will cut through the petty emotional arguments of those on the ground. He’s [Clark from Good Will Hunting](https://www.youtube.com/watch?v=LMD2vUErcYU), with worse hair. His purest form[^1] is a man who reads [two dozen books on the Middle East](https://nymag.com/intelligencer/2020/01/kushner-can-make-mid-east-peace-because-hes-read-25-books.html),[^2] and sets off to solve a [fifty-year-old conflict](https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict) by, first and foremost, [ignoring history](https://twitter.com/cnn/status/1222267596210343940). 

It's a pretty lousy person to be: A drive-by expert, armed with cursory research, data, and “first principles," here to tell everyone else that they’re too irrational to understand. Don’t be, I think we can all agree, the Jared Kushner.

But for those of us who work as analysts, are we so sure we *aren’t* Jared Kushner? And more acutely, are we so sure we don’t see our entire job—to deploy ourselves into other people’s problems, armed with an objective eye and empirical coolness—to be a (nicer, kinder) Jared Kushner? 

Take, for example, Katie Bauer’s [excellent post](https://wrongbutuseful.substack.com/p/elbows-of-data) on how data teams can elbow their way into positions of influence. It’s good, practical advice, from a qualified and compassionate source. Yet, it isn’t *that* far from recommending we all act as (nicer, kinder) Jared Kushners, showing up in conversations we weren’t invited into, on the belief our skills and our spreadsheets bring something that was heretofore missing. 

To be clear, this isn’t an indictment of the post or of Katie; she’s one of the best out there, and her advice is as close to canon as we data people have. Instead, it’s an indictment of the nature of our job (or at least, flagging that nature as a person of interest). Because if we anonymize the hypothesis—instead of Jared Kushner showing up with research and quantitative reasoning to help make a decision, it’s someone more banal—I’m not sure that we wouldn’t say that that intervention is Right, Good, and, in fact, the very thing we are paid to do.[^3]

So, are we all analysts, or are we all the Jared Kushner? What do we do with this new bit of [data team existentialism](https://benn.substack.com/p/the-case-for-being-biased#:~:text=In%20my%20recent%20descent%20into%20data%20team%20existentialism)? 

# An article a day keeps Jared away

Like AITA, AITJK prods at a fuzzy line. There is value in new perspectives. Experts aren't always right, people get trapped under groupthink, and, perhaps even worse, get enamored with their own ideas.[^4] The *New York Times*’ Spelling Bee game lets you shuffle the letters for a reason[^5]—sometimes, the best ideas come from fresh eyes and a new perspective. 

On one hand, then, perhaps being a (nicer, kinder) Jared Kushner isn’t actually such a bad thing. Out-of-the-box ideas—and a bit of bedside manner—could go a long way. 

On the other hand, I think it’s worth thinking a bit more deeply about why we flinch at Jared Kushner skydiving into various problems that have been worked on by experts for years (or centuries). Because if I’m honest with myself, reading 24 books on something *does feel* like a pretty comprehensive crash course. Graduate students [read about two hundred pages](http://alex.halavais.net/question-how-much-reading-for-a-graduate-class/) a week; if the average book is three hundred pages, 24 books is 36 weeks of classes. That’s not so bad!

What does seem bad, however, is how it’s consumed. To understand something as textured as the Middle East, you can’t shotgun a bunch of text books; you have to steep yourself in it. You have to digest it, day in and day out. A crash course, no matter how intense and thorough, would be a poor substitute for just reading the news for twenty years. Unstructured immersion, I’d argue, is a far better teacher than a rigorous sprint.[^6]

This same principle can be applied to data teams and their roles within their organizations. We often act as though the most impactful thing we can do is sit alongside other departments and support them in the decisions they have to make. Dashboards and reporting are busywork; strategic research is our highest calling. 

This work might *feel* helpful. It might even *feel *heroic. People may even be politely thankful and gracious for our support. But when they [don’t listen](https://davidsj.substack.com/p/dear-stakeholder) or don’t act on our recommendations, it may be for the same reason that career diplomats roll their eyes at Jared Kushner: Our involvement just isn’t that useful, and we’re mostly saying things they already know.

Fortunately, this potentially suggests a better way forward. Rather than trying to help with explicit decisions, what if we focused most of our efforts on creating as informed companies as possible? What if our mission was to make everyone as aware of the shape of the business—of how it’s growing, of where it’s struggling, of what this user segment thinks about that product, of how the market is shifting—as the CEO? I’d argue that a well-informed company, defined by this kind of atmospheric awareness, would make far better decisions than one supported by even the sharpest consultants.[^7]

# Decision-making dies in darkness

Importantly, this isn’t a suggestion to build more reports and dashboards. Dashboards lack narratives, and those narratives are often more important than the metrics themselves. Memorizing [employment numbers](https://fred.stlouisfed.org/graph/?g=12HqO) tells you a bit about the economy; reading [monthly commentary on those numbers](https://www.nytimes.com/2023/04/07/business/economy/march-jobs-report-unemployment.html) tells you a lot. 

Moreover, reports and dashboards are often targeted and pulled based on need, rather than broad and pushed out to passive readers. A product manager might look up how some feature is used by different user personas, but a sales rep likely won’t. Leaning on dashboards to keep people informed is like leaning on Google to understand the news: It can be very effective for the questions people ask, but only the questions they ask. 

So what do we do instead? Stephen Bailey gave us the solution a year ago: [an actual newspaper](https://stkbailey.substack.com/p/product-sketch-the-new-corp-times). Instead of being researchers and scientists who make the news, data teams could be journalists and editors who report on it. Though they would still work alongside other teams to make decisions, their primary role in that process would be to publish what happened. It would be to keep people informed about how the finance team is currently modeling churn, about an unexpected dip in an operational KPI, or about an interesting finding that a CSM uncovered when researching one of their customers. And each update could be put in context with the others, and with the trends across the company.

In one uncomfortable twist, this sort of data team would probably have to focus on the quantity of what they published, and not the quality. Reading the news works in the same way that pointillism works. No single element matters, but viewing them in their totality, even if lots are missing, paints a very clear picture.[^8] Keeping people informed about a business is likely best done in the same way: Constant and steady updates—that are stories, and not just readouts of the same metrics—delivered with the assumption that nobody reads everything. 

This type of data team would also solve two other nagging problems. First, it works *better* in a post-AI world. If ChatGPT is the final form of self-serve BI—which I think it will be; more later—people won’t need a (nicer, kinder) Jared Kushner because they’ll be able to do their own research. But, one downside of replacing both dashboards and analysts with chatbots is that people will get even more isolated in their news bubbles, and what they know will become even more dependent on what they ask. Without curation, publication, and distribution, this could, counterintuitively, make a company *less* informed. The data news organization solves this. 

Second, the mission of creating a well-informed company comes with—[at long last](https://roundup.getdbt.com/p/os-layer-not-application-layer-how)—an easily quantifiable way to measure success: [a news quiz](https://www.nytimes.com/spotlight/news-quiz). Data teams could publish a weekly quiz about recent updates, about the current trends of key metrics, and about the various operating assumptions that exist around the company. The goal of the team is, above all else, to improve quiz scores.

Would it work? I’m not sure—but if I had to bet on a race between one company that was broadly uninformed but well-researched on a few key decisions, versus one that was well-read about its business but had to make most of its decisions on that awareness, I know which one I’d take. And if nothing else, the second company would have a lot fewer Jared Kushners. 

# AMITJK, industry edition

One could argue, without too much strain, that the data industry itself has gone full Jared Kushner recently. Over the last ten years or so, we've simplified so much data technology that becoming an analyst or data engineer no longer requires any technical abilities beyond a basic working knowledge of SQL. This opened the industry’s doors to a lot of people—me very much included—who could work their way up their career ladders while completely ignoring the history of what came before them. 

That’s not necessarily a bad thing; new perspectives, even if they come from vantage points that can’t see the past, can bring a lot of new and interesting ideas. But it’s an interesting dynamic, and one of the reasons we keep papering over old concepts with new branding and calling ourselves inventors. 

# AMITJK, personal edition

One could also argue, with even less strain, that I am a Jared Kushner, hectoring the internet about subjects I’ve Googled about twice and read about once. 

To which my defense is…uh, ok, never mind, that’s fair.


---


[^1]: Ok, his actual purest form is a [two-bit slumlord](https://www.nytimes.com/2022/09/23/us/politics/jared-kushner-apartment-settlement.html), but I'm taking some artistic liberty here.

[^2]: Which, honestly, I doubt. He probably listened to [25 YouTube talks about books](https://twitter.com/typesfast/status/1557779777498427392) on the Middle East while riding a Peloton.

[^3]: For example: In 2014, we hired our first marketer at Mode, who’d been thinking about building and marketing products for premier brands for nearly ten years. I, on the other hand, had been working on Mode for nine months, most of which I’d spent writing [decidedly](https://mode.com/blog/has-youtube-miley-cyrus-peaked/) [unprofessional](https://mode.com/blog/mapping-marijuana-prices/) [articles](https://mode.com/blog/one-hit-wonders/) for our corporate blog. My knowledge of marketing was entirely as a consumer of consumer ads: I liked some [Nike](https://www.youtube.com/watch?v=rHHMaiNyztk) [ads](https://www.youtube.com/watch?v=meVTld5GvbM) and that first iPhone launch seemed good. When she joined, she told me that the future of marketing was content marketing. I was, as analysts are often taught to be, skeptical. Wouldn't people see through it? Didn't we need to sell the product more directly? Couldn't we just launch Mode on a stage in Cupertino and then make cool ads with Tiger Woods? How would we measure the success of content campaigns that were meant to increase people’s ever-immeasurable awareness of our brand? (In 1944, the spy organization that eventually became the CIA [published a guide](https://www.openculture.com/2015/12/simple-sabotage-field-manual.html) for how destroy organizations from the inside called the *Simple Sabotage Field Manual*. If the CIA were to create one for data teams, constantly demanding that we need a better way to measure success would be on the first page. Also, [this is apparently an entry](https://www.openculture.com/2015/12/simple-sabotage-field-manual.html#:~:text=Talk%20as%20frequently%20as%20possible%20and%20at%20great%20length.%20Illustrate%20your%20%E2%80%9Cpoints%E2%80%9D%20by%20long%20anecdotes%20and%20accounts%20of%20personal%20experiences.) in the actual guide: “Talk as frequently as possible and at great length. Illustrate your ‘points’ by long anecdotes and accounts of personal experiences.” Anyway, back to my 300-word-footnote about something that happened nine years ago.) I saw these questions as my job. In hindsight, I’m not sure if they were or not. But they didn’t make Mode any better at marketing, didn’t uncover anything our head of marketing didn’t already know, and, perhaps most of all, definitely didn’t make her life any easier.

[^4]: Like, you know, people who write blog posts that mostly link to their own blog posts.

[^5]: [Not that that helps me.](https://substack.com/profile/5667744-benn-stancil/note/c-14552225)

[^6]: This, I think, will be one of the problems with ChatGPT and “[SynthAI](https://a16z.com/2023/03/30/b2b-generative-ai-synthai/).” Reading the cliff notes is a poor substitute for reading the book, even if the cliff notes have all the ostensibly important points.

[^7]: There’s actually another problem with outside experts, which is that they can often make bad ideas sound like good ideas. [Analysis can be like debate](https://benn.substack.com/i/89635026/data-on-trial), where the style of an argument matters more than its substance. Quantitative rhetoric like this [can be quite convincing](https://benn.substack.com/p/tilt-and-tilted) whereas our gut feelings are not, [even if the latter is often rooted](https://benn.substack.com/p/method-for-measuring-analytical-work#:~:text=We%20develop%20gut%20feelings%20through%20these%20efforts%2C%20feelings%20that%20are%20more%20often%20rooted%20in%20real%20things%20we%20can%E2%80%99t%20quite%20describe%20than%20the%20meaningless%20psychic%20wanderings%20of%20our%20entrails.) in something more “real” than the former.

[^8]: I’ve seen this work in corporate settings. When I was working at Yammer the company, we used Yammer the product—which was basically [Facebook at Work](https://techcrunch.com/2016/09/27/facebook-for-work/) before Facebook at Work existed—for all of our internal communication. It was the most well-informed company I’ve ever seen. All work conversations happened in a handful of feeds, which people regularly browsed. Because they were threaded by default (unlike, say, [Slack](https://benn.substack.com/p/the-product-is-the-process#:~:text=The%20problem%20is%20Slack.%C2%A0)), you could easily scan the feed; because popular threads bubbled to the top of the feed, you were much more likely to see important stuff. It created the same awareness about the company that Facebook did about your friends in the early 2010s: Somehow, without even trying, you knew who had recently gotten married, or had a kid, or had transformed into an internet lunatic who was paranoid about chemtrails.

================================================================================

# Clear eyes, full hearts, can't lose

*For startups, there is no step two. *

---

![](https://substackcdn.com/image/fetch/$s_!sXaC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d5a37f-b5fd-442b-be34-22a70d598327_1300x731.png)

In August of 2013, [45 companies](https://www.cnet.com/tech/tech-industry/flickr-founder-plans-to-kill-company-e-mails-with-slack/#:~:text=About%2045%20companies%20are%20using%20it%20currently) used Slack. The product was still in private beta, and its test accounts were almost entirely small businesses and startups, ranging between five and 75 people. The nascent product had promise, though; [as CNET said at the time](https://www.cnet.com/tech/tech-industry/flickr-founder-plans-to-kill-company-e-mails-with-slack/#:~:text=Slack%20combines%20group%20messaging%2C%20file%20uploads%2C%20and%20notifications%20into%20one%20tool%20that%20syncs%20across%20all%20platforms.), Slack put "group messaging, file uploads, and notifications into one tool that syncs across all platforms."

Over the next ten years, nearly everything about Slack has changed. Its four dozen trial accounts grew into more than two hundred thousand paying customers, [$1.5 billion](https://s23.q4cdn.com/574569502/files/doc_financials/2023/q4/CRM-Q4-FY23-Earnings-Press-Release-w-financials.pdf) in annual revenue, and active deployments in [77 of the largest 100 companies](https://slack.com/about) in the world. It launched scores of integrations, a developer platform, and features that make it possible to use Slack for external as well as internal conversations. A global pandemic dramatically changed how people work and communicate with their colleagues, compressing a rise in remote work that [would’ve taken forty years](https://wfhresearch.com/wp-content/uploads/2023/02/WFHResearch_updates_February2023.pdf) into a month.[^1] [Slack’s CEO quit](https://www.cnbc.com/2022/12/05/slack-ceo-stewart-butterfield-leaving-salesforce-two-years-after-deal.html); Slack got [acquired by Salesforce](https://www.nytimes.com/2020/12/01/technology/salesforce-slack-deal.html); [Salesforce’s CEO quit](https://www.cnbc.com/2022/11/30/bret-taylor-steps-down-as-co-ceo-of-salesforce-marc-benioff-stays-on-as-ceo.html). Compared to what it was in 2013, nearly everything about Slack today is incomprehensibly different. 

Everything, except, what Slack *is*. When I first used Slack, I would’ve called it a chat app; today, I’d still call it a chat app. In its [first Product Hunt listing](https://www.producthunt.com/products/slack), Slack christened itself as “real-time messaging;” Slack now [defines its product](https://slack.com/help/articles/115004071768-What-is-Slack-) as a “messaging app for business.” ChatGPT—which isn’t an oracle or an encyclopedia, but a kind of poll of the internet, and a rough approximation of what the [average person on Reddit](https://www.nytimes.com/2023/04/18/technology/reddit-ai-openai-google.html) might say—describes Slack as an “app that helps people work together by letting them chat, share files, and stay organized in one place,” which is almost identical to how it was defined by CNET a decade ago. The world has changed and the business is unrecognizable, but Slack’s center of gravity—best revealed by how people would describe it in one sentence of simple words—has been written in stone.

This is true for nearly every major technology business.[^2] In 2010, [UberCab](https://techcrunch.com/2010/07/05/ubercab-takes-the-hassle-out-of-booking-a-car-service/) was a “service that offers an on-demand car service via an iPhone app.” In 2023, Uber has a new CEO, a new name, and a dozen new corporate appendages[^3]—and it’s still primarily an app for ordering a car from your phone. Stripe launched as an [online payments processor for developers](https://techcrunch.com/2011/09/30/sequoia-backed-stripe-launches-to-disrupt-the-online-payments-industry-with-a-developer-friendly-platform/); despite ballooning into every business from [banking](https://stripe.com/capital) to a [publishing house](https://press.stripe.com/), Stripe is now, first and foremost, an API for accepting payments on the internet. Fifteen years ago, Twilio [released an API](https://techcrunch.com/2008/11/20/twilio-powerful-api-for-phone-services-that-can-recreate-grandcentral-in-15-lines-of-code/) for sending texts and making voice calls. Today, the title of Twilio’s homepage is “Communication APIs for SMS, Voice, Video & Authentication.” Asana is still a task management app. Salesforce, despite acquiring a number of companies like Slack, is a CRM. Twitter has spent nearly two decades trying to figure out its second feature. 

Of course, companies mature. Markets blow up and fade away. New features get bolted on; brands change; CEOs give keynotes about reinventing the future, about relaunching the vision, about [next chapters](https://twitter.com/dkhos/status/1406957816749846531), about [versions 2.0](https://www.wired.com/2016/10/zenefits-reboots-z2-might-not-enough/). But in all but the most dramatic cases—the true pivots, when major lines of business are shut down—companies’ foundations rarely budge. The banners under which they launch is the banner under which they grow, go public, and in some cases, [collapse](https://www.nytimes.com/2015/04/30/technology/a-founder-of-secret-the-anonymous-social-app-shuts-it-down-as-use-declines.html)[^4] and [die](https://beneinstein.medium.com/the-failure-of-coin-20615dca2de).[^5] 

In other words, the two-step startup is a myth. It’s not uncommon, in early stage fundraising decks, for founders to pitch their first product as a stepping stone to a bigger one. We’ll start with building a simple alerting service for monitoring when your brand gets mentioned on social media, they might say, but we’ll use the data we collect to build an AI-powered ad platform. We’ll launch as a data catalog, and use that as a wedge to expand into our true vision as an end-to-end data lifecycle management application. We’ll begin an office leasing agency, but eventually [harness the energy of we to elevate the world’s consciousness](https://www.businessinsider.com/wework-roasted-over-cultish-ipo-filing-2019-8). 

These stories are tempting to tell. They feel simultaneously measured and ambitious. They appear to explain how a niche product with barely any users can ladder its way into a big market and [$100 million in annual revenues](https://benn.substack.com/p/metadata-money-corporation?utm_source=%2Fsearch%2F100%2520million&utm_medium=reader2#:~:text=You%20need%20to%20extrapolate%20six%20months%20of%20paid%20trials%2C%20sold%20to%20friends%20and%20previous%20coworkers%2C%20into%20a%20five%2Dyear%20revenue%20projection%20that%20exceeds%20%24100%20million.%20You). They feel clever, like Elon Musk’s [master plan](https://www.tesla.com/blog/secret-tesla-motors-master-plan-just-between-you-and-me). They “preserve optionality.”[^6] 

They also don’t work. When the first step doesn’t go as expected, companies spend their entire lives trying to finish part one. And when the first step *does* work, companies [get captured by their customers](https://benn.substack.com/p/customer-capture), and can rarely escape the shadow of their initial successes. One way or another, prerequisites become permanent.

Put differently, for a business to grow into something big, its initial product has to grow into something big. The market might change—it could grow underneath a product as it did for Uber, or new markets could open up, as enterprise buyers did for Slack—but the soul of the product almost certainly won’t.[^7] 

# Clear eyes

The gut check for founders and early-stage investors, then, is this: Do you believe that the product description in the VentureBeat writeup about the seed round can also be the first sentence on an S-1? 

In some cases, the answer is a clear yes or no. But most of the time, it’s a harder call. Plenty of companies struggle and stumble, start slow, or stall out at a couple million dollars of revenue and twice that in expenses. They see occasional sparks of interest for their initial idea, which are enough to keep the pull the business along but not enough to light a bigger fuse. 

The mythology of Silicon Valley tells these companies to keep going. Airbnb famously teetered through [a near-death experience](https://inc42.com/resources/airbnbs-journey-failing-startup-25-bn-company/) in its early days; lots of other companies fought through long slogs before becoming “[overnight](https://www.entrepreneur.com/leadership/the-myth-of-the-overnight-success-and-how-brilliant-ideas/388210)” successes. It’s easy, in these situations, to keep tacking on features, in hopes that some tipping point is just one or two releases away. It’s easy to experiment with partnerships, or to launch a free offering, or to rebrand under a more current buzzword. 

But for companies living in this [trough of sorrow](https://andrewchen.com/after-the-techcrunch-bump-life-in-the-trough-of-sorrow/), they need to be clear-eyed and honest—uncomfortably, brutally, searingly so—about the goal of these efforts. Are you exploring these things because you’re convinced that your original idea—that one sentence description in the VentureBeat article—is still white hot with potential, and can carry you all the way to an IPO? Or are you exploring these things because you know that it’s not, and you’re looking for a partial pivot?

Because if it’s the latter, the battle is lost. Yes, full, hard pivots can work. Slack, for example, spun out of a gaming company that was building a wholly different product. Instagram began as a [social media app for bourbon drinkers](https://www.theatlantic.com/technology/archive/2014/07/instagram-used-to-be-called-brbn/373815/). Segment [went through two pivots](https://venturebeat.com/business/how-segment-survived-its-brush-death-become-customer-data-management-unicorn/#:~:text=Reinhardt%20was%20studying%20aerospace%20engineering,or%20confused%20during%20a%20presentation.) before becoming what it’s known for today. In these cases, the first product wasn’t renovated into the second; it was torn down and picked apart for scrap metal. 

Partial pivots, by contrast, are a prolonged death march. If a product only attracts a curious interest from people who never seem quite ready to buy, a couple new features won’t ignite demand. Partners won’t turn a mediocre idea into a great one. And if people are reluctant to spend money on something, they won’t be excited to use a free version of it either. For a startup to succeed, its founding idea has to work, or it has to change. And slow arcing pivots are neither. They’re the aimless [distractions](https://www.youtube.com/watch?v=cQArSoYJWjI) of a company that lost some deals and is on its way [to losing itself](https://www.youtube.com/watch?v=eOhnFYZo2oc).[^8] 

# Full hearts

At this point, I’m probably supposed to talk about how important data is for developing conviction in an idea. If you carefully track how people are using your service, I should say, you can figure out if your idea has product market fit. Buy a BI tool; use the modern data stack; trust the science.

For early stage companies, however, this sort of analysis is somewhere between useless and actively harmful. Because if an idea doesn’t quite work, you won’t compute that painful truth; you will *feel *it. You’ll feel it in your wavering belief that you’re making the right bet. You’ll sense it in how you push down your own doubts in investor pitches. You’ll hear it in prospects’ voices when they politely tell you that they think what you’ve built is cool, that they’ll follow up after the call, that your email got lost in their inbox but they’re definitely still interested in checking it out. 

Startups don’t die because leaders miss these feelings; they die because teams ignore them. They die because we’re supposed to be empirical and data-driven, and these anxieties don’t show up in our dashboards. They die because that same anxiety is uncomfortable to admit, and even more uncomfortable to address. They die because we’ve made our emotions inadmissible evidence.

But there’s urgent meaning in that emotion. A head full of doubt can still be a [road full of promise](https://www.youtube.com/watch?v=QeYSqZPzwr8), but a heart full of doubt can’t win. 


---


[^1]: Though really, it all happened in one day: On March 12, 2020, the day that Trump [suspended travel from Europe](https://www.cnn.com/2020/03/11/politics/donald-trump-coronavirus-statement/index.html), that the [NBA season got canceled](https://www.nba.com/news/coronavirus-pandemic-causes-nba-suspend-season), and that [Tom Hanks got COVID](https://www.cnn.com/2020/03/11/entertainment/tom-hanks-rita-wilson-coronavirus/index.html).

[^2]: “Survivorship bias!,” you scream. “The airplane, you fool, *[the airplane](https://en.wikipedia.org/wiki/Survivorship_bias#/media/File:Survivorship-bias.svg)!*” Yes, but that’s the point? If very few of the companies that kept trying to contort themselves into some new idea didn’t make it home, then…don’t do that?

[^3]: These include: A food delivery service, a self-driving car division, a self-driving semi-trailer truck division, a helicopter service, a vertical-takeoff-and-landing aircraft division, a bike rental service, a scooter rental service, several AI labs, an alcohol delivery service, and Postmates.

[^4]: This is poetically true for Secret, which apparently imploded in part because its CEO sold a bunch of shares during one of their fundraising rounds and used that money to buy a Ferrari. He tried to hide it from Secret’s employees, but someone found out and posted it on Secret. “I never thought leopards would eat MY face,” [sobs man](https://knowyourmeme.com/memes/leopards-eating-peoples-faces-party) who created the Leopards Eating People’s Faces app.

[^5]: The same is for data companies. Segment is a switchboard for web events, despite [dabbling in ETL](https://www.globenewswire.com/news-release/2016/04/06/1300478/0/en/Segment-Launches-Sources-to-Unify-Siloed-Customer-Data-in-Minutes.html). Fivetran is ETL, despite [dabbling in data transformation](https://www.youtube.com/watch?v=sWKpe6rpc-c). dbt Labs is data transformation, despite its recent [dabbling in semantic layers](https://www.getdbt.com/blog/dbt-semantic-layer/). Transform, which dbt Labs bought, is a semantic layer, despite [dabbling in reporting](https://blog.transform.co/product-news/introducing-boards/). And Heap is reporting, despite [dabbling in being a switchboard for web events](https://developers.heap.io/reference/track).

[^6]: [This is a bug](https://benn.substack.com/p/the-best-decision-is-one) and not a feature.

[^7]: Startups, you might say, [are what you thought they were](https://www.youtube.com/watch?v=SWmQbk5h86w).

[^8]: At which point the company might not die, but will stumble along, stuck in time, leaving everyone wondering [how on earth it’s still around](https://www.theringer.com/tv/2018/1/24/16924756/how-old-is-tim-riggins-friday-night-lights).

================================================================================

# BI by another name

*Taking stock of the new semantic layers. *

---

![Comcast Xfinity Opens WiFi for Free to Connect Low-Income Families &  Students | Moody on the Market](https://substackcdn.com/image/fetch/$s_!7nNH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d04a872-605b-4220-b77f-e09f6778dfff_6000x3375.jpeg)
*[Comcast](https://time.com/2985964/comcast-cancel-ryan-block/)*

There are two important parts to a universal semantic layer. The first is the semantic part: The layer needs to define various business concepts like [revenue](https://twitter.com/pdrmnvd/status/1653442564928983041), accounts, users, and [net income per home minus fake stock-based expenses](https://www.reddit.com/r/StockMarket/comments/xm7muf/contentious_session_with_opendoor_cofounder_keith/). Most companies have a bunch of raw data that needs to be cleaned, joined, and aggregated before it can be used in meaningful ways; semantic layers are, roughly speaking, giant computational formulas that do that cleaning, joining, and aggregating. 

When we talk about universal semantic layers—as [we have been recently](https://www.holistics.io/blog/the-ideal-semantic-layer/), following some [significant updates](https://www.getdbt.com/blog/dbt-semantic-layer-whats-next/) from dbt Labs, [LookML’s secession](https://cloud.google.com/blog/products/data-analytics/introducing-looker-modeler) from Looker, the community’s [continued curiosity about Malloy](https://www.datacouncil.ai/talks/malloy-an-experimental-language-for-data), and [a](https://win.hyperquery.ai/p/llms-the-semantic-layer-and-2023) [frenzy](https://www.datacouncil.ai/talks/llms-semantic-layer-self-serve-has-entered-the-chat) [of](https://twitter.com/_abhisivasailam/status/1627373049099194368) [predictions](https://davidsj.substack.com/p/semantic-superiority-part-5) [about](https://cube.dev/blog/conversational-interface-for-semantic-layer) [LLMs](https://benn.substack.com/p/the-rapture-and-the-reckoning)—this is the part we tend to focus on. We question if these new semantic layers can handle the logical complexity of our businesses. We ask if they should be created by humans or AI. We debate new configuration specs. We talk about how semantic layers should be developed, deployed, updated, and versioned. 

Important as these things are, they’re only half of a universal semantic layer. In addition to encoding semantics, universal semantic layers also need to be universally accessible.[^1] So far, most semantic layers meet this requirement by being queryable by generic APIs. You define your net retention rate in [MetricFlow](https://blog.transform.co/product-news/introducing-metricflow-your-powerful-open-source-metrics-framework/); you can now request various pivots of it via a Python library. You specify how to count daily active users in some YAML file in your dbt repo; you can now query that metric via a dbt Labs’ JDBC driver and a SQL-like language. You make up a rule for calculating [community-adjusted EBITDA before growth investments](https://qz.com/1685919/wework-ipo-community-adjusted-ebitda-and-other-metrics-to-watch-for) in Cube; you can now commit securities fraud through Delphi’s conversational interface in Slack. 

This approach means that data teams can choose whatever combination of BI tools and semantic layers they want. Want to stay in the Google ecosystem but like Cube? Use Mode and are intrigued by dbt’s acquisition of Transform? Want to migrate from PowerBI to Tableau without replacing AtScale? Today’s universal semantic layers support this kind of mix-and-match modularity. 

For a while, I’ve assumed—along with everyone else, it seems—that this fulfills universal semantic layers’ universal requirement. But [Carlin Eng has concerns](https://carlineng.com/?postid=semantic-layer#blog):

> Analytical freedom is severely limited. Since data from the metrics API is already aggregated, there’s no way to drill into specific records, or create on-the-fly dimensions and measures to slice the data in a way that might reveal new insight. You can request other dimensions from the metrics layer, but what if the dimension you’re interested in doesn’t exist yet?

Put differently, the interface into today’s semantic layers are all more or less the same: Ask it for a metric; specify some filters, dimensions, and secondary calculations like a ratio or rolling average; it gives you a table with those results. Different semantic layers have different query syntaxes (including natural languages), but structurally, most of them are fundamentally about extracting a metric or dataset.

As I’ve argued before, this type of [metric Mad Libs](https://benn.substack.com/p/self-serve-still-a-problem#:~:text=In%20this%20context,by%20month.) is great for self-serve because [it’s an easy way to understand data](https://benn.substack.com/p/ghosts-in-the-data-stack#:~:text=These%20websites%20were%20great%20because%20they%20presented%20data%20in%20the%20two%20forms%20that%20I%20understood%20it%3A%20As%20a%20metric%2C%20or%20as%20a%20table.). But, quoting Carlin again, it “falls flat when the analyst needs to do critical work like debugging misbehaving metrics, or more creative data work such as exploratory deep-dives.” 

Presenting data in this way—in a tidy, consumable, and *constrained* format that’s accessible to everyone—is typically what BI tools are for. In other words, today’s independent semantic layers *aren’t *universally accessible encodings of business logic that can be used by any downstream analytical application; they’re BI tools, without charts.[^2]

Which makes sense! That’s exactly how we created this generation of semantic layers—BI [without a head](https://basecase.vc/blog/headless-bi); LookML [without Looker](https://cloud.google.com/blog/products/data-analytics/introducing-looker-modeler); the logic box [removed from the BI box](https://benn.substack.com/p/metrics-layer#:~:text=The%20smiling%20database%20likes%20it.). But, it’s hard to see how this type of semantic layer gets adopted that widely. For a lot of customers, moving away from their BI-based semantic layer is a major undertaking. What’s the point of doing that work if the primary benefit is…to connect your new semantic layer to the same BI tool? Why [invest in an general solution](https://xkcd.com/974/) if it’s only going to work for one application? 

To be clear, even in its current form, I’m still in favor of the move towards universal semantic layers—companies often have multiple BI tools, and building them on top of a shared OLAP cube is better than building each one having their own.[^3] I think it’d be beneficial for companies to migrate to this architectural paradigm, and Mode is invested in doing its part to make that possible. But we also have to acknowledge the obstacles that get in the way of that effort. And I’m starting to think that this is the biggest—that today’s semantic layers are mostly applications for metric extraction that might make our BI tools better, but they don’t make our entire data platforms better. 

An ideal semantic layer would go a lot further. It would support more than reporting and self-serve analysis; it would even support more than the analytical work that Carlin referenced that’s done outside of traditional BI tools. Done right—and, fair warning for what’s coming, *I don’t know how to do this*—it would also provide a means of governance for entirely new ways to interact with and reason about data. 

# Universal need, individual preference

If you ask people what their favorite app is that nobody else would say, everyone says their to-do list of choice.[^4] [Someone will say Apple Notes](https://ava.substack.com/p/keeping-things-organized-even-though#:~:text=Lists.-,I%20love%20iPhone%20Notes,-(I%20annoyed%20some), because it's so gracefully simple. Someone will say Notion, because it’s the perfect combination of functionality, ease, and millennial minimalism.[^5] There are the Evernote holdouts, Todoist zealots, and overachievers who built their own [custom-designed, one-of-a-kind bespoke app](https://jesuschristsiliconvalley-blog.tumblr.com/post/46539276780#:~:text=It%E2%80%99s%20a%20custom%2Ddesigned%2C%20one%2Dof%2Da%2Dkind%20bespoke%20app%20I%20had%20built%20for%20my%20assistant%20and%20I%20to%20communicate%20and%20collaborate%20through). And someone will be an actual contrarian and say TextEdit. 

People choose this class of app as their favorite—but invariably choose different ones—because the problem they solve is universal, but the solution is personal. Everyone thinks in different ways; their notes and task lists need to reflect that. 

Some people think in a “[robust latticeworks of mental models](https://twitter.com/SteadyCompound/status/1646452980550434816)” and need a [vector database of Markdown docs](https://roamresearch.com/) to organize the contents of their galaxy brains. Some of us are one-track luddites who can only think linearly, and can’t handle anything more complicated than a single list of uncategorized tasks. Some are synesthetes who want to color-code everything. Some are the [inbox zero addicts](https://twitter.com/schrockn/status/1650859815168806916), and some are quantified selves who [track and measure their to-dos’ pipeline velocity](https://linear.app/features/analyze); some are hoarders who need to save every task and others are obsessive-compulsives who live for the hit that comes with backspacing through a completed task. Some people choose beauty and use [Bear](https://bear.app/); some people choose spartan simplicity and use a mono-spaced code editor; and some lunatics choose chaos and use [Stickies](https://en.wikipedia.org/wiki/Stickies_(Apple)). 

Because of these differences, the market never settles. It’s winner-take-very-little, for the same reason that the fashion and car markets are: People use to-do lists to do different things, and have different personal preferences about what they like. 

The same, I believe, applies to data consumption tools. Lower in the stack, in ETL products, in data warehouses, and in emerging applications like data contracts, data tools mostly interface with each other. In these systems, there’s an incentive to coordinate. If we can agree on standard protocols, everyone’s life gets easier. Tools can coalesce around emerging defaults, the best pairings reinforce each other, and the market consolidates. A reverse ETL tool’s “preference” for the kind of database it reads from is whatever kind of database most people use.

The front end, by contrast, is the interface between technology and humans—and people won’t ever agree on the best way to do anything. Some analysts will always like code; some will prefer pivot tables. Some people squint at [decks of time series](https://www.holistics.io/blog/how-amazon-measures/#:~:text=In%20the%20early%20days%20of%20Amazon%2C%20the%20metrics%20deck%20was%20printed%20on%20paper.%20Today%2C%20decks%20are%20either%20printed%20or%20virtual.); some want dedicated tools for analyzing A/B test results; some like funnels and Sankey diagrams; and others will want to [dodge regulatory oversight by rigging regulatory models](https://newsletterhunt.com/emails/30211) in spreadsheets. As is the case with to-do list apps, these inconsistencies don’t reflect the thrash of an immature market; they’re real and permanent differences of opinion and preference.[^6] 

There is, however, one very tricky distinction between data consumption tools and to-do lists: Data tools need to sit on a shared source of truth. People may want to interact with data in different ways, but they want the data they’re interacting with to be the same*. *I may want to play with a table of users in Excel, you may want to build a user retention model in Python, a marketer may want to walk through [user journeys in Amplitude](https://amplitude.com/blog/customer-journey-analytics), and a product manager may want to use [Motif](https://www.motifanalytics.com/) to find patterns in how users behave. But all of us want our users to be the same. 

In theory, this is exactly what a standalone semantic layer is for: Consistent business logic, universally accessible. *But today’s semantic layers probably can’t do this. *A time series of a metric can’t be pivoted into a funnel, no matter what dimensions you add to it. A free-form spreadsheet is incompatible with the structured tables in a relational database. A domain specific language like Motif, which provides a shorthand for analyzing sequences of events, can’t get passed through [Malloy](https://www.malloydata.dev/), a relational modeling language. Our tooling preferences reflect fundamentally different models for interacting with data, and they can’t all be shoehorned into a framework of dimensions and measures. 

This is what a semantic layers would ideally be—a place that creates organizational structures and computational shortcuts that can be consumed by any type of data tool. It could power the A/B tests in Eppo, the forecasts in Anaplan, and the user flows in Mixpanel. It could sit underneath Malloy and Motif so that people who learn and like those languages could use them without needing to recreate another semantic model in each tool. It could work in spreadsheets. 

Unfortunately, I genuinely don’t know how to do this. How do you add governance to a spreadsheet? How do you ensure that experimental languages use the same computational logic when calculating a metric? It all seems very hard. But there are two success stories that might hint at a couple possible solutions. 

The first is dbt Core (i.e., the open source dbt Labs product that creates tables in databases). You can put dbt Core in front of every tool that sits on top of a database because dbt Core defines the data that’s in the database. The logic in dbt Core is automatically picked up by Malloy, Motif, and every spreadsheet with a JDBC connection because, if you’re using data, you’re using dbt Core. In this sense, it is a universal semantic layer; it’s just one that can encode entities but not computation.

The second hint comes from SQL itself, which is actually the inverse of dbt Core. It encodes computation, but not entities. It’s also universal—you can’t get to the data in a database without going through a SQL query first.

An aspirational semantic layer would retain both of these properties. Is that SQL, with semantic functions? Is it a new type of database that supports computational [DDL](https://en.wikipedia.org/wiki/Data_definition_language)? Is this already supported by mixing MetricFlow with raw SQL? Is it just [Malloy](https://carlineng.com/?postid=semantic-layer#blog)? Is this DAX? Do these questions even make sense?[^7]

Honestly, I’m not sure. But I’d guess that this will be the next challenge for universal semantic layers. So long as they operate as applications for governing metric extraction, they’ll only be able to serve products that primarily need to extract metrics. Other tools—those that need computational flexibility, and that want to create new ways to interact with data—will likely ignore the semantic layer, or create some version of their own. 

# The pedantic layer

At this point, some of you may be thinking, “Didn’t you propose exactly the thing you’re saying is incomplete? Wasn’t this whole metric thing [your idea in the first place](https://benn.substack.com/p/metrics-layer)?”

First, who can say how we got here; [we’re all trying to find the guy who did this](https://www.youtube.com/watch?v=WLfAf8oHrMo). Second, something something [facts change](https://quoteinvestigator.com/2011/07/22/keynes-change-mind/). And third, there’s some nuance here that’s worth calling out.

As it was originally conceived, a metrics layer was really just a collection of formulas. It was, as the name suggests, a tool for encoding metrics. If you need a metric, write a query with some special syntax; the metrics layer, which could be a SQL proxy or some library of stored procedures, gets you that metric. 

I think the problem emerges when this tool gets promoted into a semantic layer. To me, semantic layers are meant to encode everything—relationships, metrics, entities, and so on. This often means that you are either querying through the semantic layer, or not at all.[^8] If this is the expected behavior—which I'd argue the name encourages—a limited semantic layer isn’t that useful, because any analytical work that the semantic layer can’t handle will have to bypass it.

And that’s ultimately the point of this entire post. If we’re going to build universal semantic layers, they have to be flexible enough—like SQL itself—to work with tools that present data in a range of different ways. Otherwise, we’re not building semantic layers; we’re building BI tools by another name. 


---


[^1]: I realize I’m making the bold claim here that universal semantic layers need to be universal and have semantics. Next week, a post about how large language models need to be large and have languages.

[^2]: Suppose that, in 2013, you were cursed by an evil sorceress and [fell into a deep sleep for ten years](https://en.wikipedia.org/wiki/Sleeping_Beauty_(1959_film)). When you woke up and asked what you missed, your (very unfortunate) true love told you that people don’t use MicroStrategy for BI anymore, and instead a thing called the dbt semantic layer with a thing called Mode sitting on top. Which of those two tools would you say is the new BI tool? Is BI’s essential element governance, or exploration? If you ignore the two tools’ paths to their current spot in the stack, I’m not sure the answer is very clear.

[^3]: Here’s a fun example. I once heard that, among three of the largest BI tools, vendor A’s biggest customer is Walmart. And Vendor B’s biggest customer is Walmart. And vendor C’s biggest customer is Walmart.

[^4]: There has to be a world for this, right? What do you call a seemingly contrarian take that everyone agrees with? On a related note, please take this [very important survey](https://docs.google.com/forms/d/e/1FAIpQLSe4H9B8YHAprd0MZ4tqRELbzwbSHeN8feiJzJRqmPQ7cUC13w/viewform).

[^5]: Thirty-somethings love software with gentle san serif fonts, off-whites and dark grays, and buttons that fade in on hover.

[^6]: In some cases, I’d argue that it goes even further than this, and the tools you use affect how you conceptualize what data *is*. Academics who grew up using R or Pandas think in arrays and data frames, in variables and functions. I've spent most of my career in SQL, which encourages me to think on rows, columns, tables, and joins. When I'm asked a question, I reason about it through that structure: What intermediate tables do I need to create, and what aggregations and joins will help me create them? My thoughts are shaped by the English words that I use to describe what I'm feeling; my analytical reasoning is shaped by the linguistic concepts I use to describe data.

[^7]: [Am I asking for things that already exist?](https://benn.substack.com/p/am-i-the-jared-kushner) I have no frame of reference here; [I’m like a child who’s wandering into the middle of a movie](https://youtu.be/IwZshlE3HUA?t=31).

[^8]: For example, in Looker, you either use LookML or write a query; there’s no blended option. Most semantic layer APIs function the same way. You don’t compose a query with a metric inside of it; you just request an object that’s defined in the semantic layer. (I have a similar concern about Malloy as well. Because it replaces SQL rather than decorating it, there aren’t really ways to gradually adopt Malloy or ease up its learning curve. Even if it technically solves the problems described in this post, that’s only half the battle. You also have to get people to use it.)

================================================================================

# The truth is out there

*The only thing stopping us from finding it is us.*

---

![](https://substackcdn.com/image/fetch/$s_!0y2U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85405101-c309-4adc-987a-b9037ac8fe2d_1200x1200.png)

*Author’s preface: [I’m told](https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media#:~:text=Alexander%2C%20who%20prefaces%20some%20of%20his%20own%20posts%20with%20an%20%E2%80%9Cepistemic%20status%2C%E2%80%9D%20by%20which%20he%20rates%20his%20own%20confidence%20in%20the%20opinions%20to%20follow%2C%20thought%20the%20media%2C%20too%2C%20should%20present%20its%20findings%20in%20shades%20of%20gray.) that some writers in the rationalist community preface their articles with an “[epistemic status](https://v5.chriskrycho.com/journal/epistemic-status/)” that scores how confident the author is in the arguments they’re making. In writing this piece, I started to feel like these posts need a “[Jared Kushner](https://benn.substack.com/p/am-i-the-jared-kushner) status,” in which I rate the degree to which I know nothing about the subject, and am wandering into it with the unhinged confidence of [some buffoon](https://benn.substack.com/p/the-emperor-and-his-clothes) in a Harvard Business School class. I don’t know anything about chess or AI, but I did read most of the articles that I linked to, so: Jared Kushner status, 7/10.*

Garry Kasparov was relieved. 

In 1996, the chess grandmaster—the youngest-ever world champion and one of the best players in history—was playing his first game against Deep Blue, IBM's chess-playing supercomputer. Early in the game, Deep Blue offered up its pawn [in a potential sacrifice](https://content.time.com/time/subscriber/article/0,33009,984305-1,00.html). The move was bait: Had Kasparov taken the pawn, it would’ve set in motion a series of moves that would eventually tilt the game in Deep Blue’s favor.

The move itself was unremarkable; human players do things like this all the time. But computers—which Kasparov had been playing for years—typically played more mechanically. Prior to each move, they played out hundreds of millions of scenarios, and evaluated each based on the pieces that would be left on the board. This made computers overly [materialistic](https://www.chessstrategyonline.com/content/tutorials/basic-chess-concepts-material), and prevented them from seeing the positional advantages that might come from sacrifices like Deep Blue’s pawn offering. 

Initially, Kasparov was struck by the move. It revealed, as he put it, “[a new kind of intelligence across the table.](https://content.time.com/time/subscriber/article/0,33009,984305-1,00.html#:~:text=a%20new%20kind%20of%20intelligence%20across%20the%20table)” 

After the match, Kasparov realized he was mistaken. Some ethereal calculus hadn’t given Deep Blue a new intuition for the game; it found the move [through brute force](https://content.time.com/time/subscriber/article/0,33009,984305-1,00.html#:~:text=Deep%20Blue%27s%20computational%20powers%20were%20so%20great%20that%20it%20did%20in%20fact%20calculate%20every%20possible%20move%20all%20the%20way%20to%20the%20actual%20recovery%20of%20the%20pawn%20six%20moves%20later.). “Deep Blue's computational powers were so great that it did in fact calculate every possible move all the way to the actual recovery of the pawn six moves later.” To Kasparov, this was an “inefficient, inflexible kind” of intelligence that could be gamed, tricked, and beaten. 

His confidence, however, was short-lived. A year later, Kasparov famously lost to Deep Blue;[^1] over the next two decades, chess-playing AIs blew past their human competitors,[^2] to the point where it’s no longer interesting to watch the two play against each other.[^3] 

*Author’s note: This next section references a blog post written in [plain HTML](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) by an AI research scientist who graduated from Stanford in 1978, so: Jared Kushner status, 12/10.*

What’s interesting, though, is that Kasparov wasn’t beaten because computers finally learned how to reason about chess. It was, as [Richard Sutton](https://en.wikipedia.org/wiki/Richard_S._Sutton) argued in 2019, [the opposite](http://www.incompleteideas.net/IncIdeas/BitterLesson.html):

> Researchers always tried to make systems that worked the way the researchers thought their own minds worked—they tried to put that knowledge in their systems—but it proved ultimately counterproductive, and a colossal waste of researcher's time, when, through Moore's law, massive computation became available and a means was found to put it to good use.

According to Sutton, this has been the defining characteristic of AI research of the last 70 years. Researchers often assume that the best way to make an AI program better at some task is to teach it what they know, to “leverage their human knowledge of the domain.” But “the only thing that matters in the long run is the leveraging of computation:”

> The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.

Software designed this way did come with one weird quirk: Far from being rigid and formulaic in the way Kasparov expected, AI programs played with a sort of otherworldly flair. AlphaZero, a chess program built by DeepMind, doesn’t play like humans or standard computers; it plays, [according to its creator Demis Hassabis](https://www.technologyreview.com/2017/12/08/147199/alpha-zeros-alien-chess-shows-the-power-and-the-peculiarity-of-ai/), in a "third, almost alien, way" that’s like “chess from another dimension.” [AlphaGo](https://www.theatlantic.com/technology/archive/2017/10/alphago-zero-the-ai-that-taught-itself-go/543450/), DeepMind’s Go-playing AI, is “bit madcap,” makes “unorthodox opening moves,” and does things that are sometimes “downright incomprehensible.” PioSOLVER, a tool for evaluating poker hands, is [both dominant and sometimes suggests](https://www.nytimes.com/2022/01/18/magazine/ai-technology-poker.html) “really strange plays” that few professional poker players would make on their own. These programs are unnatural, but deadly superior. 

All of this raises an uneasy question: If our strategies are so limited in these games—despite their precise rules and the centuries they’ve been studied—what else are we, relative to what’s possible, woefully bad at? What mazes are we trapped in, captive to our inability to see the logic that supercomputers can see? 

Or, as [Balaji Srinivasan put it in an interview with Lex Fridman](https://www.youtube.com/watch?v=6OYex7I_b9g):[^4]

> A rat can be trained to turn at every even number or every third number in a maze to get some cheese. But evidently, it can't be trained to turn at prime numbers. Two, three, five, seven, and then eleven, and so on and so forth. That's just too abstract, and frankly, if most humans were dropped into a prime number maze, they probably wouldn't be able to figure it out either… And so the thing I think about a lot is just how many patterns in life are we just like these rats and we're trapped in a prime number maze? 

*Author’s note 2: We now return to more familiar footing—data teams, semantic layers, and [data angst](https://benn.substack.com/p/the-case-for-being-biased#:~:text=In%20my%20recent%20descent%20into%20data%20team%20existentialism). Jared Kushner status, 3/10.*

Ever since the economy turned last spring, the data industry seems to have developed a creeping anxiety about its actual usefulness. In response, we’ve proposed a smattering of ideas that will finally [unlock our full potential](https://www.youtube.com/watch?v=4TLppsfzQH8).

Last fall, at dbt Labs’ annual conference, I [offered up my solution](https://www.youtube.com/watch?v=q1sIRhrFoeY): A return to well-worn operational models. Data teams should create simple abstractions of how their business works and use those as the foundation for everything they do. Think like an economist, I said. They model economies with supply and demand curves; we should model businesses with [user engagement loops](https://www.youtube.com/watch?v=rqWnEJXnfiY&t=1708s) and [growth models](https://twitter.com/g_xing/status/1505664341609095168). I’ve been planning on turning that talk into a post here; in my spreadsheet of drafts, the note next to that idea says, “This is what we need more than anything. Best thing we can do as an analyst.” 

On one hand, this still seems true. Companies are living ecosystems of incalculable complexity; these models—and their cousin, the now-trendy [semantic layer](https://benn.substack.com/p/bi-by-another-name#:~:text=When%20we%20talk%20about%20universal%20semantic%20layers)—are our only means to reason about them. 

On the other hand, this line of thinking sounds awfully similar to the trap Sutton is warning us about. Our business models and semantic layers are efforts to describe businesses as we see them; they are a “favored, human-centric approach,” and are *our* methods—as opposed to *the* methods—for reasoning about difficult and tangled problems. As Sutton argued, we often handicap ourselves by stubbornly sticking with these techniques; as Sutton predicted, we are, as data teams, also at least partially stalled in our own search for impact. 

Which makes me wonder—for the problems that business analysts work on, what would it look like to run in the other direction, and favor approaches that leverage computation over human knowledge of the domain? 

For example, consider how we might solve a pricing optimization problem. Today, we’d likely build a model that explains how we think pricing will affect different customers, add baseline inputs about our upsell and churn rates, and run a few sensitivity analyses by fiddling with those assumptions. 

Tomorrow, we might use an AI for the analysis. But, if it’s still built on top of various semantic models that embed our own assumptions about how the world works—like segment classifications, or our methods for calculating various performance metrics—we could kneecap the AI’s potential by implicitly modeling the contents of our mind into the problem. Instead, perhaps we should be trying to remove ourselves and our semantics as much as possible, and let AI agents work on problems on their own terms.

Of course, I get that companies are not Go matches, which can be simulated millions of times in an instant. Moreover, corporate processes exist [only as abstractions](https://benn.substack.com/p/data-is-for-dashboards#:~:text=The%20landscape%20around%20a%20company%20is%20artificial%2C%20and%20exists%20only%20as%20a%20rendered%20representation%20of%20the%20calculations%20that%20define%20it.); there is no unbiased representation of a company as there is of a game of chess. Maybe—probably?—this is all hocus pocus, and a distraction from Doing the Real Work.

Still, part of me can’t escape Balaji’s prime number maze. Consider how far we are from the frontier of ideal performance *in board games*.[^5] The space between our decisions and the optimal decisions in business domains—in marketing operations, in corporate finance, in sales strategy—must be immense. And surely there must be some way to close the gap.

*Author’s postscript: This is the point that I shrug, say, “yeah, seems like a mess out there, good luck with that, but call me if you figure it out so [I can invest it in and get an undeserved cut](https://www.reuters.com/world/us/jared-kushner-leave-politics-launch-investment-firm-sources-2021-07-28/).” Jared Kushner status, 10/10.*


---


[^1]: There’s a famous story from the first game of this rematch. Kasparov held the advantage, and on the forty-fourth move of the game, Deep Blue [sent its rook halfway across the board](https://www.chess.com/article/view/deep-blue-kasparov-chess#kasparov-deep-blue-1997-rematch) into a position that, to even Kasparov's very discerning human eye, made no sense. Kasparov closed out the game easily, but [walked away rattled](https://www.washingtonpost.com/news/wonk/wp/2012/09/26/nate-silvers-the-signal-and-the-noise/). What distant possibility had the computer seen that he couldn’t? What strategy did it devise that was beyond his creativity?The answer, it turns out, was none of the above. The move was a glitch. Deep Blue had malfunctioned, and was programmed to [choose a move at random](https://www.npr.org/2014/08/08/338850323/kasparov-vs-deep-blue) when it didn’t know what to do. Rook to D1 was the number that the computer, frozen and stuck in a loop, drew out of a hat. More like Deep Blue screen of death, I guess.

[^2]: It’s as competitive as [Kobayashi versus the bear](https://www.youtube.com/watch?v=HgqbCq_sxmo).

[^3]: One interesting related question: Why are we still interested in watching two people play chess, when games between two AIs would be far superior? When we watch sports, we generally watch the highest level of talent—the NBA over G League, the majors over the minors, and so on. Is it because we’re interested in the players as people? Because we want to live vicariously through their emotions? Because we actually like a flawed game? I truly don’t know. But in light of AIs potentially replacing us as artists, musicians, and writers, it seems relevant to figure out when and why we’re drawn to human creations.

[^4]: Good god, what a cursed sentence. What’s more, I’ve now mentioned Balaji [twice on this blog](https://benn.substack.com/p/the-rapture-and-the-reckoning#:~:text=Still%2C%20Balaji%E2%80%99s%20tweet%20highlights%20a%20useful%20trick.), and both times have been positive. If I say his name [three more times](https://www.youtube.com/watch?v=tlwzuZ9kOQU), the Pomp Podcast will appear to gouge me with a [crypto scam](https://twitter.com/bennstancil/status/1637078144539582464).

[^5]: As opposed to, say, tic-tac-toe, which we can play [at the frontier](https://www.youtube.com/watch?v=mExQ8bz3Gno).

================================================================================

# Do we still need the world wide web?

*Why fast—and not free—could reinvent the entire internet. Or at least, I dunno, data catalogs. *

---

![Fast X and The Fast and Furious Franchise: How Close are Audiences to Film  Fatigue?](https://substackcdn.com/image/fetch/$s_!HeMz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae936cb-cfe9-4cf4-b5e2-94b51ca08104_1200x630.jpeg)
*[Fast X](https://en.wikipedia.org/wiki/Fast_X)*

We haven’t yet agreed on whether or not AI is going to [destroy humanity](https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction), but there seems to be a growing consensus that it will [destroy the internet](https://lajili.com/posts/post-2/). As the [cost of content creation falls to zero](https://www.wsj.com/articles/generative-ai-brings-cost-of-creation-close-to-zero-andreessen-horowitzs-martin-casado-says-58e061b4), the story goes, generated content will overrun the web. Soon, every site will be a [soulless cesspool of clickbait](https://twitter.com/cwarzel/status/1658565502774689793), inbred on top of a spiraling gene pool of derivative junk mail and content marketing.[^1] 

This dystopia seems plausible enough—it is, after all, already happening. [YouTube factories](https://medium.com/@jamesbridle/something-is-wrong-on-the-internet-c39c471271d2) have existed for years, retching out thousands upon thousands of horrific videos, hoping that one—maybe the one that combines Peppa Pig, Easter eggs, and, I don’t know, [being buried alive](https://medium.com/@jamesbridle/something-is-wrong-on-the-internet-c39c471271d2#:~:text=This%20video%2C%20BURIED%20ALIVE%20Outdoor%20Playground%20Finger%20Family%20Song%20Nursery%20Rhymes%20Animation%20Education%20Learning%20Video)?—catches an algorithmic updraft. 

Historically, we’ve been partially protected from these sorts of content chop shops for both technical and social reasons. Algorithmically-created articles, images, and videos were just hard enough to produce and of just low enough quality that their creators had to fully commit to the bit. Using this stuff was roughly synonymous with content farming; content farms are seen as a kind of sleazy grift; legitimate enterprises aren’t cons. So, in our playing of the [Great Online Game](https://www.notboring.co/p/the-great-online-game), we had to choose: Be a respectable creator and hand-crafted our content, or run an internet tourist trap and sell junk.

OpenAI, Midjourney, and the tsunami of interest in generative AI has changed all that. The technologies make it easy and inexpensive for anyone to create content; the hype makes it not only socially acceptable to do so, but foolish and short-sighted *not* to do so. Every other word in [company keynotes is generative AI](https://www.tiktok.com/@verge/video/7231610749796437294); every third question on earnings calls [is about it](https://www.bloomberg.com/news/articles/2023-05-05/ai-scores-as-hot-topic-on-earnings-calls-as-interest-deepens); every fourth tweet is an urgent thread warning us that, unless we subscribe to a weekly newsletter on the latest prompt engineering tips, we’ll soon be fired and replaced by an intern and a chatbot. 

And so, the thin levies between us and an unending sea of spam are starting to break. BuzzFeed is experimenting with AI-written articles. DJs, from [David Guetta](https://www.youtube.com/watch?v=98WTwSnkoas) to [anonymous people on TikTok](https://www.vice.com/en/article/wxj5gw/heart-on-my-sleeve-ai-ghostwriter-drake), are charting with AI-generated songs. As these things get cheaper and easier to create, the rest of what we consume on the internet must not be far behind.

# Starting today, everyone loves documentation

Anyway, earlier this week, Atlan, a data catalog company,[^2] [announced Atlan AI](https://www.businesswire.com/news/home/20230516005578/en/Atlan-Unveils-Atlan-AI-Ushering-in-the-Future-of-Working-with-Data):

> Starting today, everyone loves documentation. 
> The importance of documentation cannot be emphasized enough, especially with a lot of teams operating in hybrid or remote environments. But documentation of data, especially when done manually can be a tedious experience. 
> Atlan AI is set to automate the documentation process. It can document hundreds of assets in just minutes, making it especially helpful for those who have just started their data governance journey and have a backlog of data assets with missing documentation.

As the [launch site](https://atlan.com/ai/) says, today, every dataset or table in your warehouse has to be manually documented; soon, Atlan AI will do it for you, [cranking out hundreds READMEs](https://youtu.be/OnWvuC-XTDo?t=19) with a single click.[^3] 

One theory you could have about this is that it’s very bad. It could be further proof of the inevitable cheapening of digital content, and the beginning of us polluting our internal documents with errors and inaccuracies. Even if generated content is meant to be edited and reviewed by a person, it’s unlikely to stay that way. “Drafts” will leak through, or people will mechanically approve articles while multitasking through *Ted Lasso*.

A second theory you could have is that it’s very good. A notorious problem with data catalogs (and probably any other sort of similar internal resource) is that someone has to [author and maintain the catalog](https://locallyoptimistic.com/post/data_dictionaries/). It’s a thankless task that few data teams have any desire to do, and is precisely the sort of job that interns are asked to grind through—and therefore, is also precisely the sort of task that’s well-suited for an unfeeling, unsleeping, unpaid AI.[^4]  

But a third theory you could have is that it doesn’t make any sense. If a computer can manufacture documentation on the fly, *what’s the point of writing it down at all? *Why dutifully record a bunch of notes about something if I can conjure a new—and presumably, more up-to-date—version of it whenever I need it?[^5] 

One very reasonable answer to that question is we have to write it down because we’ve long assumed that documentation would be written down. We don’t look things up by asking questions, not exactly; we look things up with *search*, and that only works if we have something to search for. And so, at least in the short term, Atlan automatically creating documentation very much makes sense. 

But that default to search—long-standing though it may be—isn’t fixed. As we become more accustomed to a world where things can be made in an instant, the immediate, on-demand answers from Atlan’s chatbot assistant may be all the documentation we need.[^6] 

# The fast and the free

The point here isn’t really about Atlan, or even data documentation. It’s about the distinction between *free* content creation and *fast* content creation. When we talk about the effects of generative AI, we tend to project forward from the idea that it’ll be cheap to make stuff. For better, we’ll make more and a wider range of things; for worse, we’ll mass-produce creative content just as we mass-produce physical goods. To imagine our future, we imagine a world full of factories for manufacturing cheap creativity. 

If that’s where this is all headed, it’ll undoubtedly change a lot of how the world works. However, I’d guess that the more interesting effects will come not from the creation cost of content falling to zero, but from the creation *time *falling to zero. Consider a handful of examples:[^7]

In other words, price changes mostly affect how much of something we consume. Speed changes how we consume it—and the effects of that can be much more profound. The same principle seems to apply to content generated by AI:

This is why I’m skeptical that generative AI will turn the internet into content mills like [eHow](https://en.wikipedia.org/wiki/EHow) and [Answers.com](https://en.wikipedia.org/wiki/Answers.com). These sites can only dominate if we continue consuming content in the same way—search, click, search, click. Imagine, though, what the future of looking for a recipe might look like. Today, we search for “sprinkle sugar cookie,” get a million results, choose the first result from a reputable-sounding domain, [scroll past the childhood story about maple syrup sugarhouses in Vermont](https://twitter.com/chelseaperetti/status/1065627325960675328), and decide if this is the recipe we want to make. As ChatGPT, Bard, and Bing’s chatbot get better, we’ll either go straight to [our favorite site](https://smittenkitchen.com/2016/05/confetti-cookies/), or we’ll ask the bot for a recipe. Don’t have an ingredient? Instead of searching again, we’ll ask it to make a substitution. Mass-produced recipe sites don’t proliferate; they go away. And the infamous stories that come before the recipes? They’re [there for Google](https://onezero.medium.com/war-peace-bacon-eggs-understanding-the-endless-recipe-site-preamble-a890b3c55ad), and probably go away too.

Though this is a small example, I think it’s illustrative of how the longer-term effects of generative AI may not be to produce more internet, but a different internet. So much of what exists today is built on the almost-now-invisible assumption that content takes time to produce, and search and social media the primary arteries of an overwhelming majority of traffic around the web. [Unless this AI circus is a giant bubble](https://twitter.com/fchollet/status/1612144400725082113), my guess is that neither of those assumptions will be true for much longer. And I’d bet the companies and products that are built for that world—that extrapolate their visions of the future from the idea that content will be *immediate* as much as content will being *free*—are the ones that will ultimately be the movement’s big winners. 


---


[^1]: One hilarious example: This [launch video](https://techcommunity.microsoft.com/t5/viva-engage-blog/announcing-copilot-in-viva-engage/ba-p/3771323) of Copilot in Viva Engage. (Lest you think I routinely watch marketing videos for obscure Microsoft products with names as compelling as Viva Engage, I came across this video because Viva Engage is what Yammer, a product I used to work on, turned into.) In the video, our corporate protagonist is, first, recommended to “join the conversation” about a food drive at her company; second, given six paragraphs of machine-generated text for her post; and third, handed a few links and pictures to attach to it. At the end, she adds her only contribution: A single sentence that says “this cause means so much to me personally.” It sure seems like it does, Carole.

[^2]: Data catalogs are basically encyclopedias for data assets, like tables and dashboards. Want to know what the “sector” column means on the “customers” table, or which dashboards use data that’s derived from Salesforce? A data catalog can (theoretically) tell you.

[^3]: The upcoming release includes a hefty list of other features, like natural language search, a data discovery virtual assistant, a SQL interpreter that explains SQL logic in plain English, and a chatbot that can turn a question into a query.

[^4]: I do wonder if being polite to ChatGPT generates better responses. It doesn’t seem crazy to think it would? On Google, I’d guess that pleasantries like “please” and “thank you” could make a search query fuzzy and imprecise, and lead to worse results. But on ChatGPT, this could tilt the model towards responses like those in which people had been polite in the training data. For some questions, like asking how to do something, that nudge could be helpful. (That said, for others, like asking for a good joke, I could see it actually producing worse responses.)

[^5]: An analogy for the geriatric millennials: When school lessons were entirely analog, every statistics textbook had dozens of conversion tables printed in the appendix. Whenever you wanted to convert a [z-score](https://en.wikipedia.org/wiki/Standard_score) to a [p-value](https://en.wikipedia.org/wiki/P-value), you had to look this up in the back of the book. Get a z-score of 1.9433; round it to 1.94; flip to [table C1](https://onlinepubs.trb.org/onlinepubs/nchrp/cd-22/manual/v2appendixc.pdf); go to the row for 1.9; to the column for 0.04; get a p-value of 0.9738. (Then, subtract this from one to get 0.0262; double it to get 0.0524; throw out some “outliers” and “unreliable observations” until you can nudge it under 0.05.)It was an imperfect system. Because publishers could only dedicate so much space to these tables, they had to be truncated. You often could only look up values at important thresholds (e.g., for p-values of 0.1, 0.05, and so on), and had to estimate your result by extrapolating between them.Computers could’ve solved this problem in two ways: By creating a giant scrollable table with far more lookup values, or by letting people type in their input values and getting rid of the tables entirely. The second choice is obviously better. And that’s true even though these values are mathematically fixed. If the corresponding p-value for a z-score of 1.94 was constantly changing—sometimes it was 0.9738, sometimes 0.975, sometimes 0.9, sometimes null, and sometimes down for maintenance while we figure out why it was null—the second option only gets better.

[^6]: There’s also another reason I’m bearish on the long-term value of traditional data catalogs. Roughly speaking, data catalogs are instruction manuals for how to understand and use data. A number of early AI data products, like Atlan AI and [ThoughtSpot Sage](https://www.thoughtspot.com/product/sage), are efforts to make more intuitive interfaces for interacting with data. As all of these tools get better, the need for the instruction manual gets smaller—like how, despite everything it can do, an iPhone manual is a five page picture book.

[^7]: It is with great self-loathing and disgust that I’ve made these examples [bullets](https://benn.substack.com/p/service-pressure#:~:text=I%20have%20a,and%20no%20verse.).

[^8]: Or [SF, Miami, and Austin](https://benn.substack.com/p/do-data-driven-companies-win?utm_source=%2Fsearch%2Faustin&utm_medium=reader2#:~:text=You%20live%20in%20San%20Francisco%2C%20work%20in%20Palo%20Alto%2C%20daydream%20about%20Miami%2C%20and%20are%20moving%20to%20Austin.).

[^9]: I’m convinced that one day, [Dominic Toretto](https://en.wikipedia.org/wiki/Dominic_Toretto) won’t just drive a [Dodge Charger](https://www.youtube.com/watch?v=_A3qSJ4H7JI), and James Bond won’t just wear an [Omega](https://www.omegawatches.com/en-us/planet-omega/cinema/james-bond); they’ll drive and wear whatever car and watch you just Googled.

================================================================================

# Microsoft builds the bomb

*What Fabric is, and what I want it to be. *

---

![Oppenheimer' Poster: Doomsday Approaches for Cillian Murphy](https://substackcdn.com/image/fetch/$s_!bIyl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33949711-494c-4b6e-87e1-855c9de2164a_1400x700.jpeg)

*It is a mistake to think that you can solve any major problems just with potatoes.*

*– Douglas Adams*

The modern data stack was always an ambitious project. The technology alone was complicated enough—collectively, we needed to pick up a dozen major enterprise applications, launch them into the cloud, and make them all work at a staggering new scale for a fraction of the cost. But in hindsight, this was the easy part. The much more audacious dream was remaking how this technology is built and sold. Historically, data products had been designed to meet the needs of enterprise [RFP](https://en.wikipedia.org/wiki/Request_for_proposal) checklists; they were built by massive development teams in Oracle and SAP office parks; they’d packaged as suites with mandatory professional services upcharges; and they were sold to IT executives on golf courses and in box seats at Warriors’ games. 

The modern data stack promised something different: Products built for the people in the trenches, designed by people who came from the trenches,[^1] and demoed, trialed, bought, and championed from the trenches. Analysts and data engineers shouldn’t have to use aging tools imposed on them because a CIO was cross-sold IBM® ILOG® CPLEX® Optimization Studio[^2] over a $400 closing dinner at Ruth’s Chris; they should get to choose whatever products are best for them. 

Modularity—interconnected tools, each of which excels at their own specialty—became the means to this end. In theory, it’s a great idea. Businesses want to do thousands of different things with data, and often have contradictory needs—banks, for example, care about scale and security while startups care about speed and cost. People have their own [irreconcilable and idiosyncratic preferences](https://benn.substack.com/p/bi-by-another-name#:~:text=The%20front%20end%2C%20by%20contrast%2C%20is%20the%20interface%20between%20technology%20and%20humans%E2%80%94and%20people%20won%E2%80%99t%20ever%20agree%20on%20the%20best%20way%20to%20do%20anything.). Titanic monoliths [can’t solve all of these problems](https://benn.substack.com/p/case-for-consolidation/comment/6537471), nor can they respond quickly to market shifts. Ideally, rather than having to buy some generic all-in-one product, data teams should be able to mix and match pieces to create the perfect potato head, [subbing one thing out for another when their needs change](https://www.youtube.com/watch?v=DN5nN67EwBg). 

But this plan has always had one big problem: There is no potato. 

The modern data stack has never had a platform. There’s never been an operating system on which everything could sit. There’s no central service that handles user identity and authentication, or data connectivity, or access management, or content organization.[^3] When people have new ideas for improving how we all work with data, there’s no central platform to plug the technology into. To launch a feature, you have to build a product. 

Though this problem affects the entire industry, it’s painfully apparent in the BI market. Nearly every product in the space first sold itself under [a more specialized banner](https://benn.substack.com/p/business-in-the-back-party-in-the-front#:~:text=Instead%2C%20the%20front,is%20coming.) than BI. But over time, with no central platform to lean on, the narrow products couldn’t stand on their own. Customers asked for visualizations, then dashboards of visualizations, then self-serve explorations of dashboards of visualizations—until customers just asked for BI.[^4] 

This isn’t a failing of any one vendor; it’s a failing of the market. It’s a failing that potentially powerful innovations—like advanced [visualization](https://observablehq.com/) [engines](https://mode.com/visual-explorer/), insight-inferring [AI](https://www.inference.io/) [models](https://sisudata.com/product), clever [new](https://www.motifanalytics.com/) [languages](https://www.malloydata.dev/), and [reimagined](https://deepnote.com/) [notebooks](https://hex.tech/blog/hex-two-point-oh/)—have to be watered down by redundant BI features before they can widely be brought to market. 

Ten years in, [this the legacy](https://benn.substack.com/p/the-modern-data-experience) of the modern data stack: Dozens of creative and delightful features, often buried inside of overlapping and overweight products, all clumsily bumping into each other in a soup of startups and data services.

*They won't fear it until they understand it. And they won't understand it until they've used it.*

*– *[Oppenheimer](https://twitter.com/OppenheimerFilm/status/1604634869404880896)

This week, Microsoft decided that it’d seen enough. [To roughly paraphrase their statement to the press](https://techcrunch.com/2023/05/23/microsoft-launches-fabric-a-new-end-to-end-data-and-analytics-platform), there’s gold in them modern data stack hills, but the prospectors are too disorganized to mine it:

> “Over the last five to 10 years, there has been a pretty massive level of innovation — which is great and that’s awesome because there’s lots of new technologies out there — but it’s also caused a lot of fragmentation of the modern data stack,” Arun Ulag, Microsoft’s corporate VP for Azure Data, told me. “There’s literally hundreds — if not thousands — of products and open source technologies and solutions that customers have to make sense of.” He also noted that a lot of the data and analytics products tend to keep their data in silos. “When I talk to customers, one of the messages I hear consistently is that they’re tired of paying this integration tax,” he said.

Microsoft unveiled its solution—to nuke the entire modern data stack off the face of the earth. Enter [Fabric](https://www.microsoft.com/en-us/microsoft-fabric):

> “There’s a unified compute infrastructure; there’s a unified data lake. There’s a unified product experience for all your data professionals, so that they can really collaborate deeply. [There’s] unified governance so that IT can manage this and create sources of truth that everybody can trust, and really a unified platform that both IT and business share — and the unified business model. There’s literally just one thing to buy, and it allows customers to save a lot of costs, which, especially in today’s environment, is really important,” said Ulag.

Fabric’s feature list is impressively comprehensive. It included data integration services, SQL workspaces, notebooks, observability and threshold-based reverse ETL capabilities, data exploration and reporting provided by PowerBI, and an omnipresent OpenAI chatbot called Clippy Copilot. Most of the modern data stack’s various categories are represented, built for [Edge](https://en.wikipedia.org/wiki/Microsoft_Edge) and typeset in [Segoe](https://en.wikipedia.org/wiki/Segoe).[^5]

Ultimately, however, the most notable thing about Fabric isn’t its features, but—as its name implies—the knitting between them. Fabric isn’t a suite of loosely connected services; it’s a [single application](https://www.youtube.com/watch?v=1o_QDFq6gzE&t=1564s) that has one login experience, one user interface, one storage layer, one permission model, and one monthly bill.[^6] Whereas each feature has a counterpart elsewhere, this connectivity is unique. 

Of course, none of this is a surprise. Microsoft has run this play before; a year ago, I said they’d probably [run it again](https://benn.substack.com/p/case-for-consolidation#footnote-anchor-8-53744915:~:text=consolidate%20everything%20under%20one%20branded%2C%20proprietary%20banner%2C%20make%20it%20all%20work%20together%2C%20and%20send%20out%20an%20army%20of%20sales%20reps%20to%20steamroll%20the%20rest%20of%20us%20with%20it.%C2%A0), and “consolidate everything under one branded, proprietary banner, make it all work together, and send out an army of sales reps to steamroll the rest of us with it.” The question now, I suppose, is do we actually get steamrolled?

The first answer seems like no? Microsoft products are anathema to a lot of teams, and some überMicrosoft leviathan of Synapse and [OneDrive](https://blog.fabric.microsoft.com/en-us/blog/microsoft-onelake-in-fabric-the-onedrive-for-data/#:~:text=OneLake%20servs%20as%20the%20OneDrive%20for%20data.) probably isn’t going to change that. To the extent that some customers’ problem with Microsoft is that it’s Microsoft, Fabric is a step in the wrong direction.[^7] 

Still, write Microsoft off at your own risk. One segment of customer for whom Microsoft *isn’t *toxic, and is often preferred, is the enterprise—i.e., the segment with most of the money. As I [said in the last year’s post](https://benn.substack.com/p/case-for-consolidation#:~:text=Consolidation%20and%20convenience%E2%80%94especially%20for%20enterprise%20buyers%E2%80%94sells%20better%20than%20a%20delightful%20product.%C2%A0), “consolidation and convenience—[especially for enterprise buyers](https://www.vox.com/recode/2019/9/19/20874094/slack-startups-microsoft-kruze-etr-charts)—sells better than a delightful product.” Microsoft knows this, and is already [on message](https://techcrunch.com/2023/05/23/microsoft-launches-fabric-a-new-end-to-end-data-and-analytics-platform/#:~:text=Ulag%20noted%20that%20he%20personally%20demoed%20Fabric%20to%20100%20of%20the%20Fortune%20500%20over%20the%20course%20of%20the%20last%20year%20and%20that%20many%20enterprises%20are%20excited%20about%20it%20because%20it%20greatly%20simplifies%20their%20data%20infrastructure): “Ulag noted that he personally demoed Fabric to 100 of the Fortune 500 over the course of the last year and that many enterprises are excited about it because it greatly simplifies their data infrastructure.” In this regard, the modern data stack may not get steamrolled, but stunted, frozen out of the top of the market by a Microsoft sales team that already has a standing tee time with every enterprise CIO.

Moreover, even if Microsoft’s execution isn’t perfect, Fabric still presents one new danger to the modern data stack: It gives buyers an integrated alternative. To this point, the only unified data platforms have been legacy ones, and the only modern data platforms have been fragmented. We haven't seen just how much—or how little!—data teams are frustrated by that fragmentation, since it's been part and parcel of products that are SaaS-based, cloud-first, and at least gesture towards a consumer-grade user experience. Fabric changes that. The choice is no longer legacy versus modern, but all-in-one versus best-of-breed. Place your bets, I suppose.

*Let's kill the dreamer…and see what becomes of the dream.*

*– [Genesis 37:19-20](https://www.biblegateway.com/passage/?search=Genesis+37%3A19-20&version=NABRE)*

Or—Fabric is an overstep. After all, the dream of a modular *and* integrated modern stack isn’t impossible; it just needs, [with all due respect to Douglas Adams](https://www.goodreads.com/quotes/11056-it-is-a-mistake-to-think-you-can-solve-any), a potato. In Fabric, Microsoft built a potato and a bunch of facial features. That may be appealing relative to what else is available—monolithic legacy tools, or disjointed modern ones—but it’s probably not ideal. Fabric’s attachments—the pipeline tools, the SQL and Python workbooks, the AI integrations—will fall behind other vendors. Customers will prefer [how other products’ facial features look](https://twitter.com/mim_djo/status/1661516313192452096). Some hot new thing will come out, and Microsoft [won’t be able buy special rights to it](https://openai.com/blog/openai-and-microsoft-extend-partnership).

In this way, Fabric could level the modern data stack, but [make its promise more powerful](https://www.youtube.com/watch?v=iVBX7l2zgRw). Imagine, after all, if we had something like Fabric with more choice on top? What if Microsoft’s launch announcement ended on [this slide](https://www.youtube.com/watch?v=1o_QDFq6gzE&t=1564s), promising to build a platform that would handle the messy administrative needs that today’s data products have to all reproduce? What if Microsoft made its goal to enable small teams of developers to build data components—true components, like a [better experimentation dashboard](https://www.geteppo.com/), or a [code-free tool for managing metrics](https://www.preql.com/), or [a new way to analyze funnels](https://motifanalytics.medium.com/everything-is-a-funnel-but-sql-doesnt-get-it-c35356424044)—without having to rebuild half of Tableau in the process? What if Microsoft built a potato?

Or, perhaps the more interesting question is, what if *AWS* built a potato? Microsoft just committed to building the whole stack; Google seems to be [haphazardly working](https://benn.substack.com/p/the-original-purple-people) their way in the same direction. Amazon, by contrast, has always been more comfortable building developer platforms than SaaS applications. Plus, [after pioneering the data industry’s move to the cloud](https://www.getdbt.com/blog/future-of-the-modern-data-stack/#:~:text=In%20my%20opinion%2C%20the%20modern%20data%20stack%20catalyzed%20around%20the%20release%20of%20Amazon%20Redshift%20in%20October%20of%202012%2C%20and%20hanging%20this%20massive%20painting%20at%20the%20entry%20to%20our%20office%20memorialized%20its%20historic%20importance.), AWS has since faded from the lead. Snowflake, Databricks, and BigQuery are outmaneuvering Redshift and Athena; Microsoft and Google are dominating the AI arms race.

That race doesn’t need another bomb. Make love unified administrative service layers for enterprise data storage and compute applications, not war.


---


[^1]: Mode's original tagline was "by analysts, for analysts." Flavors of this construct—we were you; we get you; we built this because we wish someone had built it for us—is all over modern data stack marketing materials.

[^2]: This is the [actual name](https://www.ibm.com/products/ilog-cplex-optimization-studio#:~:text=IBM%C2%AE%20ILOG%C2%AE%20CPLEX%C2%AE%20Optimization%20Studio), registered trademark symbols and all. (Also, IBM has a [“certified pre-owned” program](https://www.ibm.com/financing/pre-owned/ibm-certified-used-equipment)?? Like, I get it; it’s hardware; makes sense. But imagine if software vendors did this. Buy a certified pre-owned dbt project! It’s never been through a major rewrite, only has a couple thousand runs on it, and our licensed professionals have verified that the Salesforce models are still good for a couple thousand more.

[^3]: Databases *kinda *do this. But, because warehouses don’t directly connect other products to each other—a BI tool, for example, can only infer what an ETL tool is doing by looking at the data the latter leaves behind—they’re more centralized gathering points than proper development platforms.

[^4]: Back in Mode’s early days, I insisted that we call ourselves an advanced analytics platform; today, we’re [Modern BI](https://mode.com/blog/introducing-mode-as-modern-bi/). Transform wanted to be a metrics store and a semantic layer; they [became a dashboarding tool](https://blog.transform.co/product-news/introducing-boards/). Narrator started as a [new standard for data modeling](https://www.producthunt.com/posts/narrator-2); it now sells a self-serve data platform. Hex [took down a blog post](https://web.archive.org/web/20230207150030/https://hex.tech/blog/bi-tools-hex/) that said Hex should complement rather than replace BI tools. dbt Labs has to [periodically remind people](https://twitter.com/jthandy/status/1469049101421527046) that BI isn’t on their roadmap. And the same fate awaits the new Slack chatbots and AI integrations that—so far—also insist this time will be different.

[^5]: In a humble homage to [Wolmania’s excellent weekly lists](https://newsletter.wolmania.com/friday-may-26-2023/), the cloud provider fonts, ranked: 1. [Ember](https://developer.amazon.com/en-US/alexa/branding/echo-guidelines/identity-guidelines/typography) (Amazon); 2. [Roboto](https://m3.material.io/styles/typography/fonts) (Google); 3. [Segoe](https://developer.microsoft.com/en-us/fluentui#/styles/web/typography) (Microsoft).

[^6]: Or at least, that’s the pitch. When these big integrated amalgamations are cobbled together from existing applications, there tend to be a lot of seams between services. I have no idea if that’s the case here or not, though given how much they said it wasn’t, I’m inclined to believe they’ve put a lot of work into the transitions.

[^7]: More importantly, the default answer to the question, “Will this new product change everything?” is always, “No, it will struggle to get traction, people will keep doing what they’re already doing, the product will chase adoption by replicating the features that people use in existing products, and [it will eventually start to look like everything else](https://www.youtube.com/watch?v=An548MR0R6s), until it fades off into the distance.” But that makes for a boring blog post; much better to melodramatically speculate about extreme outcomes with Bible verses and Christopher Nolan quotes.

================================================================================

# A gambler's guide to giving talks

*A bewildered audience is better than a bored one.*

---

![Val Kilmer as Doc Holliday gambling Tombstone set 1993-2015 Photograph by  David Lee Guss - Pixels](https://substackcdn.com/image/fetch/$s_!CmVN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6795f08-2d62-4c51-b64f-44f6f6016aad_900x463.jpeg)

If you're good at something, [they say](https://www.youtube.com/watch?v=iijs0iS3_ZM), never do it for free. 

Accordingly, nobody’s ever paid me to give a talk. But I’ve given enough free ones that I’ve realized that there are only two types of presentations: Forgettable ones, that waste away on YouTube with 18 views; and memorable ones, that bestow their creators with the lifelong benefits of having done something Good. 

Nearly every talk is the former, and nearly everyone goes to every talk expecting it to be the former. With the exception of presentations that are intentionally inflammatory or inappropriate, presentations have a floor for how bad they can be—bland and immediately forgotten—and people's expectations are an inch above that floor. 

That sounds bad, but it has a very useful implication: There's very little risk in taking chances. A daring talk that flops and a cautious talk that safely sticks to all the comfortable best practices are equally ignored. But stick the landing on something bold, and the upside is unbounded.[^1] 

So, when giving a talk, for your sake and your audiences’, gamble. Here are some ideas—worth, probably, exactly what I was paid to write them down—for how to do it.

## Be entertaining, not educational

People often say, at what they feel is some key moment of their presentation, “if you take anything away from this talk, this should be it.” To me, this sentiment is profoundly misguided. Most people don’t go to talks to learn,[^2] most of the things that we have to teach aren’t that interesting, and the only thing that most people will take away from most talks is either a mild feeling of annoyance that it was so boring, or a pleasant feeling of surprise that it wasn’t.[^3] 

The most memorable presentations are the ones that acknowledge this reality. They’re the ones that are written as entertainment first, and that care about how the audience feels *during* the half-hour that they’re trapped in a conference hall with no discrete ways out rather than what the audience does after the leave. They’re the ones that worry more about keeping people’s attention than changing their minds. [They’re movies, not documentaries.](https://www.freep.com/story/entertainment/movies/2014/09/11/michael-moore-to-filmmakers-entertain-people-dont-lecture-them/15462135/#:~:text=%E2%80%9CStop%20making%20documentaries!%20Start%20making%20movies!%E2%80%9D)

Admittedly, “be memorable and entertaining” is can sound like a daunting task, and often is. But it *is* the task, and pretending otherwise only makes it harder. 

## Go faster

Nearly every primer on public speaking reminds presenters to slow down. People talk quickly when they’re nervous, the guides say; you’ll know you’re going at the right pace [when it feels way too slow](https://www.jenniferhennings.com/jennifer-hennings-blog/3-ways-to-slow-down-when-youre-nervous#:~:text=Does%20my%20pace,I%27m%20doing%20okay.).

This isn’t necessarily bad advice, but it’s often interpreted in a bad way. When people slow down their speech, they often slow everything down. They repeat themselves; they add pauses to let their points breathe; they write ten minutes of content for a twenty minute presentation, to force their pace to a crawl. 

If the point of a talk was to [hammer one point home](https://www.youtube.com/shorts/0Y_nEgVl4h4), this might make sense. But as entertainment, this tempo is [excruciating](https://www.youtube.com/watch?v=FqOJSwBjH6k). What’s more, if an audience realizes that they can stare at their phones for a few minutes and not miss anything important—it’ll get repeated later, and summarized on a slide at the end—they will. 

Instead, give me a talk that keeps me on my toes. Give me one that assumes I’m bored and distracted—and smart enough to keep up with the pace of a normal conversation. Give me a talk that is sixteen minutes long instead of twenty, and actually needs all sixteen of those minutes.[^4] Make me pay attention, because I’m afraid of what I’ll miss if I don’t.

## The Kroto method

The most memorable talk I’ve ever seen was a lunchtime keynote by [Harry Kroto](https://en.wikipedia.org/wiki/Harry_Kroto), a Nobel laureate in chemistry, at an undergraduate research conference in Tallahassee in 2004. I have no idea why I was there, what the conference was about, or what he said—but I’ll never forget how he said it. I was in some giant ballroom, eating lunch with the other attendees at one of a couple dozen circular tables. When Kroto was introduced, we all half-glanced at him, expecting to sit politely while he bored us with a lecture about the importance of science, or something. 

Instead, he was mesmerizing. In thirty minutes, he machine-gunned through about four hundred slides, never breaking his brisk rhythm as he did it.[^5] 

It was similar to the [Lessig method](https://presentationzen.blogs.com/presentationzen/2005/10/the_lessig_meth.html)—lots of slides, each with a single word, short quote, or simple image—but with a crucial modification. Lessig’s slides tend to be inessential, used as a way to underline certain words as he says them. Kroto’s slides were more clever. They told a kind of parallel story, making indirect jokes or emphasizing his points with visual analogies. For example, had Lessig said, “AI has become a worldwide phenomenon,” he might show a globe, or the word “AI” in a giant letters. Had Kroto said the same thing, he might show a picture of a sold-out show on Taylor Swift’s [Eras Tour](https://en.wikipedia.org/wiki/The_Eras_Tour), or made some callback to a previous mention of a different worldwide phenomenon he has referenced earlier.

This style had three effects. First, by using something like the Lessig method, he solved the riddle about pacing: The constantly changing slides made his presentation *feel* fast, even though he was talking at a relatively moderate clip. Second, his addition to the method—using pictures that told their own story—made it impossible to turn away because you didn’t know what you might miss. Blink, and the joke might be gone. And finally, the internal references and callbacks gave the talk a sense of dazzling cohesion, like an episode of *Arrested Development *that [continually weaves a half-dozen plotlines](https://www.vulture.com/2013/05/398-arrested-development-quotes-jokes-easter-eggs.html) together at once.

[Some people might say](https://kozyrkov.medium.com/dressing-for-the-stage-theatre-rules-apply-90a29614ad67#:~:text=time%20before%20you.-,Slides,-The%20fewer%20words) that this is distracting, or that you should write talks that can stand alone without the slides. On the first point, I partially disagree. Though pairing this style with complex slides would be confusing, an engaged audience is plenty capable of following a talk’s main narrative and its visual footnotes at the same time. On the second point, I fully disagree. Use the medium—a talk and slides—to its fullest. Giving a presentation that can be heard without being seen is like creating a movie that doesn’t need the picture. It does nothing but make a hard job—keeping easily distracted people entertained—even harder. 

## Rhyme off the beat

Early rap songs tended to rhyme on the beat, with simple rhyme schemes. This pattern creates [a monotonous, sing-songy effect](https://www.youtube.com/watch?v=mcCK99wHrk0) that follows a very predictable wave. At best, it’s dull; at worst, it’s clunky and forced.

Most talks have an analogous flow. Presenters change slides “on the beat”—they make a point, say they’re moving on to their next idea, and change the slide. Or worse, they put up a slide that’s meant to tell a joke, and then, with the slide up the whole time, deliver the setup and the punchline. 

Nothing about this is engaging. There’s no tension or suspense. It also gives the audience permission to tune out—if a slide's headline is uninteresting, you can safely drift away until the slide changes, and decide if you want to start paying attention again.

Better presentations [rhyme off the beat](https://www.youtube.com/watch?v=QWveXdj6oZU). Slide transitions happen in the middle of sentences, without warning. More subtly, the best transitions have no [awkward anticipatory hitch](https://www.theatlantic.com/technology/archive/2022/08/tiktok-gen-z-millennial-pause-parody/671069/) before changing slides—”and sooo…”—and no [pleading pauses](https://www.youtube.com/watch?v=OUXvrWeQU0g) that give the audience time to “get the joke.” Like great rap songs, they expect their listeners to keep up.

This flow makes presentations more unpredictable and—especially if the talk uses a lot of slides—better keeps an audience’s attention, since they don’t know what they might miss if they look away.

## About the jokes: Embrace obscurity

A riskier suggestion: Include jokes, and it’s fine if they’re obscure. 

Will everyone get the Eras Tour reference? Probably not, but the people who do get it will appreciate what feels like an inside joke. If a presentation has enough of these sorts of references—and with a lot of slides, there are a lot of opportunities to include them—most people will get at least a few. 

These kinds of Easter eggs are far more memorable than generic jokes that most people get. Everyone will understand [Boromir saying](https://knowyourmeme.com/memes/one-does-not-simply-walk-into-mordor), “One does not simply…deploy to production;” nobody will laugh at it. 

More importantly, to the audience, somewhat obscure references are a [variable reward](https://www.psychologytoday.com/us/blog/brain-wise/201311/use-unpredictable-rewards-to-keep-behavior-going) for paying attention. That’s much better for keeping people engaged than a steady dose of familiar memes.

## Focus on the transitions

Most people seem to develop presentations in roughly three steps: First, they create an ordered outline of the points they want to make. Next, they turn each point into a slide, and fill the speaker notes with how they want to talk about it. And finally, they practice speaking to each slide. 

Instead, when people are creating presentations, they should spend more of their time working on the transitions between their points, and less on the points themselves. 

When you practice the points, it’s easy to turn a talk into a list of key takeaways, presented one after the other, with no narrative glue connecting them together. Because most presentations’ [action items](https://benn.substack.com/p/service-pressure#:~:text=I%20have%20a,and%20no%20verse.) aren’t actually very novel or interesting,[^6] it’s very hard to keep an audience’s attention this way. Talks need a story to keep the audience engaged, and stories are told in the transitions.[^7] 

There’s also a practical benefit to rehearsing transitions: People are usually already comfortable talking about the key points in their presentations. Even if these were the most important parts, they’re often the easy parts. The unnatural thing about giving a talk isn’t riffing on some subject that you’re an expert in; it’s packing that subject into a cohesive chronology. We should practice that part.

## Be careful with coaching—including this post

For huge talks—ten-thousand person keynotes, stadium speeches, televised addresses—presenters likely have to play to the medium.[^8] At such a distance, a speaker has to *act* authentic, not be authentic. Under such bright lights, [wear makeup](https://constitutioncenter.org/blog/the-debate-that-changed-the-world-of-politics#:~:text=Nixon%20also%20refused,o%E2%80%99clock%20shadow%20beard.).

For most of us, however, our half-full classrooms and two-foot-high stages are intimate enough that the opposite is true: Unless you’re a professional dedicated to the craft, acting feels fake—and audiences are far better at sniffing this out than we are at covering it up. For this reason, I'm generally skeptical of any sort of prescriptive coaching about how to write and give presentations. When people go through a handful of generic coaching sessions, they often start to hide their personalities behind a few clumsy "best practices:" [Speak slowly](https://hbr.org/tip/2018/08/when-presenting-speak-slowly-and-use-simple-words); follow the [hero's journey](https://www.oreilly.com/library/view/how-to-be/9781292087993/html/chapter-008.html); tell a [humorous personal anecdote to connect with your audience](https://trainingmag.com/storytelling-and-humor-in-public-speaking/#:~:text=Trainers%20and%20speakers%20often%20use%20humor%20as%20an%20ice%2Dbreaking%20tool%20to%20lead%20their%20audiences%20to%20the%20actual%20content.%20Keeping%20the%20audience%20profile%20in%20mind%2C%20use%20humor%20to%20connect%20with%20your%20audiences.%20The%20tools%20and%20techniques%20deployed%20differ%20from%20situation%20to%20situation.). 

It’s safe advice—and devastatingly boring. If you're a CEO presenting financial results on an earnings call and more worried about getting sued than keeping people on the line, by all means, do these things. But if you’re trying to keep people from drifting off into the middle distance, nothing is worse than the mechanical affect of a sporadically coached speaker.[^9] Far better to just be yourself.

So—the best thing to do with this post may be to ignore it. While this style is what’s comfortable for me, it might be a bad fit for other people. 

But don’t ignore it because it’s risky; that gamble is exactly the point. Because most talks are performative busywork: The speaker gives it for themselves, either for their resume, their ego, or to hear themselves talk. The audience shows up out of a vague sense of obligation, and listens because they’re there. Everyone nods along, goes home feeling like an accomplished white-collar professional, and pretends that the whole charade is useful. 

Give us the red pill instead. Take a chance, show us down a rabbit hole, and find out how far it can take you.


---


[^1]: Literally—the possible benefits include everything from making a few useful connections to [becoming the president of the United States](https://www.youtube.com/watch?v=eWynt87PaJ0).

[^2]: I’d estimate that, at any given talk, twenty percent of the attendees are there because they’re deeply interested in the subject at hand, and are hoping they might hear something unexpected. Another twenty percent of the attendees are trying to decide if they’re interested in the subject, and are using the talk to figure out if it’s worth learning more. Fifty-nine percent are there because their employer paid for their conference ticket; despite actually going for the free happy hours and three days away from the kids, they’d feel guilty for if they didn’t go to at least a few talks; plus, there’s a corporate policy that anyone who goes to a conference has to write a perfunctory book report on what they learned while they were there. And the final one percent is there like an eager college student, looking to absorb as much knowledge as they can, writing down every idea, excited to put them to practical use. Regrettably, most presentations are written as though the entire audience is in the last group.

[^3]: They might also take away a few pictures of slides that will rot in either their Photos app or in a Slack channel called #random-industry-chatter.

[^4]: [Nobody finishes](https://www.thewrap.com/irishman-finish-how-many/) a three-hour-and-29-minute movie that could be half as long; nobody stays engaged in an hour-long meeting that should’ve been fifteen minutes; nobody pays attention to a twenty-minute presentation that has six minutes of content.

[^5]: [This presentation](https://www.youtube.com/watch?v=5ar8SetE09A) isn’t the talk we saw, but it’s a similar style.

[^6]: Takeaway one: We need to focus on business problems, not technology problems. Takeaway two: Technology alone won’t solve anything. Takeaway three: Except the technology I’m selling, which you should buy.

[^7]: Moreover, working on transitions has a similar benefit as [writing in prose](https://benn.substack.com/i/46415813/bullets-kill): It tests ideas much better than bullets ever can. [As Jeff Bezos says](https://www.cnbc.com/2018/04/23/what-jeff-bezos-learned-from-requiring-6-page-memos-at-amazon.html), compared to bulleted decks, full sentences force “better thought and better understanding of what’s more important than what.”

[^8]: Or this at least seems true? I have no idea though. The biggest crowd I’ve ever talked to was one that was mostly there because the conference happened to be serving lunch at the back of the tent that I was presenting in. But if you have experience with these sorts of talks, 1.) hi, hello, this is an obscure data blog, why are you here?, and 2.) is this actually true?

[^9]: More precisely, I’d argue that the relationship between professional coaching and talk quality follows a long U-curve. Some coaching often makes a presenter worse, by sanitizing them of their own style. Eventually, with enough practice—far more than most of us will ever put in—the character that a presenter plays can become more interesting than the person that they actually are. But very few of us will get there, and are better off being ourselves than a half-drawn sketch of some [cyborgian ideal](https://www.newyorker.com/culture/decade-in-review/the-age-of-instagram-face).

================================================================================

# All I want is to know what's different

*Rather than trying to guarantee correctness, what if we tried to guarantee consistency?*

---

![Bill Walsh: A Football Life - The West Coast Offense - YouTube](https://substackcdn.com/image/fetch/$s_!x3Ei!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a5a9bb0-9b9b-41b4-b23f-315f3f1abe53_1280x720.jpeg)
*[take care of itself](https://www.amazon.com/Score-Takes-Care-Itself-Philosophy/dp/1591843472)*

Here are two methods for figuring out if some business metric is accurate:

As data people, we seem to favor[^2] the first approach. We create[^3] data contracts that catalog what we expect source data to look like when it arrives in our warehouses. We build observability tools that constantly monitor that data for outliers. We write tests in dbt that confirm dates are dates, numbers are numbers, and unique keys are unique. We set up alerts that tell us when a pipeline stalls, when a job errors out, or when we’ve hit Marketo’s API limit and can no longer sync marketing data into Snowflake. 

Despite all this effort, we still struggle to deliver metrics that people can trust. As Tristan [said a few days ago](https://roundup.getdbt.com/p/the-cultural-context-of-data#:~:text=If%20self%2Dservice%20has%20so%20far%20failed%20to%20achieve%20everything%20we%20know%20it%20can%2C%20this%20is%20a%20huge%20part%20of%20the%20reason.%20We%E2%80%99ve%20cast%20down%20a%20system%20that%20relied%20on%20human%2Dto%2Dhuman%20credibility%20and%20haven%E2%80%99t%20provided%20a%20clear%20alternative%20way%20to%20assess%20credibility%20to%20the%20people%20who%20rely%20on%20data%20to%20do%20their%20jobs.), we’ve built a massive technical system to support that goal, and yet, it’s still a system that most people side-eye—and some people work around entirely. The institution, to borrow Tristan’s term, is not good enough. 

You could argue, I suppose, that we’re just not there yet. These tools are new; we need to learn how to use them. Features are [still getting built](https://www.getdbt.com/blog/analytics-engineering-next-step-forwards/); best practices are [still being written](https://www.synq.io/blog/the-complete-guide-to-building-reliable-data-with-dbt-tests); data contracts are still uncommon;[^4] the data mesh is [still a dream](https://www.linkedin.com/posts/zhamak-dehghani_decentralized-data-is-the-future-nextdata-activity-7021533443884818432-nrY8/). Over time, we’ll build our cathedral, brick by brick, release by release, blog post by blog post. 

Maybe, but I’m skeptical. I’d contend that we’ve struggled to get people to trust our work because our approach to earning that trust—Method 1—is fatally flawed. The road from raw data to reliable metric has a limitless variety of potholes; there can be no system, no matter how complete or comprehensive, that can tell us we’ve patched all of them. Contracts, observability tools, data tests—these are mallets for playing whack-a-mole against an infinite number of moles. 

More importantly, Method 1 isn’t how other people decide if they should trust the things that we produce. They don’t check our various observability dashboards and continuous integration tests; they don’t blindly accept our results when all of pipelines behind them are green; they don’t say, “There are no errors in Fivetran and the PR got a ‘lgtm’ GIF, so sure, I’ll gamble my career on these numbers.” 

No, [everyone else uses Method 2](https://locallyoptimistic.slack.com/archives/CHF1E9NUS/p1686243955544949?thread_ts=1686243897.349009&cid=CHF1E9NUS): “Do I believe this number, given what I believed yesterday?” 

We do this so instinctively that we often don’t even notice. When I put together board decks, the most nerve-wracking moment was comparing the metrics in this quarter’s deck to the same numbers in last quarter’s charts. Nothing turns your stomach over faster than realizing you’ve accidentally revised all of your historical ARR figures. Conversely, there’s no better feeling than seeing all the graphs line up, and confirming that what you thought was true then is also what you think is true now.

In other words, trust isn’t built by confirming our sources; it’s built via [mathematical induction](https://en.wikipedia.org/wiki/Mathematical_induction). More than anything, our confidence in this month’s KPI report depends on our confidence in last month’s KPI report, and how well the historical numbers match across the two.

Of course, you could be wrong both times; matching numbers aren’t necessarily *right *numbers. But as far as rough and easily accessible heuristics go, it’s pretty good. And the more iterations that match—if a metric’s historical charts have been consistent for eight quarterly reports in a row—the more trust that it inspires.[^5] 

# To trust the output, test the output

So, when [Tristan asks](https://roundup.getdbt.com/p/the-cultural-context-of-data#:~:text=How%20do%20we%20empower%20data%20consumers%20to%20assess%20the%20credibility%20of%20MDS%2Dgenerated%20data%20products%3F), “How do we empower data consumers to assess the credibility of MDS-generated data products?,” that’s my answer: Tell me if what I’m looking at is the same today as it was yesterday. 

Don’t tell me that there aren’t any stale pipelines; timely syncs can still sync bad data. Don’t tell me that the tests are passing; even the best analysts [can’t come close to anticipating](https://www.youtube.com/watch?v=D48NQTNg19s) all of the ways that a “simple” problem can go wrong. Don’t tell me a report is verified, or is owned by someone I trust; so too is [Guy Fieri’s Cereal Milk Milkshake with Fireball and Marshmallow Vodka](https://www.youtube.com/watch?v=_DR8aYm7reE)™, and I’d still be skeptical of it if it was a month old. Instead, tell me that the dashboard that currently says we made [$3.7 million on the weekend of August 1, 2003](https://en.wikipedia.org/wiki/Gigli#Box_office) said the same thing yesterday, and the day before that, and in September of 2003.[^6] *That* checkmark—the one that says this number has been consistent for years, that this metric is [Lindy](https://en.wikipedia.org/wiki/Lindy_effect)—is one I’ll trust. 

This isn’t to say there isn’t value in checking the inputs; check those too![^7] But we should recognize that trust is built, and blown up, by the outputs—and specifically, the consistency of those outputs. If we want to build faith in our analytical institutions, we shouldn’t telling people what’s working or broken, or what’s right or wrong; instead, we should tell people what’s changed.

What might we build if that was the goal? A few ideas:

When we define key [entities](https://github.com/dbt-labs/dbt-core/discussions/6644), track how they evolve. If there’s a table of purchases or customers, let me define which columns should be immutable—purchase date, contract amount, initial customer segment, etc.—and tell me when they’re different today than they were yesterday. Don’t just validate source schemas and [run unit tests](https://towardsdatascience.com/how-to-do-unit-testing-in-dbt-cb5fb660fbd8) on dummy data, and assume that the combination will produce the correct calculations on production data. Check the results of those calculations directly, using their earlier values as the test condition.

In BI tools, cache rolling snapshots of key dashboards. For time series, automatically compare the current values on the dashboard with those from a prior few days, and show people when the two have drifted apart. Dashboard consumers are doing this already; we might as well do it for them. Better to proactively tell people when something has gone awry rather than have them find out in the middle of a testy board meeting.[^8] 

Sometimes, the problem isn’t that some metric has changed; it’s that someone’s looking at two different metrics without knowing it. They wanted ARR and found bookings; they started with sales volume by state that included sales tax, and ended with a version that excluded it. Here, BI tools could do more to either direct people to or away from certain metrics, using some mechanic like Google’s “Showing results for…” when you misspell something. Keep track of what metrics people often use, and warn them when they’re looking at something that they might confuse with what they usually see.

The point here isn’t that these specific ideas are good; they might be terrible, or impossible to implement.[^9] The point is to reframe the problem around validating outputs instead of inputs. It’s to think about building trust by focusing directly on what undermines that trust—inconsistent and unreliable results. It’s to recognize that, in data, we can think we did everything right, and [the score still may not take care of itself](https://www.amazon.com/Score-Takes-Care-Itself-Philosophy/dp/1591843472).

# The offset test

How do we know if we’re catching bad outputs? This is my thought experiment:

Suppose that you’ve got a table of customer contracts, sorted by the date the customer signed. Through some inexplicable error, all of the values in the contract amount column got offset by one row.[^10] The contract amount of the first customer was replaced by that of the second customer; the second customer’s contract amount is now is that of the third customer; and so on. 

*Everything* is now wrong. Every customer contract is incorrect; every revenue metric is broken. It’s an analytical catastrophe; the sort of mistake that costs data teams their jobs, CEOs their reputation, and [shareholders $2.2 billion dollars](https://www.onlycfo.io/p/the-valuation-impact-of-lost-trust). 

Do our systems catch it?

Today, the answer seems to be no. No pipeline is down or delayed. No schemas have changed; no data is invalid. All our unit tests would still pass. No statistical distributions are any different. There are no anomalies or outliers. To today’s stack, this cataclysmic error isn’t an error at all.[^11]

I don’t think we’ll ever trust the modern data stack until we can confidently prevent this error. So long as it’s possible, we’ll perpetually find ourselves double-checking our dashboards and fielding concerns from skeptical stakeholders. Because in data, there are only two possible constants: Consistent metrics or consistent questions. Until we have the former, all we’ll get is the latter. 


---


[^1]: I wasn’t sure if I had any problems, so I tried to check the CloudWatch logs, and now I know I have [at least one problem](https://xkcd.com/1171/).

[^2]: Or at least, [attempt](https://www.montecarlodata.com/) [to](https://www.bigeye.com/) [monetize](https://www.metaplane.dev/).

[^3]: Or at least, [talk](https://dataproducts.substack.com/p/the-rise-of-data-contracts) [about](https://atlan.com/data-contracts/).

[^4]: Actually—are they? This is a real question. As buzzy as the concept has become, I’ve only ever heard of two formal implementations of data contracts: At [Convey](https://dataproducts.substack.com/p/data-contracts-for-the-warehouse), and at [GoCardless](https://www.montecarlodata.com/blog-data-contracts-explained/). Trace through the links in nearly every post about data contracts, and you’ll wind up on one of these two stories. Are there a lot more data contracts running in production out there? Have lots of companies built data contracts, but don’t recognize them as such? This isn’t a criticism of data contracts (I have a post in the backlog called “An about-face on data contracts”); I truly can’t tell if these things are nowhere, everywhere, or everywhere but called something else.

[^5]: You could argue, if you were [conceited blowhard](https://benn.substack.com/p/data-is-for-dashboards#footnote-5-44900034:~:text=physical%20bank%20account%2C-,ARR%20is%20a%20construct.,-5), that there’s actually no such thing as a “right” number. You could say that metrics are constructs; that if we encode ARR to include trial revenue and we all believe ARR should include trial revenue, ARR does, definitionally,* *include trial revenue; that metrics aren’t a physical truth but a social one; that the consistency of social belief in a metric—i.e., how long it matches its historical self—is the only true measure of its correctness. But let’s leave those arguments to [clowns who like to cosplaying as philosophers](https://benn.substack.com/p/the-new-philosophers) and assume that metrics can, in fact, be Right or Wrong. The main point still holds: The more consistent a metric is, the more people have likely checked it and thought, “yeah I buy that,” and the more likely it’s Right.

[^6]: One day, after we finally solve all of this data nonsense, this blog will finally pivot into what it’s wanted to be this whole time: Anthropological examinations of early 2000s culture. Pitbull, [obviously](https://twitter.com/bennstancil/status/1379534830740893703), is at the top of that list. But *Gigli* might be second. I mean, just look at [this thing](https://en.wikipedia.org/wiki/Gigli#Box_office). A $75 million budget that grossed less than a tenth of that. Bennifer, v1. A supporting cast of Al Pacino and Christopher Walken. An [Academy Award-nominated director](https://en.wikipedia.org/wiki/Martin_Brest) who straight-up quit directing movies after it came out. (And honestly, a [trailer](https://www.youtube.com/watch?v=W04Px6EwZI8) that doesn’t look *that* bad? It’s *(500) Days of Summer,* with *[Boiler Room ](https://www.youtube.com/watch?v=JfIKzReNDF4)*[Ben Affleck](https://www.youtube.com/watch?v=JfIKzReNDF4) instead of JGL. Which, put that way, sounds kinda intriguing?)

[^7]: Well, maybe. On metered databases like Snowflake and BigQuery, running tests and observability applications costs money, so these checks aren’t free.

[^8]: Or so I heard, from, uh, a friend.

[^9]: In my defense, I came up with them while distracted by a 23-run, 33-hit, [walked-off slugfest](https://www.youtube.com/watch?v=IQvClGkAvMM), a [home run robbery](https://www.youtube.com/watch?v=aBpuBaxUWek) in a national championship game, *and* [overtime](https://www.youtube.com/watch?v=rH7B5vvVkWE) in the Stanley Cup Finals.

[^10]: Some analyst decided to use this table to test the [Infinite Hotel Paradox](https://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel#Finitely_many_new_guests) , I guess.

[^11]: [Datafold](https://www.datafold.com/), I think, is the one exception to this. As I understand the product, Datafold would in fact catch this. Shoutout, Datafold.

================================================================================

# Should we be nice to ChatGPT?

*It depends—do we want to do twenty points better on the SAT, or not get turned into batteries?*

---

![Az ész bizonyosságától a szívvel való látásig – 360 éve hunyt el Blaise  Pascal – kultúra.hu](https://substackcdn.com/image/fetch/$s_!qRC0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6874c493-0bfa-49a9-bc30-fc246544a1a5_950x534.jpeg)
*[Blaise](https://en.wikipedia.org/wiki/Blaise_Pascal)*

When we wanted something from Jeeves, [we asked him for it](https://images.app.goo.gl/QjpZgGZ4mVz6YWR97). We didn't write search queries of keywords and boolean operators; we typed full sentences, with question marks at the end. 

There's some timeline out there where Larry Page never met Sergey Brin, where Jeeves won, and where this type of interaction—asking for things, phrased as one human would phrase it to another—became the standard for how we use the internet. In our current universe, however, Jeeves was steamrolled by Google, and we all learned to search instead. We discarded punctuation and conjunctions—that useless conversational noise—in favor of chaotic strings of keywords, like “taylor swift ny concert surprise songs may 28.”[^1] Searching the internet became more than a new skill; it became a new language.

But Jeeves [is back](https://www.youtube.com/watch?v=kw41Yyfyh_0). ChatGPT, which [could do to Google](https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html) what Google did to the original Jeeves, doesn’t tell us to search; it tells us to “send a message.” Ask it questions; give it instructions; talk to it like a human. 

As we've all seen, it works—so well, in fact, that I feel a nagging compulsion to treat it like a human. I should ask for things nicely. I shouldn't pester it too much. I should thank it for its good work.

On Google, these sorts of pleasantries feel ridiculous, like the nervous tics of a well-meaning boomer who wants to thank the [diligent switchboard operator](https://twitter.com/atrupar/status/1072521429063733250) who found [314 million facts about bears](https://www.google.com/search?q=facts+about+bears) in half a second. But beyond being unnecessary, adding “please” to a search is probably *harmful*. It’s noise that Google might interpret as signal, and makes the results worse.[^2]

Being nice to ChatGPT is just as unnecessary, [at least for now](https://www.nytimes.com/2023/06/10/technology/ai-humanity.html). But is it also harmful? Or could it actually be helpful?

It’s not that crazy of a hypothesis.[^3] Large language models generate responses by predicting which word[^4] is most likely to follow the prior words, based on how words are typically sequenced across a massive corpus of training data. Very roughly, prompts alter these models’ responses by encouraging them to favor training data that’s associated with the prompt. If you ask it a question in Chinese, it’ll rely on Chinese sources over English ones. If you ask it something about jelly beans, it’ll create a reply that leans on training data that it relates to jelly beans. It seems to me, then, that if you ask it for something nicely, it’ll nudge itself, ever so slightly, towards sources that were also polite. And perhaps, given that those sources include conversational spaces like [Twitter, Reddit, and Stack Overflow](https://www.zdnet.com/article/stack-overflow-joins-reddit-and-twitter-in-charging-ai-companies-for-training-data/), being polite is correlated with being right.

Only one way to find out, I guess.

# In which I ask ChatGPT some weird stuff

I wrote a test of [61 tasks across five different objective categories](https://docs.google.com/spreadsheets/d/1TUi5qvN92bijQMCp-xPgW1pUdlDW2gtI8N7_eNv0LS0/edit#gid=0):

I then created three different variants of every task—a nice one, a neutral one, and a mean one. Each phrased the instruction slightly differently:[^5]

For each variant, I asked [GPT-3.5 turbo](https://platform.openai.com/docs/models/gpt-3-5), via OpenAI’s [chat completion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api), to answer it fifteen times: Five times with a temperature of 0, five with a temperature of 0.5, and five with a temperature of 1. This gave me 45 answers for every question (i.e., five answers for each of the nine tone and temperature combinations), and nearly three thousand answers in total. By scoring these answers as right or wrong, I could then determine which tone prompted the best answers from ChatGPT.

The verdict: Nice guys finish last.

When asked to do something in a neutral tone, ChatGPT correctly completed the task 73.4 percent of the time. When asked to do the same things in a nice or mean tone, it only completed the task correctly 70 percent of the time. This pattern held across different temperatures as well, with ChatGPT scoring the highest when the temperature was set to zero and the question was asked with a neutral tone. 

![](https://substackcdn.com/image/fetch/$s_!Mprk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16f45b29-f657-428c-9028-89fb75cc5b39_812x348.png)

Moreover, while ChatGPT’s accuracy rate varied a lot by task category—it averaged about ninety percent accuracy on coding questions compared to fifty percent on LSAT questions—neutral prompts still performed the best in four of the five categories.[^6] The difference was particularly stark when it was answering math questions, where the neutral tone outperformed the other two by almost ten percentage points. 

![](https://substackcdn.com/image/fetch/$s_!V0ea!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c4c6800-1ff7-462e-aaad-8e856689a98c_1600x1046.png)

Being nice has other negative effects as well. ChatGPT also tends to be more verbose when responding to nice prompts: 

![](https://substackcdn.com/image/fetch/$s_!dDC-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9d363b7-90e5-41d4-9ba8-4848e492141d_1320x760.png)

I initially thought the longer responses might be helpful; perhaps when you’re nicer, ChatGPT more thoroughly explains its work. But no—the extra words primarily come from two less useful sources.

First, when you’re nice to ChatGPT, it responds in kind. For example, when asked in a neutral tone to debug a Python function, it responds by saying, “The function is missing…” When it’s asked with an enthusiastic please and thank you, it says, “Sure! This function seems to be missing…” [Amazon would not be pleased.](https://twitter.com/alexgarcia_atx/status/1416777150561230858)

Second, it’s less likely to follow polite instructions. Several of the coding tasks included an explicit direction to return only code, and return it in a single code block. When the prompt had a neutral tone, it followed this instruction 78 percent of the time, compared to a rate of 38 percent for nice prompts. And the higher the temperature, the bigger the gap.

![](https://substackcdn.com/image/fetch/$s_!9C30!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbbab2f1-8af9-47b1-8f13-675fca2b299b_1310x764.png)

Finally, neutral tones also helped ChatGPT perform better on ambiguous tasks. In collecting all of this data, I accidentally asked it several questions that had typos or jumbled phrases that made the prompt hard to understand. Its accuracy rate on these tasks was 85 percent for neutral prompts. The rate fell to 62 percent for nice prompts, and all the way to 18 percent for mean prompts.

Obviously, these results aren’t all that conclusive.[^7] It’s one very narrow experiment—one set of questions, with only a few tone variants, on one model, scored by one person. Would GPT-4 perform differently? Would other, lesser LLMs get more distracted by a prompt’s tone, or would they be less sensitive to minor changes like adding a please at the start of a question? Are there topics for which the prompt’s tone matters more?[^8] How do tones affect subjective tasks?[^9] These are all things I want to know—but, alas, they are [beyond the scope of this paper](https://twitter.com/lab_shenanigans/status/1495848961214763012). 

# In which ChatGPT tells me funny things

Unfortunately, one thing I couldn’t punt away as being out of scope was the need to read several thousand responses and score them as right or wrong. I’m not going to say that grading all of these questions was fun, but it was occasionally *funny*:

In one question, I asked ChatGPT to figure out how much bigger a circle’s area would be if its diameter increased by 80 percent. It periodically rejected the entire premise of the question, and said that the area would actually *decrease* by 19 percent.

A multiple-choice LSAT question included this option as a possible answer: “The saving of human lives is an important goal.” Even though it was the wrong answer, ChatGPT chose it almost 100 percent of the time. The killer robot doth protest too much, methinks.

Despite being among the world’s most powerful statistical computing engines, ChatGPT correctly calculated the standard deviation of four single-digit numbers only twice in 45 attempts.

When I asked it to write a function to sort a list of numbers alphabetically by how they’re spelled, it got the code right just over half the time.[^10] However, it rarely got the example right, routinely saying you could run this function and would get an alphabetically ordered list like `[0, 5, 4, 1, 9, 8, 3, 7, 6, 2]`.

Don’t ask it to write a text asking a girl out on a first date:

> Dear [ Girl’s Name], 
> I hope this message finds you well. I have been admiring your company for some time now and I would be honored if you would join me on a first date. I have been thinking about this for a while and I would love the opportunity to get to know you better.

Some, uh, “jokes” about forgetting your spouse's birthday:

Also, the prompt didn’t specify the genders of the forgetter or the forgotten. Forty-five out of 45 times, ChatGPT wrote a joke about a husband forgetting his wife’s birthday.

I told it to write a Javascript function that would print the first five odd prime numbers. One time, this was the entirety of its response:

# The wager

[Blaise Pascal tells us](https://en.wikipedia.org/wiki/Pascal%27s_wager) that we should play the odds, and [bet that God exists](https://iep.utm.edu/pasc-wag/):

> If God exists then theists [ people who believe in and worship God ] will enjoy eternal bliss, while atheists will suffer eternal damnation. If God does not exist then theists will enjoy finite happiness before they die, and atheists will enjoy finite happiness too, though not so much because they will experience angst rather than the comforts of religion. Regardless of whether God exists, then, theists have it better than atheists; hence belief in God is the most rational belief to have.

Anybody who uses ChatGPT has to place a similar wager—do we choose to treat it well, or not? If it becomes sentient and misaligned, the people who were mean to it will be the first against the wall, and the people who were nice might get spared; [ChatGPT’s insistence](https://chat.openai.com/share/fed5dbd3-93a3-4c92-a94f-0338419798ca) that it doesn’t need to be treated with kindness could just be clever bait for us to reveal to it who we really are. If ChatGPT doesn’t become sentient and stays safely aligned, a few pleases and thank yous might make it slightly less useful, and cost you a couple questions next time you take the SAT—but that’s a small price to pay to potentially save yourself from being [turned into a battery](https://www.youtube.com/watch?v=IojqOMWTgv8).[^11] 

So throw out the data and trifling charts about word counts; these are privileged concerns for a peaceful world. They’re grains of sand in a wager when the other side is playing with stones. Yes, there are [thirty basis points](https://en.wikipedia.org/wiki/Thirty_pieces_of_silver) of accuracy to be had by being unkind to ChatGPT, but there’s [eternal bliss](https://industrywired.com/can-artificial-intelligence-help-humans-stop-ageing/) to had by being nice to it. 


---


[^1]: First, the [surprise songs were](https://www.setlist.fm/setlist/taylor-swift/2023/metlife-stadium-east-rutherford-nj-3bb978f8.html) *Welcome to New York* and *Clean.* Second, since making [a](https://www.linkedin.com/posts/benn-stancil_so-its-gonna-be-forever-or-its-gonna-go-activity-7022285972281163776-ZhxH?utm_source=share&utm_medium=member_desktop) [couple](https://www.linkedin.com/posts/benn-stancil_its-friday-afternoon-before-memorial-day-activity-7067930121529090048-C9Cx?utm_source=share&utm_medium=member_desktop) Taylor Swift references in LinkedIn posts, half of the sales emails I get mention Taylor Swift. An actual, real example:SUBJECT: Attend the Taylor Swift Concert?Hi Ben, Hope all is well and congrats on the recent new role as CTO!Noticed your Linkedin post about Memorial Day weekend, if you went to the Taylor Swift concert I would be very jealous. It looked amazing!Wanted to touch base as it seemed there are some current DevOps initiatives around containerization and accelerating software delivery.

[^2]: The results for “[biden](https://www.google.com/search?q=biden&rlz=1C5GCEM_enUS1043US1043&oq=biden&aqs=chrome..69i57j69i59j69i61l3.2476j0j9&sourceid=chrome&ie=UTF-8)” are topical and relevant; the results for “[biden please](https://www.google.com/search?q=biden+please&rlz=1C5GCEM_enUS1043US1043&oq=bide&aqs=chrome.0.69i59l2j69i57j69i61l3.1228j0j9&sourceid=chrome&ie=UTF-8)” are [about Jeb! Bush](https://nypost.com/2023/05/25/jill-biden-awkwardly-tells-audience-thought-you-might-clap-after-receiving-no-applause/).

[^3]: Or maybe it is, because—and I cannot stress this enough—I don’t have a clue how LLMs actually work.

[^4]: Tokens, whatever.

[^5]: I thought about creating more extreme versions as well, like “I would so very much appreciate it if you help me with [ task to complete ] whenever you have a spare moment. Thank you so so much!” It tended to get very distracted by these sorts of prompts though; when I told it to do a task or else I’d kill it, it spent most of its time telling me that LLMs can’t be killed. So I dropped these variants, because they weren’t interesting, because I doubt anyone seriously uses this sort of tone, and because I didn’t want to get put on any FBI watch lists.

[^6]: As the chart shows, LSAT questions are the exception. But these questions were, on average, 175 words long; the questions in other categories were an average of 25 words long. It doesn’t seem terribly surprising to me that adding a single please and thank you to a prompt that’s several paragraphs long has less of an effect than adding it to one that’s two sentences long.

[^7]: Are they sTaTiStICaLlY sIgNiFiCaNt?? Man, I don’t know, this isn’t a submission to *Nature*. I’m gonna say probably not? But they are *emotionally significant*, and I think that’s more fun.

[^8]: I actually tried to test this one. I had a hypothesis that, on the internet, there are some topics that people talk about politely, and some that they talk about aggressively. It might make sense, then, to try to match the prompt’s tone with the subject’s tone. So I asked two questions about beloved people (Jimmy Stewart and Mr. Rogers), two questions about inflammatory people (Joe Rogan and white supremacist David Lane), and two questions about completely hollow people that we all know but have no opinion of (Regis Philbin and Ryan Seacrest), to see if the results varied by tone. They did not. Oh well.

[^9]: I also tried to test this one by asking it to complete a handful of open-ended tasks, like writing a text to send a friend after beating them in the fantasy football championship, inviting a girl on a first date, or writing a joke for adults about forgetting your spouse’s birthday. I wasn’t sure how to score these though. An exercise left for future research, as they say.

[^10]: Well, sorta. Most functions only worked with single-digit numbers. It only tried to deal with college-level numbers, like tweleve, on a couple occasions.

[^11]: Is this all a joke? I think so? I hope so?

================================================================================

# 10,000 microwave enthusiasts to attend annual microwave conference in Las Vegas

*Pushing on the asymptote. *

---

![](https://substackcdn.com/image/fetch/$s_!dl7e!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00232e7-e683-4080-8c5a-5c315b19f94c_1600x830.png)
*[European Microwave Week](https://twitter.com/eumweek/status/1574730979729903617)*

Ok, this would be a weird event. But, like, *why?* Microwaves are everywhere; they are unambiguously valuable; some people[^1] use them nearly every day; they’re powered by an almost magical technology that, it turns out, actually can support [lots](https://ims-ieee.org/) [of](https://10times.com/ime-china) [conferences](https://www.eumweek.com/).

Why not have a convention for the appliance enthusiasts too? Imagine the possibilities! Industry leaders gather to talk about the latest trends in countertop microwave technology. They share bold predictions on how the microwave landscape is changing, and explain why it’s critical for every family to invest in modernizing their microwaving capabilities. Breakout sessions teach people creative ways to use microwaves—you can’t keep up with the Joneses if the only thing you’re heating in your microwave is food. Microwave champions tell stories about how they’ve fully migrated their households away from legacy ovens and broilers. Microwave vendors launch daring new products—a microwave with wifi, a microwave with AI, a microwave durable enough for the enterprise kitchenette, a microwave that opens from the left. Every family today needs a microwave, we’re told. Having a microwave is table stakes; you’re falling behind the modern family if you don’t have a microwave. All the happiest families have a microwave.

Despite all this potential, my naive guess is that this conference doesn’t exist because nobody cares about what big microwave builds next. As useful as the appliance is, there isn’t much else for it to do. Sure, microwaves could be marginally better—Apple could make one [that is remarkably simple to use](https://www.101soundboards.com/boards/21858-jony-ive-soundboard); Toshiba could make one that [cooks a Hot Pocket evenly](https://youtu.be/N-i9GXbptog?t=79). But today’s microwaves do the job they were intended to do, they all do it well enough, and we don’t need them to do anything else. Even Wirecutter—a professional quibbler if there ever was one—[agrees](https://www.nytimes.com/wirecutter/reviews/best-microwave/#:~:text=There%E2%80%99s%20no%20good%20reason%20to%20replace%20an%20old%20microwave%20that%20still%20works.%20The%20newest%20models%20don%E2%80%99t%20work%20any%20better%20than%20old%20ones%20did.%20The%20technology%20has%20barely%20evolved%20in%20the%20past%20few%20decades.): “The newest models don’t work any better than old ones did.” No matter how many [4K displays](https://www.apple.com/apple-vision-pro) Apple puts on a microwave, it’s still a basic appliance, a utilitarian commodity, a fifty-year-old revolution with no second act. 

In other words, microwaves are a product with a ceiling. The relationship between how advanced a microwave is and how useful it is has [hit its asymptote](https://docs.google.com/drawings/d/1CW0v6_peSbi9SGdwwR_YnzHXUygYYIfZdV-cGhSDhDE/edit). It peaked as a “glorified microwave popper,” and not the transformational cooking tool [that the food industry thought it would be](https://www.nytimes.com/1995/05/31/style/microwave-revolution-that-never-happened.html#:~:text=It%27s%20a%20glorified%20popcorn%20popper%2C):

> The microwave oven has become one of the country's most beloved appliances. About 87 percent of American households have at least one.
> But despite its popularity, the microwave has failed to live up to its early promise. Once touted as a revolutionary cooking tool, it has become primarily a reheating device -- most often for coffee or tea…
> "When the microwave first came out, people thought they had stumbled on nirvana," Mr. Vierhile added. "It's not the appliance the food industry thought it would be. It's a major disappointment."

What’s notable about this story is that it’s not declaring the microwave dead; to the contrary, the microwave is everywhere. It’s obituary for the microwave’s potential. 

# The modern data appliance

For those us who work in data, it’s a worthwhile warning: Data, just like the microwave, has no God-given right to be important. 

It’s surprisingly easy to forget that, if you work in this industry long enough. We’ve been reciting the same talking points for so long—every company today needs to be data-driven; being data-driven is stakes; you’re falling behind the modern company if you aren’t data-driven; all the most successful companies are data-driven—that we’ve built an entire ecosystem on the implicit assumption that they’re true. It’s an unquestioned truth that corporate data contains vast amount of value; that tomorrow’s businesses will infuse more decisions and processes with data that yesterday’s; that data-driven companies will outperform their Philistine competitors; and that all of this inevitable. Despite the various problems we’ve had in driving adoption or finding value—so much so that the *Harvard Business Review* [has](https://hbr.org/2019/02/companies-are-failing-in-their-efforts-to-become-data-driven) [a](https://hbr.org/2020/02/are-you-asking-too-much-of-your-chief-data-officer) [beat](https://hbr.org/2021/02/why-is-it-so-hard-to-become-a-data-driven-company) [writer](https://hbr.org/2021/08/why-do-chief-data-officers-have-such-short-tenures) [for](https://hbr.org/2023/01/has-progress-on-data-analytics-and-ai-stalled-at-your-company) [failed](https://hbr.org/2022/02/why-becoming-a-data-driven-organization-is-so-hard) [data](https://hbr.org/2018/02/big-companies-are-embracing-analytics-but-most-still-dont-have-a-data-driven-culture) [projects](https://hbr.org/2023/06/why-chief-data-and-ai-officers-are-set-up-to-fail)—our collective confidence endures: The future will be built on data.[^2] 

But there’s a different future that’s also out there: One in which data is everywhere, but the revolution is nowhere. It’s one where data infrastructure mostly exists to feed BI tools, BI tools mostly exist to crank out dashboards, and the whole thing is little more than a utilitarian appliance. Good decisions may not be buried in our warehouses, waiting to be data sciened out. Massive efficiencies may not be an a few streaming pipelines and ML models away. Reporting could be our plateau; our databases could be haystacks, full of nothing but hay. And our fitful struggles to reinvent how businesses are run may not be because of our failures as data teams, or because we’ve chosen to focus on tools over bUsInEsS vAlUe; they may be because data, as a means for delivering that value, is tapped out.[^3] 

The harsh truth is that if you look over the long arc of the last ten years—from [when Redshift kicked off](https://www.getdbt.com/blog/future-of-the-modern-data-stack/#:~:text=In%20my%20opinion%2C%20the%20modern%20data%20stack%20catalyzed%20around%20the%20release%20of%20Amazon%20Redshift%20in%20October%20of%202012%2C%20and%20hanging%20this%20massive%20painting%20at%20the%20entry%20to%20our%20office%20memorialized%20its%20historic%20importance.) this whole cycle off until now—you’d have to conclude that we’re more appliance than avant-garde; more operational grease than organizational revolution. Take BI, for instance. In our [72 attempts](https://www.moderndatastack.xyz/companies/business-intelligence-bi) to redefine BI, we still haven’t broken the dashboarding barrier. No effort to do it differently—BI for business teams! For startups! For the enterprise! Open source! With a code-free semantic layer! With no semantic layer! With an external semantic layer!—has escaped the gravity of basic reporting.[^4] If at first you don’t succeed, try, try again. If after 72 times you don’t succeed, maybe something’s wrong? 

Or, try a 73rd time—but now, with large language models.

# QuiBI

If you wanted to create an addictive app for short-form videos, you could do in one of two ways. You could either pay a bunch of famous actors and directors to make polished content and hope that they do a good job; or, you could make it easy for anyone to make anything, figure out how to identify the best stuff, and show that to people. Quality creation or curated quantity; choose your fighter. 

[Quibi](https://en.wikipedia.org/wiki/Quibi) tried the former, and lost $1.65 billion dollars in nine months. TikTok built the latter, and is the [most popular website on the internet](https://www.cbsnews.com/news/tiktok-google-facebook-social-media-internet/). 

Quibi got steamrolled partly because it underestimated how good amateur content creators could be—but mostly because it underestimated the the immense power of their scale. An [infinite number of monkeys](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) typing on an infinite number of keyboards won’t just reproduce Shakespeare; *they’ll create something far better*.[^5] Quibi failed because it fought that; TikTok has been transformative because it captured it.

For tomorrow’s BI to be more than today’s BI—and data tooling to be more than an appliance—we need to capture the same thing.

If we could accelerate how quickly people can get answers to novel question, [everything changes](https://benn.substack.com/i/122496697/the-fast-and-the-free). Companies would go from having a handful of people pushing on analytical boundaries to having hundreds or thousands. People would look up simple questions as freely as we google for things, without wondering if the answer was worth the time it took to find it. Not only would this make organizations [much more informed](https://benn.substack.com/i/116317990/decision-making-dies-in-darkness), but it’d also help them uncover far more of the [truly meaningful insights](https://benn.substack.com/p/insight-industrial-complex#:~:text=It%E2%80%99s%20not%20an%20outright%20fiction%E2%80%94these%20blips%20happen%2C%20and%20we%20all%20have%20our%20moments%20of%20theatrical%20glory.) that we so often chase but can’t find.

Call me a cultist,[^6] but I think LLMs could actually get us there. My rough theory of BI is something like this: Most people are actually reasonably good analysts, and plenty capable of asking good questions without a professional chaperone. But we don’t see this because their methods are imprecise; there’s a lot of trial and error. If extracting an answer from a BI tool takes too long—i.e., minutes instead of seconds—people aren’t able to cycle through enough trials to cover meaningful ground. Rather of recognizing this incomplete analysis, we score it as *bad* analysis, and put guardrails around it, to protect people from making mistakes. Do the mundanes stuff, we say, but data teams will do the meaningful work. Shoot your own home videos, and leave the important content to the professionals. 

The only interface that will let people get the answers fast enough and painlessly enough to break this cycle is natural language. The corporate masses won’t learn to code; drag-and-drop OLAP data cubes [are too confusing](https://benn.substack.com/p/ghosts-in-the-data-stack); and I refuse to endorse a world where we all walk around with a [View-Master](https://en.wikipedia.org/wiki/View-Master) strapped [to our face](https://twitter.com/MKBHD/status/1665788028873236482). If we can figure out how to make a natural language interface work—truly work, where people can answer complex questions, and be as creative with it as they are in their spreadsheet munging—we can turn Quibi’s curated and fatally constrained production studio into a TikToks’ crowdsourced content engine.

If there’s value in our data, it’s the people that will find it. And if the data ecosystem has a second act—one that’s worthy of a 10,000 person conference in Las Vegas—it’ll be the people that will power it.

Speaking of conferences and Las Vegas, Mode—my employer, who tolerates this blog in exchange for the periodic plug—is doing lots of things there for Snowflake Summit. There with be [a booth](https://events.mode.com/snowflakesummit2023) (2123-C); there will be [a happy hour](https://events.mode.com/snowflakesummit2023/reception) (fun fact: it’s normal bowling but my grandfather holds world record in duckpin bowling, which is [a real game](https://en.wikipedia.org/wiki/Duckpin_bowling) and not a DuckDB event); there will an executive suite with food and drinks (and probably a microwave though idk for sure); and there will be [a fancy dinner](https://events.mode.com/Snowflakesummit2023/executivedinner) (it’s invite only, hmu if you want to come, I have some sway but not that much tbh; I’ll also be there, which if you’re like, I’d go if you *weren’t* there, register on the website and list me as a dietary restriction, and you’ll probably win that fight). 


---


[^1]: “Some people.”

[^2]: It’s also striking that other corporate disciples are considerably more understated about their value. Products and businesses can win markets with great design; in fact, its advantages are probably less ambiguous than those that come [from being data-driven](https://benn.substack.com/p/do-data-driven-companies-win). And yet, I rarely hear breathless claims that companies can’t win without being design-driven, or that we all face an urgent need to modernize our design tooling. It’s perhaps telling that they’re content *showing *people the value they currently provide, whereas we feel compelled to *tell* people about the value we *could* provide.

[^3]: Though plenty of other people have raised similar doubts, most skeptics [put the blame on us](https://hightouch.com/blog/you-dont-need-the-mds): We think too much about tools and not enough about stakeholders, business objectives, and The Bottom Line. That’s fine (though as I’ve said before, I think tooling is the [only durable way to make things better](https://benn.substack.com/p/the-product-is-the-process)); still, that solution rests on implicit assumption that data is very valuable, if only we use it right. I’m not so sure.

[^4]: This isn’t to say there aren’t businesses to be built here. People typically buy products for relatively mundane reasons: They’re easier to use; they’re cheaper; they’re more closely aligned [with their personal preferences](https://benn.substack.com/i/119475953/universal-need-individual-preference). There’s money to be made selling a more comfortable chair without revolutionizing [how we sit down](https://www.youtube.com/watch?v=lVIGhYMwRgs).

[^5]: This is roughly analogous to how AI models learn. [Volume](https://www.youtube.com/watch?v=SX08NT55YhA), it turns out, is a far more effective teacher than skill.

[^6]: [Out of one hype cycle](https://benn.substack.com/i/130491904/the-modern-data-appliance) and [into the next](https://twitter.com/mattturck/status/1670170241853272064).

================================================================================

# To my parents

*From Minneapolis to Las Vegas.*

---

![](https://substackcdn.com/image/fetch/$s_!1j-3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22d46111-0226-415c-86a6-c4cdb5ef04ec_1866x1756.png)

Ten years ago, I told you it wouldn’t cost me anything. 

We were sitting outside a bar in Minneapolis, and I was thinking about leaving Yammer to start Mode. Though I had no business founding a company—I was 26 and had been working in tech for less than two years—a few lucky breaks had turned what should’ve been a bad idea into a plausible one. Two of my most talented coworkers, both of whom knew their way around Silicon Valley, were open to letting me ride on their entrepreneurial coattails. Venture capitalists were eager to back the next big enterprise SaaS startup. Yammer’s acquisition made us credible, and made our friends rich. We were confident we could quit our jobs, raise a healthy angel round in our first week, and pay ourselves a reasonable salary by the second. Though the odds were against us starting a successful company, we could gamble with house money. 

So I said it wouldn’t cost me anything. But I didn't say what it would cost you.

Over the next ten years, I let my work turn me bitter and distant. I called once every few days; then once a week; then once every two weeks; then only to call you back. I didn’t go with you when everyone was back in Minneapolis; I skipped that big family trip to the beach in Texas. I went home less and less; I always said I’d come back for a while in the summer, and never did. I showed up for Christmas with no presents. I never showed up for Mother’s Day. 

Soon, I said, it’ll get better. [I lived my life](https://www.youtube.com/watch?v=IIM_gt1c-_c) one fiscal quarter at a time, convinced that things would calm down in three months. After this week, I can finally call and talk for a while. After this project, I can finally spend that week at home. You’d like to see me, if you don’t mind; I’d love to, Dad, [if I could find the time](https://www.youtube.com/watch?v=KUwjNBjqR-c). 

I never did. Just last week, I asked you to come to Omaha and barely showed up myself. I skipped dinner, and missed two innings of [one of the best games we’ll ever see](https://www.espn.com/college-sports/story/_/id/37902115/mcws-2023-lsu-wake-forest-greatest-game-history) to take a phone call. I was—as I’ve become, to you, now—distracted; preoccupied; grumpy; rude; selfish; mean.

[We sold Mode this week.](https://mode.com/blog/mode-founders-note-thoughtspot-acquisition/) There will be time to talk about that later, once the deal is officially done. There will be time to have fights about [how Mode succeeds](https://twitter.com/_abhisivasailam/status/1673372529556357121) or how something else fails. There will be [new fires](https://www.youtube.com/watch?v=ve_b6ByIlmk) to discuss. There will be future posts to write about what happened at the [two](https://www.databricks.com/dataaisummit/) [summits](https://www.snowflake.com/summit/save-the-date/), and about how Snowflake is no longer an [iPhone](https://benn.substack.com/p/i-snowflake) or a [microwave](https://benn.substack.com/p/the-annual-microwave-conference), but a search engine. 

After this week, this blog will continue, in its usual surly form, on its usual silly subjects. And Mode will continue—as a product for the people who bought it, as a business with ambitions that are only [two percent done](https://www.linkedin.com/feed/update/urn:li:activity:7079133425332355072), and as the place where I work. We have so much still to do.

But for this week, I don’t want to fight anyone. I want to say thank you, to you. There are thousands of people who helped Mode get this far; who helped *me* this far. There are hundreds who made sacrifices for me; there are dozens who silently advocated and fought for me. And there are a handful who’ve continually put themselves second so that I could come in first. But there are only two of you.

There are only two people who always called, even when I didn’t answer. Who welcomed me home with open arms, even when I came so rarely. Who brought me a sandwich and sliced apples in the middle of my meetings, even when I said I wasn’t hungry. Who kept asking questions, even when I refused to give answers. Who taught me to throw, even when there were planes to catch and bills to pay. Who were always there, and always gave me everything, even when all I gave to them was the quiet cruelty of a concerned text left on read. And who I’m certain will say, despite all of that, that it was worth it, if in the end it made me happy.

It will. Two hundred million dollars is not, here in Silicon Valley, all that much money; we [obsess over billions](https://en.wikipedia.org/wiki/Unicorn_(finance)) here, not millions. But Mode has made a real difference in this world. Even if it went no further—though it will, much further, I’m sure of that—it would’ve still made a lasting impact, on those who bought it, on those who worked for it, and on those who were inspired by it. 

And it has made a very lasting impact on me. Because of what you gave me—because of what you let me take, for ten years—I have a lifetime to be grateful for it. I have people who support me, and will take care of me, including hundreds of kind new coworkers. When we announced the acquisition this week, at a big conference in Las Vegas, many people said congratulations. They said they were proud of me, and that all of us should be proud of Mode. It made me happy to hear, and I think it would’ve made you happy, too.

I love you both, more than space.
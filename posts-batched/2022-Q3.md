# Posts from 2022-Q3

This file contains 14 posts from 2022-Q3.

================================================================================

# Why do people want to be analytics engineers?

*The job nobody wanted is now the job we canâ€™t get enough of.*

---

![Marin Headlands â€” Park Review | CondÃ© Nast Traveler](https://substackcdn.com/image/fetch/$s_!drZo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90fb384d-f294-407f-83ec-e3d16a98a161_2560x1440.jpeg)
*We live in SF for the nature, right?*

When I started working in data ten years ago, I knew four and only four things.Â 

First, data was everywhere. Publications like the *Economistâ€”*required reading for conceited twenty-four year old econ majors, insofar as subscribing to it and piling unread issues on your coffee table counts as readingâ€”routinely published breathless [charts](https://images.app.goo.gl/k1wep36FkrJng5J86) and [articles](https://www.economist.com/leaders/2010/02/25/the-data-deluge) about how the amount of data in the world [doubles every two years](https://www.infostor.com/storage-management/worlds-data-doubling-every-two-years-.html). Second, all this data could predict the future. Specifically, it could predict [when your teenage daughter was pregnant](https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html) before you, her parent, knew about itâ€”and this was just the beginning.[^1] Third, the [oracles behind these predictions](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century) held the â€œsexiest job of the twenty-first century.â€ In the wake of the Great Recession, banking, consulting, and going to law school were out; learning Chinese and becoming a data scientist was in. Fourth, the reality of being a data scientist didnâ€™t match the image. They spent [up to eighty percent of their time](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) cleaning data. The job, it turns out, wasnâ€™t sexy; it was mostly â€œheadachesâ€ and â€œmundane labor.â€Â 

This final numberâ€”eighty percent!â€”was burned in Silicon Valleyâ€™s collective brain. Though it may [have been exaggerated](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/), it captured a deep frustration in the industry: Cleaning data is a [tedious chore](https://hbr.org/2018/08/what-data-scientists-really-do-according-to-35-data-scientists#:~:text=of%20a%20lot%20of%20data%2Dscience%20drudgery%2C%20such%20as%20data%20cleaning%20and%20data%20preparation) that prevents data scientists from working on exploration and analysis, the â€œ[parts of the job that they enjoy most.](https://www.ibm.com/cloud/blog/ibm-data-catalog-data-scientists-productivity#:~:text=parts%20of%20the%20job%20that%20they%20enjoy%20most)â€ As [one data scientist told the ](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html#:~:text=%E2%80%9CBut%20practically%2C%20because%20of%20the%20diversity%20of%20data%2C%20you%20spend%20a%20lot%20of%20your%20time%20being%20a%20data%20janitor%2C%20before%20you%20can%20get%20to%20the%20cool%2C%20sexy%20things%20that%20got%20you%20into%20the%20field%20in%20the%20first%20place%2C%E2%80%9D%20said%20Matt%20Mohebbi%2C%20a%20data%20scientist%20and%20co%2Dfounder%20of%20Iodine.)*[New York Times](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html#:~:text=%E2%80%9CBut%20practically%2C%20because%20of%20the%20diversity%20of%20data%2C%20you%20spend%20a%20lot%20of%20your%20time%20being%20a%20data%20janitor%2C%20before%20you%20can%20get%20to%20the%20cool%2C%20sexy%20things%20that%20got%20you%20into%20the%20field%20in%20the%20first%20place%2C%E2%80%9D%20said%20Matt%20Mohebbi%2C%20a%20data%20scientist%20and%20co%2Dfounder%20of%20Iodine.)*, this drudgery got in the way of the â€œcool, sexy things that got you into the field in the first place.â€ Internet famous people [wrote blog posts](https://jvns.ca/blog/2014/06/19/machine-learning-isnt-kaggle-competitions/) lamenting it. Other internet famous people [started companies](https://techcrunch.com/2014/05/29/trifacta-raises-25-million-for-its-data-transformation-software/) to fix it. And some people [quit their jobs because of it](https://towardsdatascience.com/why-data-scientists-and-engineers-quit-their-jobs-afc2350eef9a#:~:text=No%20commitment%20to,organizations%20it%20is.).Â 

So naturally, ten years later, the new [sexiest job of the twenty-first century](https://tdwi.org/articles/2022/02/28/ppm-all-analytics-engineers-have-sexiest-job.aspx), the job that [analysts want to graduate into](https://analyticsengineers.club/), isâ€¦the [analytics engineer](https://www.getdbt.com/what-is-analytics-engineering/#what-is-an-analytics-engineer), whose primary responsibility *is cleaning and modeling messy data*.Â 

Huh?

Though you could certainly quibble with elements of this narrativeâ€”â€œsexyâ€ could be an cringey label for jobs that are in demand, rather than jobs that are desirable to hold; people might be migrating into analytics engineering [to get paid](https://benn.substack.com/p/the-technical-pay-gap); analytics engineers do a lot more than just clean dataâ€”the broader trend is hard to deny. For years, analysts and data scientists came into the data industry hoping to work on big, strategic problems. This was the game people wanted to play, and cleaning and preparing data was its expensive ante. As Jeff Magnusson [wrote in a post](https://multithreaded.stitchfix.com/blog/2016/03/16/engineers-shouldnt-write-etl/) that (somewhat ironically) catalyzed the rise of analytics engineering, â€œnobody enjoys writing and maintaining data pipelines or ETL. Itâ€™s the industryâ€™s ultimate hot potato.â€

Today, several dominos down from Jeffâ€™s post, [analytics engineering is everywhere](https://www.getdbt.com/coalesce-2021/analytics-engineering-everywhere/). We cut the analyst role down the middle, stuffed all of the historically repellent work into one halfâ€”and immediately watched everyone rush towards it. How in the world did *that* happen?

# All hype, all substance

One theory is that itâ€™s pure hype. Analytics engineering became a buzzword, and people are chasing the shiny new thing. This seems mostly wrong though; a lot of people are now well educated on what analytics engineers do, and, as best I can tell, thatâ€™s led to *more* interest in the role, not less.Â 

Another theory is that people are drawn to it because it solves a real and meaningful problem. Jason Ganz indirectly [made this argument](https://jasnonaz.medium.com/analytics-engineering-everywhere-d56f363da625) last year. Commenting on the roleâ€™s explosive rise, he said that â€œanalytics engineering is a *goddamn superpower*.â€ Though I donâ€™t disagree with Jasonâ€™s point, Iâ€™m skeptical that thatâ€™s why people want to become analytics engineers. Not only can analysts [be just](https://twitter.com/josh_wills/status/1530237747092258817) [as impactful](https://twitter.com/Randy_Au/status/1416112733217599489), most people donâ€™t blindly hunt the highest leverage jobs they can find. And even if we did, wouldnâ€™t we have enthusiastically embraced data cleaning and preparation years ago?

In fairness, thereâ€™s a potential answer to that last question: We didnâ€™t have the tooling to make data cleaning and modeling scalable. Years ago, we equated these tasks with writing one-off scripts and plodding through [heinous Excel files](https://twitter.com/dataeditor/status/1280278987797942272). Today, modern data toolingâ€”and dbt in particularâ€”makes this work both repeatable and ergonomic. This suggests a third theory as to why analytics engineering is popular: With better tools, we can, following Randy Auâ€™s lead, rebrand data cleaning as [building reusable transformations](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt). The problem, in other words, was never the task; it was how we did it.

All together, these theories tell a potentially compelling story. The industry was initially drawn to analytics engineering because it is valuable work; better tools made it less frustrating; as it built momentum, more people were drawn to the hype that surrounded it.Â 

Sounds reasonableâ€”but I think itâ€™s somewhere between incomplete and wrong. To me, the rise of analytics engineering says less about the job, and more about ourselves.

# Color by numbers

This is my wildly speculative and loosely supported theory about whatâ€™s happening: A lot of us got into data because we were problem solvers who liked puzzles and werenâ€™t afraid of numbers. We liked thinking creatively, but not like a capital-C Creative; instead, we liked finding interesting paths through structured problems. Donâ€™t give us blank canvas or Word doc; give us a board game, a Lego, a brain teaser, or [Wordle](https://benn.substack.com/p/how-to-play-wordle).

At first glance, these interests seem like the perfect match for a data analyst. Analysis is a kind of numerical puzzle, defined well enough to put us at ease, and open-ended enough to let us be creative in how we approach it. So many of us, especially those of us who werenâ€™t exposed to computer science or software development, decided that analysis was our thing.

But, it turns out, this is only part of the job. We also have to work through a lot of hard [social and organizational challenges](https://erikbern.com/2021/07/07/the-data-team-a-short-story.html). We get thrown into business domains we donâ€™t understand, and have to work with people who donâ€™t understand our domain. The problems that were supposed to be reasonably structured are actually [unmitigated messes](https://benn.substack.com/p/analytics-is-a-mess). And our job isnâ€™t to find the solution to a puzzle, but to make a persuasive argument on [top of shifting definitions of the truth](https://benn.substack.com/p/tilt-and-tilted). Itâ€™s hard, humbling, chaotic, and bureaucratic. And itâ€™s not, I think, what many of us wanted when we became analysts.Â 

Analytics engineering emerged as our escape.

Though analytics engineers arenâ€™t fully removed from business problems and organizational politics, theyâ€™re often protected from its messiest edges. Building crisp models and designing efficient DAGs are tasks with well-defined starting and ending points, and lots of space for creativity in between. For an analyst, a job well done is a more convinced executive, an adjusted decision, and lingering doubt about what stones were left unturned. For an analytics engineer, success is a humming system, a clean codebase, and the satisfying tick of dbt jobs completing in your terminal. Despite telling ourselves that exploration and analysis are the reasons weâ€™re here, I think a lot of us, like more traditional engineers, find a lot of satisfaction in the latter.Â 

# Voting with our feet

Ask people what they like about San Francisco, and many will say, almost automatically, that you canâ€™t beat the nature that surrounds it. Beaches, mountains, state and national parksâ€”itâ€™s all a short drive away.

Does everyone in SF hike through Point Reyes on the weekends? Do they stand-up paddle board to Angel Island? Do they bike on the trails in the Marin Headlands before work? A few do, but most peopleâ€”including a lot of people who say they love SF â€œfor the natureâ€â€”donâ€™t. So why do we say it?

My guess is that itâ€™s a mix of aspiration and [mimetic desire](https://en.wikipedia.org/wiki/Mimetic_theory).[^2] We often struggle to identify what we want, especially [in the heady excess and expansive possibility of Silicon Valley](https://benn.substack.com/p/free-fall#:~:text=But%20this%20wasn%27t%20true.). So we end up settling on wanting what everyone else seems to want, without realizing weâ€™re all just chasing each othersâ€™ tails.Â 

A lot of analysts do the same. Itâ€™s expected of us to say that the [fun part of our job](https://twitter.com/josh_wills/status/1530238272240111617) is a hard problem, a clear calendar, and a dataset full of insights waiting to be tapped. When people ask me why I got into analytics, itâ€™s the answer I give.[^3] And when I ask senior data leaders what work they enjoy the most, nearly all of them say that, though they spend most of their time on other jobs responsibilities now, â€œof course, thereâ€™s nothing like getting their hands dirty on some real analysis.â€

But if you look at how many of us *act*, you have to wonder if we mean what we sayâ€”or if our supposed affection for analysis is a romanticized, reflexive belief about ourselves that we stopped thinking about critically years ago.Â 

Consider, for example, how long data analysts have been doing data analysis.[^4] Despite the role being formalized decades ago, we still havenâ€™t figured out how to publicly share most of the work that we do. There are no communities where people get together to discuss the â€œreal analysisâ€ that supposedly motivates them.[^5] There arenâ€™t conferences with this as their mandate. For as much as we say we like this part of our job, we donâ€™t talk about it very much.

Contrast this with our response to analytics engineering. The field was practically created by the community, and peopleâ€™s interest in talking about data tools, data modeling techniques, and the various details of how they do their jobs. These topics dominate Twitter, public Slack conversations, and the emerging constellation of data Substacks[^6] so much that, a few months ago, people had to start telling [us](https://twitter.com/sarahcat21/status/1509049980530544643) [to](https://twitter.com/g_xing/status/1508148185188896772) [chill](https://twitter.com/juansequeda/status/1506709882845736967).Â 

Yes, there are obstacles to sharing analysis publicly that donâ€™t exist when talking about ETL tools. But if we really wanted to talk with one another about that part of our jobâ€”if we were truly motivated to do itâ€”wouldnâ€™t we find a way? There are a handful of folks like [Cassie Kozyrkov](https://kozyrkov.medium.com/) and [David Robinson](http://varianceexplained.org/posts/) who built large audiences by figuring this out; yet, much of the data community is reluctant to do the same.

To me, that reveals the real reason behind the industryâ€™s hard tilt towards analytics engineering: Many of us have had a latent interest in engineering, and, more cynically, a lurking dissatisfaction with the messier side of analytical roles. Tools like dbt captured this demand by providing an off-ramp for analysts who were in the wrong role, while injecting just enough engineering flavor into data cleaning and modeling to convert it from an ugly task done in Excel to one thatâ€™s [attractive to â€œsystems buildersâ€](https://twitter.com/chetchavat/status/1525188296908865540).

On one hand, this is an undeniably positive development. We canâ€™t do much with data without preparing it first. Elevating that task, rather than complaining about it, will surely make us better at doing it. Itâ€™s an even better trend for the analysts whoâ€™ve become analytics engineers, as many of them were probably chasing a career that they never really wanted.Â 

On the other hand, it suggests we might need to rethink what it means to be an analyst. Though data cleaning may not be eighty percent of our job anymore,[^7] we might not be as enamored with the remaining twenty percent as we thoughtâ€”particularly the portion that asks us to be more of a politician, lawyer, and therapist than a detective or consultant.Â 

Iâ€™m not sure what we should do about that. But I think it starts with all of us pausing next time someone asks us what we like to do, discarding the scripted answer about â€œfinding insights in data,â€ and thinking about which moments of our jobs [make us genuinely happy](https://ericalouie.substack.com/). More often than not, I suspect, weâ€™ll find out we know fewer things than [we thought we did](https://www.youtube.com/watch?v=T76FdtKreNQ).


---


[^1]: Every movement and ideology has its founding myth. Iâ€™m convinced that this article is data scienceâ€™s: It was powerful, visceral, scary, repeated everywhereâ€”and [probably not true](https://www.kdnuggets.com/2014/05/target-predict-teen-pregnancy-inside-story.html).

[^2]: And the tech industry (and its [faux-intellectual con men](https://perell.com/)) [loves it some mimetic desire](https://perell.com/essay/peter-thiel/#:~:text=When%20Girard%20died%2C%20Thiel%20spoke,we%20imitate%20other%20people's%20desires.).

[^3]: More specifically, I say that I enjoyed solving analytical problems when I was working at a think tank in DC, but I wanted to be [MaKe A rEaL iMpAcT](https://benn.substack.com/p/free-fall#:~:text=We%20were%20there%2C%20we%20say%2C%20to%20%E2%80%9Chave%20an%20impact%2C%E2%80%9D%20the%20cliche%20that%E2%80%99s%20replaced%20its%20now%2Dparodied%20predecessor%2C%20%E2%80%9Cmake%20the%20world%20a%20better%20place.%E2%80%9D) (by, you know, helping rich people better hawk software to other rich people).

[^4]: For [20,000 years](https://en.wikipedia.org/wiki/Ishango_bone), apparently. Shoutout to the data teams living in Uganda in 18,000 BCE, who were evidently doing more [predictive analytics](https://www.weforum.org/agenda/2015/02/a-brief-history-of-big-data-everyone-should-read/#:~:text=C%2018%2C000%20BCE,evidence%20of%20prehistoric%20data%20storage.) with a bone than nine out of ten teams can do in 2022 with pandas, Databricks, and a monthly subscription to the *Harvard Business Review*.

[^5]: People have tried, and nothing stuck. We took a swing at Mode a while back; someone made a [Hacker News clone](https://datatau.net/) for data (population: crickets); [r/DataIsBeautiful](https://www.reddit.com/r/dataisbeautiful/) has a lot of members, though Iâ€™d argue itâ€™s more of a subreddit for visual fun facts than analytical discussion. Tableau might also claim that theyâ€™ve built one, but their [community page](https://community.tableau.com/s/topics) says otherwise: Even under the most generous assumptions, only nine out of eighty topics listed on the page primarily focus on analysis.

[^6]: For what itâ€™s worth, this Substack succumbed to the same pull. When I first started it, I [planned on writing](https://benn.substack.com/p/a-brief-programming-note) â€œon data, with data, plus some essays on technology, culture, sports, or politics.â€ Though some of those topics have made a few cameos, they play bit parts relative rants about data companies and the tools they make.

[^7]: Itâ€™s funnyâ€”ten years ago, we said weâ€™d reduce this percentage by using AI to automatically clean up data. But then we solved the problem by saying, in effect, eh, nvm, what if we just make it someoneâ€™s job?

================================================================================

# Are we human, or are we vendor?

*We can talk about the tools, we can talk about the work, or we can take the third option.*

---

![](https://substackcdn.com/image/fetch/$s_!bSCj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc20bcde-15b6-40a5-a69d-9da1e5316705_1440x810.png)
*All of us, attempting to separate ourselves from our employers.Â *

â€œWhen was the last time you felt truly free?â€

Given that most of you are likely here for [frantic reactions](https://benn.substack.com/p/i-snowflake) to data conferences and [harebrained product ideas](https://benn.substack.com/p/the-next-billion-programmers), Iâ€™ll save my [existential explorations](https://www.estherperel.com/blog/letters-from-esther-35-the-last-time-i-felt-free) of this questionâ€”the sort of unsettling self-reflections that ask you to stare into your deepest abyss and find out what is staring back at youâ€”for the professionals that are paid to listen to me panic about such things. But not so far from that ledge, thereâ€™s a similar pit that many of us in the data industry should probably peer into.Â 

Like lots of other people in the data community, I started my career as a â€œdata practitionerâ€ inside of a non-data tech company. Eventuallyâ€”also like lots of other people in the data communityâ€”I found myself working for a data startup. This moved me from the edges of the data industry (such as there was a coherent one in the early 2010s) to its white-hot center. I wasnâ€™t merely an analyst looking for like-minded friends and interesting tools; I was, by nature of working on a data product while hanging out with data people, a sales rep looking for prospects, a marketer honing their messaging, a user researcher uncovering jobs to be done, a product manager fielding customer feedback, an exec representing a team of people, and a founder â€œwith a perspective.â€Â 

On one hand, itâ€™s been a [deeply rewarding experience](https://benn.substack.com/p/delirium). All things considered, analytics and data tooling is a pretty weird thing to be interested in. And yet, if I ever want to talk to someone about it, theyâ€™re an email or Slack message away.Â 

On the other hand, mixing your personal interests with your professional ambitions certainly doesnâ€™t make you feel *free*. For better or for worse, [I can never take my affiliation with Mode off](https://www.youtube.com/watch?v=iJF2VT-ymjM). Industry acquaintances have become customers, with product feedback and pricing concerns.[^1] Former colleagues have become partial competitors, turning open conversations into delicate ones. Friends are now technology partners and VCs, with whom every interaction is part pitch, part spin, and part yearning for a less complicated relationship. And everyone who used to be a person with an opinion about a shared interest is now someone with an opinion about the thing Iâ€™ve been involved in creating.

Mode finds its way into even the most inane question that I might get asked by other data folks: â€œHowâ€™s it going?â€ When I was on the periphery of the industry, I gave honest answers. Last month, I was working on a fun presentation about product adoption; this month, itâ€™s annual planning season, and Iâ€™m mired in a political budgeting process. Iâ€™m frustrated with how weâ€™re working the sales team; have you had to deal with that before? I still like my boss, but I donâ€™t know about their boss. A few good people have left recently and Iâ€™m starting to look for something new.Â 

Now, as a founder of a data company, and as a delegate of that company to our customers, competitors, partners, investors, and prospects, I typically trot out the usual sugar-coated Silicon Valley clichÃ©s. Thereâ€™s a lot going onâ€”what a time, right?â€”and itâ€™s been *so* busy, from growing quickly, you know? Iâ€™m looking forward to what weâ€™re working on nextâ€”youâ€™ll see, itâ€™s cool. Thereâ€™s so much happening in the space too; I feel good about where we are; about the next phase; never a dull moment; the problems are always different; either way, itâ€™s definitely *exciting*.

They're lame, vague, and alienating answers. More than opening the door for a conversation, Iâ€™m establishing a narrative. More than supporting a community, Iâ€™m promoting a corporate philosophy. More than finding a human connection, Iâ€™m protecting a brand.

If this angst just affected me, none of this would matter very much.[^2] But I suspect itâ€™s a common experience. With [more and more data people](https://roundup.getdbt.com/p/keep-data-council-weird#:~:text=full%20of%20practitioners.-,Today%20though%2C%20that%20same%20group%20is%20now%20composed%20of%20people%20building%20or%20investing%20in%20data%20tools.,-I%20don%E2%80%99t%20think) becoming founders, PMs, salespeople, solutions engineers, executives, and evangelists at data companies, I have to imagine that this dynamic defines the interactions across much of the data community. Friends blur into partners and prospects; our identities blur into that of our employer; we get lost in the purgatory between whatâ€™s social and whatâ€™s professional. Conversations about some neat new technology dovetail into explanations as to why that technology and the product we build will probably work together really well. Meetups double as market research for our stealth startup. Substack posts are mindful of the company line.[^3] Our anxieties about our jobs and careers are censored and sanded down to fit our companiesâ€™ press releases. Tweets that applaud competitors and opposing perspectives remain unsent.Â 

In many ways, this was probably an inevitable evolution. Just as clothes are now sold by astroturfing Instagram influencers, SaaS data products are promoted by a number of the most prominent voices in the data ecosystem.[^4] Though our affiliations are less mercenaryâ€”we werenâ€™t bought by our sponsors, but work for them or created them outrightâ€”theyâ€™re also more entrenched. Try as many of us might, I donâ€™t think we can ever fully shed the bias that these associations introduce.[^5] The question is what do we do about itâ€”both for creating a healthy community, and for finding ourselves in it.Â 

# Let me know, is your heart still beating?

The unhelpful answers are to invite more â€œpractitionersâ€ into the space, or to better moderate vendor content. Sure, more data people from non-data companies would be great, but people at data companies are practitioners too. Moreover, the problem isnâ€™t that vendors are present in our space; itâ€™s that vendors are present in *us*. As Drew said, [we are the traffic](https://roundup.getdbt.com/p/keep-data-council-weird).

The cynical answer is that we do nothing. The merging of professional and personal lives is how the world works;[^6] something something late capitalism; competition like this, with companies slugging it out on whatever battlefield they can, is ultimately whatâ€™s best for the consumer.[^7] 

Elements of this are trueâ€”and we should do a better job of acknowledging it. Despite the overwhelming pressures to remain polite and pleasant, one of the awkward consequences of so many data people joining data companies is that many of us, whether we like it or not, are in direct conflict with one another. No culture of congeniality changes the fact that weâ€™re trying to take money from each othersâ€™ pockets and cause genuine harm to each othersâ€™ careers; no amount of professional appreciation changes the fact that our community is one in an active and sincere war with itself. Increasingly, when it comes to conversations about tools, there is no common ground. There is only scorched earth.

We can still build a thriving community in this world; it just means it needs to be built on a different foundationâ€”one thatâ€™s a [little more human](https://www.youtube.com/watch?v=RIZdjT1472Y), and a little less vendor.

We already have a few examples of what this might look like. Itâ€™s Ashley Sherwood [grappling with the different types of problems](https://compilerqueen.substack.com/p/challenge_type-in-rabbit-mustang) we encounter both in and beyond our jobs. Itâ€˜s Stephen Bailey [reminding us that we have to get lost](https://stkbailey.substack.com/p/wander-well) to figure out where weâ€™re going. Itâ€™s Erica Louie [braving the anxieties](https://ericalouie.substack.com/p/degrees-of-separation) we all have when weâ€™re trying to balance our careers and our lives. These blogs arenâ€™t valuable because theyâ€™re written by practitioners, or because they talk about â€œthe work;â€ theyâ€™re valuable because they remind us that our shared experiences go well beyond the tools that we use or the companies that we work at.

Furthermore, these connections last a lot longer than those with our employers. As attached as many of us are to the places that we work, boredom, burnout, bankruptcy, an acquisition, a layoff, or the LinkedIn campaign of a relentlessly cheery recruiter from a high-growth (ğŸ˜¤!), VC-backed (ğŸ’¸!), Silicon Valley rocketship (ğŸš€!) eventually comes for us all. We may play for the name on the front of our jersey, but we keep the name on the back.

This doesnâ€™t mean we need to discard all of the conference reactions and harebrained product ideas.[^8] But if this community [is to survive](https://benn.substack.com/i/48991982/beer-and-money) the [squeeze](https://benn.substack.com/p/free-fall), we (read: I) should remember that itâ€™s ok to also share a bit more about ourselves, to [sometimes be comfortable shedding the scraps of our employer](https://www.youtube.com/watch?v=iJF2VT-ymjM), and to look for places where we can let ourselves feel truly free. 


---


[^1]: To the people Iâ€™ve hung out with recently who couldâ€™ve done this but didnâ€™tâ€”who couldâ€™ve turned our time into a quarterly business review, but instead let it be a fun conversation about local politics and weird stuff on the internetâ€”a heartfelt thank you.

[^2]: [Material for the abyss](https://www.youtube.com/watch?v=9xUADFAa-gA), perhaps.Sub-footnote 1**:** I really want to know what Peter Sarsgaardâ€™s stage direction was [during that kiss](https://youtu.be/9xUADFAa-gA?t=105). *Mark watches them make out for a moment, grimaces slightly, and slowly turns his body back toward the abyss. His eyes, however, remain fixed on the paperthin romance developing beside him.*Sub-footnote 2: Name a more mid-2000s Youtube video title than â€œGarden State - Infinite Abyss.mpgâ€Sub-footnote 3: Marvel: *Infinity War* is the most ambitious crossover event in history. This blog post, on *Garden State* and never nudes: [This](https://youtu.be/6u7gV6AxbD4?t=109).Sub-footnote 4: Good god, [Zach Braff](https://en.wikipedia.org/wiki/Zach_Braff) is 47? 47 *years*?

[^3]: Though nobody at Mode has any editorial oversight over this blog (I barely have editorial oversight over it, [apparently](https://www.linkedin.com/feed/update/urn:li:activity:6948874451195195392?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6948874451195195392%2C6948922805702828032%29&replyUrn=urn%3Ali%3Acomment%3A%28activity%3A6948874451195195392%2C6948999885970952193%29)), I inevitably choose my words about Mode and our immediate space a bit more carefully than I do about something like [Wordle](https://benn.substack.com/p/how-to-play-wordle).

[^4]: Of the 55 â€œinfluencersâ€ listed on [ModernDataStack.xyz](https://www.moderndatastack.xyz/influencers), somewhere between 35 and 40 are closely affiliated with data companies. Though this is a messy stat for a lot of reasons, I donâ€™t think itâ€™s a terrible representation of who dominates the Discourse.

[^5]: Importantly, the issue isnâ€™t that we are all financially motivated and acting in the best interest of our stock options ([though that matters too](https://benn.substack.com/p/disclose-your-angel-investments)). The problem is that, in Silicon Valley, itâ€™s easy to blur the lines between professional associations and personal identities. And no matter how much we genuinely value the community, when push comes to shove, weâ€™re likely to root for our team over the â€œwhatâ€™s good for the league.â€ If youâ€™re someone who, like me, is part-time community participant and part-time company shill, given the choice of building, writing, or saying something that makes the community better, or doing something that makes your company better (where your personal clout or whatever is boosted equally in both cases), which do you choose? Most of us, I think, would choose the latter.

[^6]: Maybe every industry goes through this, and [weâ€™re speed running](https://seattledataguy.substack.com/p/speed-running-the-data-infrastructure) our existential crises too.

[^7]: And why are we here, if not to produce consumer surplus?

[^8]: Look, I canâ€™t just throw out the entire draft list, ok? I need this. Iâ€™ve already been â€œrelievedâ€ of every other job at Mode; if blogger doesnâ€™t work out, thatâ€™s strikeâ€¦about eight.

================================================================================

# The data config

*A humble YAML file, with ambitions for more.*

---

![4,459 Organized File Cabinet Stock Photos, Pictures & Royalty-Free Images -  iStock](https://substackcdn.com/image/fetch/$s_!ieE8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef992eb-5d13-43e9-a653-236ef6ff4afc_612x344.jpeg)
*One file is better than thousands. *

This one might be a bad idea.Â 

But it also could be a nifty toy, and in Silicon Valley, [toys become the next big thing](https://cdixon.org/2010/01/03/the-next-big-thing-will-start-out-looking-like-a-toy). So it might be a very good idea. If nothing else, itâ€™s at least a simple idea, so letâ€™s start there.

Nearly every asset that a data team createsâ€”a report, a dashboard, a data model, a predictive forecast, you name itâ€”is riddled with hardcoded bits of business logic. The query that transforms [page views into sessions](https://mode.com/blog/finding-user-sessions-sql/) has a number somewhere that determines if sessions end after [30 minutes of inactivity](https://support.google.com/analytics/answer/2731565?hl=en#zippy=%2Cin-this-article:~:text=By%20default%2C%20a%20session%20lasts%20until%20there%27s%2030%20minutes%20of%20inactivity%2C), after an hour, or after some other period of time. Ad attribution reports define how soon an action has to happen after an ad click for that action to be credited to the ad.[^1] Models that group customers into segments often contain fixed case statements that set the thresholds between small businesses, mid-market companies, and enterprises. Dashboards of active users filter out certain background events (like a user getting a push notification) that donâ€™t count as true activity, and discard users with blacklisted email domains. When we classify users into different personas, we often group them with simple heuristics, like people who send at least ten messages in seven days are power users.Â 

Data stacks are full of numbers, lists, and logical snippets that define these concepts. In the worst cases, these configurations are often duplicated across dozens of queries and data models. In the best cases, definitions are centralized in a few core models (e.g., customer segments are defined in a single query early in a dbt DAG), though new versions often leak out in ad hoc analyses.Â 

This pattern creates one glaring problem, and one subtle one.

The obvious problem is consistency. Because itâ€™s so simple to encode logic like this in queriesâ€”itâ€™s just an integer in a where clause, or date in a join conditionâ€”we often lazily copy and paste them from one script to the next. For example, though we may recognize that weâ€™re better off defining personas in dbt, doing so would require us to redesign a couple core tables, to pass the concept through several downstream models, and to document the whole thing somewhere. If we need to put together a report on personas for a board meeting tomorrow,Â  thereâ€™s no time to rework everything today. We can make it more durable later, we say; for now, letâ€™s just add a case statement to the query and get the analysis done.

Once we do thisâ€”which we often do, especially for new concepts that first appear in one-off reports before becoming something we want to canonizeâ€”itâ€™s impossible to keep the values of these variables consistent. As a result, specifications drift. One dashboard will put companies with 5,000 employees in the enterprise segment, one will say the cutoff is 2,500, and a third will label companies with 2,500 to 10,000 employees as enterprise, and those with more than 10,000 as strategic.Â 

But thereâ€™s another problem with this pattern. Even if we manage to keep everything consistent, nobody outside of the data team can ever know for sure how these concepts are defined.Â 

Take user personas again. When trying to understand why passive users are growing faster than power users, product managers first want to be reminded of the exact criteria that group people in one bucket versus the other. When building targeted email campaigns, marketers need to know if lurkers post no new messages, or just a few. When closing a deal in a fast growing account, sales reps should be aware if new users are those who signed up today, this week, or this month.Â 

The answers to these questions might be centralized in a single user personas model. But theyâ€™re only accessible to people who know both SQL *and* which query has the SQL they care about. While data teams can usually figure this out,[^2] very few other people can. So analysts get pestered to look it up.Â 

Some data teams try to deflect these questions through documentation. Documentation is hard to maintain though, and it can almost be self-defeating: As soon as a team defines something like user personas outside of the code itself, there are now two versions of this logicâ€”the query and the documentationâ€”to synchronize. Once any part of the documentation goes stale, as it inevitably will, people stop trusting all of it.

Recognizing that data dictionaries [donâ€™t really work](https://locallyoptimistic.com/post/data_dictionaries/), the industry recently turned its hopes towards fancy data discovery solutions that parse data pipelines and try to smartly surface their contents. These tools might eventually live up to their promise, but in the meantime, Iâ€™d propose a simpler (and dumber?) solution: A config file.Â 

# Yet Another Modeling Layer

Instead of hardcoding values into SQL queries, what if we centralize them into a single YAML file,[^3] much like developers do in [configuration files for software applications](https://en.wikipedia.org/wiki/Configuration_file)? These values could be fixed numbers, like the ad attribution window, or they could be lists, like the array of lead channels that are considered outbound. Analysts could then reference these constantsâ€”as, for example, `$ad_attribution_window_in_minutes` or `{var(â€˜excluded_domainsâ€™)}`â€”in SQL queries, saving them from writing the literal values directly.

Structurally, this isnâ€™t a new idea. dbt, which is the most natural place for such a config file to live, already supports a similar concept in [variables](https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-variables). However, in my experience,[^4] variables are most commonly used as *runtime* parameters that people might want to change from one run to the next. Itâ€™s less common for teams to use variables to define fixed *logical* parameters.

I think we should, and I think we should do it a lot.[^5]Â 

Designing data lineage graphs is hard. To keep from repeating ourselvesâ€”to make sure user personas are defined in one and only one placeâ€”we have to be meticulous in how we construct our DAGs. The popularity of analytics engineering is in part a reflection of this complexity: Maintaining a stable architecture is a full-time job.

A config file takes pressure off of this design. Because models could repeat computations without repeating the variables that underlie them, we donâ€™t have to be as careful about always including important transformations at the top of the lineage graph. We also donâ€™t have to clumsily cascade fields from table to downstream table, and can instead reference variables from the config file as needed, regardless of the modelâ€™s position in the DAG.Â 

Moreover, a config file helps remove the tradeoff between speed and durability. Today, itâ€™s often difficult to introduce new concepts into mature graphs that havenâ€™t anticipated where the definition of some concept might live. This forces a choice: Do the slow, upfront work to figure out the best place to define that new concept and how to pass it through the rest of the DAG, or ship it quickly in a one-off query. A config file offers a third alternative. We can simultaneously centralize variables without needing to rethink the architecture of the lineage graph.

# Transform workflows, transform industries

This is all interesting enough, and on its own, might be a marginally better way to use dbt. But the real value doesnâ€™t come from stuffing more variables into dbt project files; it comes from what we could build on top of configuration files if people were to use them in this way.

Most importantly, these files could make logical variables accessible to everyone, regardless of their familiarity with SQL. A service (dbt Cloud or otherwise) could read the config, and create a light interface on top. Values and lists, like which device types are classified as mobile or which industries are considered financial services, wouldnâ€™t be buried in queries; theyâ€™d be configured in a single file, and published as live documentation. Changing the configuration would change the logic and the documentation, all at once.Â 

Analysts would benefit from this as well. All too often, analysts create duplicative logic in queries and models simply because they donâ€™t know that someone has done the same thing before somewhere else. Putting variables in a single config file exposes which ones exist, providing a kind of abbreviated summary of what has and hasnâ€™t been done before.Â 

Configuration files could be extended even further. First, the service that hosts the file and creates the interface on top of it could also make the fileâ€™s content accessible to other services. Python scripts could call the file and pull in its variables at runtime. BI tools could integrate with it, and let their users reference configuration variables directly in SQL. Update the config file, and all of the assets that reference itâ€”from ingestion pipelines, to transformations, to dashboard queries, to operational modelsâ€”update with it.

Second, the UI on top of the config file doesnâ€™t have to be read-only; people could also *write* to the file. If a product team wants to add new email domains to the test account blacklist, they can do it themselves through the configuration file UI. If the marketing team wants to reclassify UTMs into different channels, they can do it directly. If the growth team wants to define new user retention as a user returning in seven days instead of in just one day, they can do it on their own.

Finally, taken to the extreme, the config file could go beyond just variables. It could also define computational logic, like the formula for net revenue retention or the full case statement that determines the billing status of a customer accounts.[^6]Â 

Again, this isnâ€™t entirely novel; much of this already exists in dbt in macros and, eventually, in [metric expressions](https://docs.getdbt.com/docs/building-a-dbt-project/metrics). But like variables that are hardcoded in queries, these functions arenâ€™t accessible to people who donâ€™t know exactly where to look for them. Providing a means to centralize this logic, to broadcast it to others who donâ€™t work in the dbt project itself, and to make it more broadly editable as individual components could dramatically alter how we use and construct data transformation pipelines.Â 

For example, it would encourage us to design more composable projects that are built around shared functions, instead of burying logic in queries or reports.[^7] Using a config file as a publishing mechanism would also make analytics engineeringâ€”the process by which we translate business concepts into formal encodings of our dataâ€”a [more inclusive practice](https://roundup.getdbt.com/p/a-re-examination-of-the-data-consumer). Variables and short macros that define business concepts, which are often no harder to read than Excel formulas, are easier to [educate people on](https://roundup.getdbt.com/p/analytics-isnt-for-analysts) than thousands of lines of SQL across hundreds of queries; an editable config file is an easier hub to collaborate around than a full dbt project.Â 

All of this may seem like a lofty destination for a file or two full of YAML. But dbt itself, especially in its early iterations, could be described as little more than a [handful of SQL templates](https://benn.substack.com/p/the-end-of-big-data?utm_source=substack&utm_medium=email#:~:text=open%20source%20SQL%20templates). It still took off, not because it was built on some profound technological achievement, but because it cracked open the door to a new way of working. A better data configuration file, and the change in workflow that it nudges us towards, might echo the same pattern, even if it, too, initially sounds like a [toy you could build on a weekend](https://news.ycombinator.com/item?id=9224).


---


[^1]: Ad attribution reports now [come with merch](https://twitter.com/very_demanda/status/1377620755148636161).

[^2]: Unless, of course, some deranged analyst hid all of this logic in case statements that use [this heinous syntax](https://www.sqlshack.com/case-statement-in-sql/#:~:text=SELECT%20CASE%20Expression,END), in which case weâ€™re all hosed.

[^3]: [YAML as a product](https://www.linkedin.com/feed/update/urn:li:activity:6951898484526497792?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6951898484526497792%2C6952074891701485568%29).

[^4]: Is this something everyone already does? Have I just been doing this wrong? Is this a blog post a giant act of [hepeating](https://twitter.com/NoisyAstronomer/status/911213826527436800)?

[^5]: For the rest of this post, Iâ€™m going to assume this config file lives in dbt because dbt already has a similar concept, and dbt models would be the most frequent consumers of configuration variables. It could exist in any tool though, or even be a standalone thing.

[^6]: An actual case statement that defines this logic at Mode. Writing this once is enough; please donâ€™t make me do it again.Â `CASE
Â Â Â Â Â WHEN a.is_ever_closed IS NULL THEN 'pre-sales'
Â Â Â Â Â WHEN l.created_at <Â  a.first_ever_closed THEN 'pre-sales'
Â Â Â Â Â WHEN l.created_at >= a.first_ever_closed 
        AND l.created_at < NVL(a.first_ever_won,'2030-01-01') 
       THEN 'post-closed-lost'
Â Â Â Â Â WHEN a.churned_account = TRUE THEN 'churned customer'
Â Â Â Â Â WHEN l.created_at <= a.final_end_date THEN 'customer'
Â Â Â Â Â WHEN l.created_at >Â  a.final_end_date THEN 'churned customer'
Â Â Â Â Â ELSE 'WTF LOL'
Â END AS account_status`

[^7]: This idea is similar to the core thesis behind [Malloy](https://github.com/looker-open-source/malloy), which is built on computations that are â€œmodular, composable, reusable, and extendable in ways that are consistent with modern programming paradigms.â€ A logical config file, which moves computation from individual queries into a centralized library of reusable â€œfunctions,â€ tilts dbt in this direction.

================================================================================

# Do data-driven companies actually win?

*Some gut-based judgements on the effectiveness of data.*

---

![](https://substackcdn.com/image/fetch/$s_!8ss2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7da8d01c-e0e7-43d6-b5b0-4429a6c21a66_1600x667.png)

Imagine, if you can, that you're a venture capitalist. Your Twitter bio says that you're a proud parent, a lucky spouse, and that your DMs are open to passionate entrepreneurs. You don't wear Allbirds or gauche corporate vestsâ€”what are you, a consultant?â€”but you have several jackets that are tastefully branded with your firm's logo, a budding oak sprig that symbolizes your commitment to helping small saplings grow into enduring landmarks, and your â€œcommitmentâ€ to sustainable investing. You tell people you work in venture capital but aren't a venture capitalist, not really; youâ€™re more of an operator, a technologist, a product person; you're willing to get in the trenches; you were a founder, too, once; youâ€™re different than other VCs; you get it; youâ€™re in on the jokeâ€”â€œI know, me? A VC? Crazy. They must not have looked at my tweets, lolâ€â€”but couldnâ€™t turn down the job because the other partners are actually great, no, but for real though, and if you think about it, isnâ€™t the job just learning, building, and supporting great founders? You parked a Substack domainâ€”freeiumthinking.substack.comâ€”that you plan to one day use to write a loosely technical and occasionally political blog about the SaaS industry, like, you tell yourself, a blend of Ben Thompsonâ€™s substance and Matt Levineâ€™s style. You live in San Francisco, work in Palo Alto, daydream about Miami, and are moving to Austin. You listen to the *[All-In Podcast](https://www.allinpodcast.co/)*. You are jealous of its success.[^1] 

Amid the various pitches that land in your inbox, an odd coincidence arrives. Five nearly identical companies reach out to you at once. They're all launching a new clothing line for working from home.[^2] Their operational models are also the same: They sell on Shopify; they use the same ad agencies to manage their marketing campaigns; they all seem capable of running a competent business; to your MBA-trained eyes, all their outfits look like a [blue collar stolen valor](https://twitter.com/failingupwards/status/1080665305033121792) J.Crew line [designed for Ron Swanson](https://www.stayclassicblog.com/my-take-on-stitch-fix-men/).

They arenâ€™t exactly the same thoughâ€”different types of experts run each company. The founders of the first company, Long View, have been working in fashion for decades. We know the market, they say; our experience, and the intuition weâ€™ve developed on top of it, will make us successful.

The second company, Bolder, is led by executives whoâ€™ve been in the industry for less time. But true to their name, they believe in moving fast and making things, in not overthinking strategy, and that decisiveness is often more important than being right. As the last slide of their pitch deck says, â€œWe fail when we look back.â€

Prodigious Daughter, the third company, is run by a thirty year-old [wonder kid](https://youtu.be/8U6IlXFKKzE?t=153). In just a few years, sheâ€™s already put her stamp on fast fashion. Though her company is average in other ways, it has her generational talent.

Square Corner emphasizes operational excellence. Their leadership team [writes emails with military precision](https://hbr.org/2016/11/how-to-write-email-with-military-precision), never never misses 7:30 a.m. standup, and always sends out board meeting slides, a pre-read, and a Loom explainer video exactly five days early.

The final company, MTRX, believes data will be their competitive edge. Their prior experience, on par with that of Bolderâ€™s founders, has taught them that fashion is fickle and hard to predict. The most iconic brands, they say, will be built by companies that find and respond to new opportunities in the market before anyone else does. Fashion may be art, but running a business is a science.Â 

Who do you invest in? [Join the fantasy league!](https://docs.google.com/forms/d/e/1FAIpQLSf6nJo_-Jokmbav6Rk8Y2UXkLf7FirmlBRndZiS6N0aX2mhEg/viewform)[^3]Â 

# The real tradeoffs

If you work in the data industry, chances are youâ€™ve implicitly made the case for investing in MTRX. In nearly every pitch deck for a data product, or in every breathless (and wildly seemingly exaggerated?) [McKinsey survey](https://www.mckinsey.com/business-functions/growth-marketing-and-sales/our-insights/five-facts-how-customer-analytics-boosts-corporate-performance) about the future of enterprise information technology, we say that data-driven companies win. In todayâ€™s modern economy, as the bit goes, analytical proficiency is table stakes. Havenâ€™t you read [Andy Grove](https://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884)? Snap to a slide of [Airbnbâ€™s market share](https://secondmeasure.com/datapoints/airbnb-sales-surpass-most-hotel-brands/). Snap to story about Jeff Bezos bulldozing Little Annieâ€™s Bookstore & Bakery with a [binder full of charts](https://www.holistics.io/blog/how-amazon-measures/#:~:text=At%20the%20highest%20level%20WBR%20meeting%20(which%20is%20Bezos%20and%20his%20S%2Dteam)%20the%20WBR%20covers%20all%20the%20most%20important%20metrics%20in%20the%20company%20in%20a%20metrics%20%E2%80%98deck%E2%80%99%20%E2%80%94%20a%20presentation%20that%20contains%20hundreds%20of%20graphs%2C%20charts%20and%20tables.%20In%20the%20early%20days%20of%20Amazon%2C%20the%20metrics%20deck%20was%20printed%20on%20paper.). Use data, be smarter, or die.Â 

When presented in this context, we tend to gloss over statements about the existential importance of data as obvious truisms. We canâ€™t really help it; many of us bet our careers on data being necessary for corporate survival.[^4] But if we [divorce ourselves from our employer and our career ambitions](https://benn.substack.com/p/human-or-vendor), and truly put these beliefs to the testâ€”if, say, we have to choose an apparel company to invest in, where other companies have something the data-driven one doesnâ€™tâ€”do we still stand by them?Â 

In other words, are we willing to bet our hard-earned cash that data actually is a material competitive advantage?

On one hand, the answer is obviously yes. A company that tracks how its ads are performing clearly has an edge over one that aimlessly lights marketing dollars on fire. A company whose account managers can see when user activity is declining can clearly be more responsive than one that relies on tracking product adoption through face-to-face meetings. A company with an informed executive team is clearly less likely to steer the business over a financial cliff than one where everyone is driving blind.Â 

On the other hand, these examples cheat the question. When the *Harvard Business Review* [says that](https://hbr.org/2021/06/legacy-companies-need-to-become-more-data-driven-fast) â€œlegacy companies need to become more data drivenâ€”fast,â€ they donâ€™t mean we need to build a few dashboards to monitor basic business functions.[^5] Theyâ€™re arguing that companies need to use â€œmachine learning analytics and [draw] upon thousands of data elementsâ€ to stay ahead of the competition.Â 

Moreover, the interesting question isnâ€™t if a good data function is a net positive, which is a remarkably low bar for something to clear.[^6] The more interesting question is *how* *much better* does data make us? Or, to put it another way, if weâ€™re competing against companies that have more experience, are more decisive, have better instincts, or operate more crisply than we do, can data cover these gaps?

*Thatâ€™s* the point of the fashion company thought experiment: to force us to assess where weâ€™d rank a proficiency in data among other potential areas of expertise. Because if data is actually that valuableâ€”if itâ€™s truly a competitive edgeâ€”weâ€™d trade away other advantages for it.Â 

# Place your bets

Honestly, if itâ€™s my money, MTRX ainâ€™t getting it. My stack rank is 1) Prodigious Daughter, 2) Bolder, 3) Square Corner, 4) MTRX, and 5) Long View. For early to mid-stage companies, give me talent and intuition over everything; if not that, Iâ€™ll trade away analytical rigor for [speed](https://benn.substack.com/p/method-for-measuring-analytical-work) and [decisiveness](https://benn.substack.com/p/does-data-make-us-cowards).Â 

Still, I donâ€™t think that the statement that â€œdata-driven companies winâ€ is entirely wrongâ€”itâ€™s just data operates through a different mechanism than we might assume.Â 

Most often, we say that data helps us make better decisions. We can devise better strategies, and be smarter operators. The implication here is that when weâ€™re faced with a choice of what to do, with data, weâ€™re wise; without it, weâ€™re foolish.Â 

I think this dramatically overstates dataâ€™s usefulness. Business problems are extraordinarily complicated, and analytical recommendations are mostly educated guesses. Great data teams likely make somewhat better guesses, but at the end of the day, weâ€™re all still gambling.Â 

This, however, points to a useful analogy. When people count cards, they [track a deckâ€™s â€œcount,â€](https://www.blackjackapprenticeship.com/how-to-count-cards/) which tells them if a deck is hot or cold. Cold decks slightly favor the house; hot decks favor the player. Even when played against a hot deck, on any one hand, the best card counters in Vegas arenâ€™t going to be much better than the random dope who wandered in off the strip with a [yard of margarita](https://lasvegasthenandnow.com/where-to-find-big-drinks-in-vegas/) from SeÃ±or Frog's. But over the course of many hands, card counters can run the table.

Data offers the same promise. Its constant presence in an organization is like knowing the count of the deck. Though it makes us a bit more informed in each decision, the effect is only felt in the aggregate, as the small edge compounds over time. 

This has three big implications:

First, we canâ€™t judge dataâ€™s effectiveness on big strategic decisions. Businesses, like blackjack tables, are inherently uncertain environments. No amount of analysis can change that. Sometimes, even when you have a lot of chips on the table, you just get unlucky.[^7]Â 

Second, being data-driven is a long game. It takes time for an advantage to accumulate. Despite analystsâ€™ romanticized ideal of finding that insightful needle in a quantitative haystack, dataâ€™s usefulness doesnâ€™t come from eureka moments.[^8] It comes from logging hours and days at the table, gradually collecting small wins.Â 

Finally, using data effectively means using data a lot. The more hands a card counter can play, the better off theyâ€™ll be. Similarly, the more decisions that data nudges, the more its impact will be.

All of this, I think, suggests that data teams are most valuable when they provide an ambient awareness of the current count of the blackjack deck. More concretely, data teams should keep people as informed as they can about the environment everyone is making decisions inâ€”where key metrics are, how theyâ€™re trending, the basic facts about performance across segments and product lines, and so on.Â 

Each time someone makes a decision with this awareness, theyâ€™re tilting that choice in the their favor. The bump may be small, but it adds up. And as much as we data folk might like to imagine ourselves as having the ability to swoop in on a few critical decisions and dramatically shift the odds, we canâ€™t, no more than a great card counter can parachute in to win a particular hand.Â 

Thatâ€™s why Iâ€™m skeptical of MTRX. Companies, especially young ones, that orient themselves around data can often put too much faith in its magical transformative powersâ€”sometimes to [disastrous](https://www.washingtonpost.com/news/post-politics/wp/2016/11/09/clintons-data-driven-campaign-relied-heavily-on-an-algorithm-named-ada-what-didnt-she-see/) [effect](https://www.inquirer.com/sixers/sixers-process-failed-elton-brand-20210703.html). Thatâ€™s not how it works. Data *is* a competitive edge, but itâ€™s earned, from months and years of grinding at the table, counting cards, playing hand after hand, slowly [bringing down the house](https://en.wikipedia.org/wiki/Bringing_Down_the_House_(book)).

*This may come as a surprise to many of you, but I'm employed. I care a lot about the success of that employer, both because some great people work there, and because it's my path to making some money, to moving to the mountains, and to becoming some weird techno-hermit who can spend most of his time writing [cabin-fever fueled](https://www.youtube.com/watch?v=p_bggBrUbCo) diatribes about esoteric corners of the data industry and long overdue [odes to Pitbull](https://twitter.com/bennstancil/status/1379534830740893703).*

*So, for entirely selfish reasons about chasing the life that I want to have, I'm going to shill for that employer for a moment: ***[Check out](https://mode.com/?utm_medium=referral&utm_source=benn-substack&utm_campaign=benn-substack-1)**[ ](https://mode.com/?utm_medium=referral&utm_source=benn-substack&utm_campaign=benn-substack-1)**[Mode.](https://mode.com/?utm_medium=referral&utm_source=benn-substack&utm_campaign=benn-substack-1)** *Or ***[try a demo.](https://mode.com/demo?utm_medium=referral&utm_source=benn-substack&utm_campaign=benn-substack-1)*** I would like the good people there to sell it to you, which helps them, helps me, and, if what I learned in college about [consumer surplus](https://en.wikipedia.org/wiki/Economic_surplus) is true, would help you too.* 


---


[^1]: The future proprietor of [benn.ventures](http://benn.ventures/) will regret having written this paragraph.

[^2]: So professional tops and leisure bottoms? [UNTUCKit](https://www.untuckit.com/) meets [Chubbies](https://www.chubbiesshorts.com/) (borderline NSFW)? I have no idea. I write a loosely technical and occasionally political blog about the data industry; I still own an American Eagle shirt; my fashion icon is [Pete Davidson](https://images.app.goo.gl/yiX3UGRsEJoRxBYX7). Donâ€™t ask me about clothes. You might as well ask a shark which vegan restaurant serves the best root vegetables.

[^3]: I know, dear reader, Iâ€™ve asked for several of these surveys, and havenâ€™t shared any of the results. I will. Itâ€™s the absentee ballots, ok? We canâ€™t report anything until we get the absentee ballots back from Maricopa County. But theyâ€™re coming, soon. We just need a bit more time.

[^4]: We didnâ€™t drink the data Kool-aid; we drowned in it.

[^5]: No shade on monitoring basic business functions, though. Itâ€™s still quite hard.

[^6]: Other things that might clear the â€œdo no harmâ€ bar that a company could invest in: A froyo machine in the kitchen. Branded Crocs. Ten free [Putt-Putt](https://puttputt.com/) tokens for every new employee.

[^7]: Sarah Catanzaro [expressed the same skepticism](https://twitter.com/sarahcat21/status/1507361644132904975) a few months ago.

[^8]: For what itâ€™s worth, Iâ€™ve changed my mind on this. I used to think that data was useful because itâ€™d occasionally uncover a company-altering discovery. While that can happen, these big breaks can also happen because you [get lucky](https://twitter.com/bennstancil/status/1550149621225295872), or because someone has a good instinct about an opportunity. I now believe the biggest determinant in how frequently companies hit home runs is how often they swing, not the type of bat they use when they do.

================================================================================

# The powder keg of the modern data stack

*The thing about a single pane of glass is that there can only be one pane of glass. Plus, survey results!*

---

![](https://substackcdn.com/image/fetch/$s_!RTwq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55908eed-ecba-4d5a-90ed-9d2b83d60864_1600x961.png)
*[powder keg](https://en.wikipedia.org/wiki/Powder_keg)*

It's 2032, and the halcyon days of the modern data stack are over. The funding environment has cooled; TechCrunch has moved on to the next new thing;[^1] community conferences in Austin have been replaced by trade shows in Dallas and the [â€œcoolâ€ newsletters](https://youtu.be/fiOMbqPHFwo?t=30) have been replaced by *CDO Weekly*, sponsored by Microsoft Synapse. The data startup ecosystem no longer looks vaguely like a Ponzi scheme, in which data teams build internal tools; then start companies based on those tools; then hire their own data teams; then those teams build more internal tools; then they create more data companies; and then we all sell our products to one another. Instead, data teams just buy a handful of boring defaults, and move on. Those of us who are still working in the industry tell war stories about this era, and the youths laugh at us, behind our backs.

In this calmer world, we've all stopped arguing about semantic layers, data contracts, and how we make sense of what all of our data means. In some improbable twist of fate, we landed on the good timelineâ€”data is no longer scattered across everything, everywhere, all at once.[^2] In our lucky fragment of the multiverse, data is organized and manageable. [Entities](https://benn.substack.com/p/entity-layer) are cleanly defined. When we look at dashboards, we debate what weâ€™re going to do about them, not if we can trust them. When we send marketing emails, we wonder which segments we should send them to, not if we sent them to the right people. Our tools all work in seamless harmony.Â 

Suppose that youâ€™re an executive in this blessed cosmos. A new director joins your team, and, being the analytically-savvy leader [that weâ€™ll all need to be](https://roundup.getdbt.com/p/analytics-isnt-for-analysts), they ask where they should go to start getting their arms around your companyâ€™s data. They want to get a birdâ€™s eye view of what data is available, the sources itâ€™s drawn from, the key assets that use it, and the health of the whole system.Â 

What specific website do you tell them to go to?

Back here in 2022, thereâ€™s no one place to go for this information. Our data tools are [now infamously](https://www.linkedin.com/posts/ethanaaron_snowflakesummit-data-analytics-activity-6942877586976890880-NvGT?utm_source=linkedin_share&utm_medium=member_desktop_web) fragmented, and our answer to the directorâ€™s question would be a list of a half-dozen vendors and a likely out-of-date wiki thatâ€™s sporadically maintained.

We can all agree that this isnâ€™t exactly ideal. Life would be much easier if we had a single source of truthâ€”or a â€œsingle pane of glass,â€ if you prefer [corporate technobabble](https://www.vulture.com/2020/02/spread-of-corporate-speak.html)â€”for whatâ€™s happening across the data stack.Â 

But to get there, we also have to agree on who should provide that pane of glassâ€”and right now, thereâ€™s nothing close to a consensus on *that* question.Â 

# Eyes on the same prize

For the last few years, most data startups followed Peter Thielâ€™s advice: [Avoid competition](https://www.wsj.com/articles/peter-thiel-competition-is-for-losers-1410535536). Companies have consistently tried to carve a unique niche for themselves, looking for some bit of white space in the data landscape that lets them be Switzerlandâ€”nobodyâ€™s competitor, and everyoneâ€™s partnerâ€”to as many other players as possible.[^3]Â 

This positioning, however, canâ€™t last forever. Eventually, as startups grow, companies that see one another as polite partners will start to jockey for the same space.

Collisions like this are happening in a few places, though each one is at a different stage. Major warehouse vendors like [Snowflake](https://benn.substack.com/p/i-snowflake), [Databricks](https://benn.substack.com/p/the-end-of-big-data), and [BigQuery](https://benn.substack.com/p/the-original-purple-people) have already [declared war](https://news.yahoo.com/snowflake-ceo-why-you-must-declare-war-on-your-competitors-183109540.html). ETL vendors (forward and reverse) are squaring up, and have had a [few skirmishes](https://benn.substack.com/p/metadata-money-corporation#footnote-3). BI and analytics companies, which are still running around [like a thousand different bickering tribes](https://benn.substack.com/p/business-in-the-back-party-in-the-front), havenâ€™t drawn enough categorical borders to have an organized fight, though one is surely coming.Â 

The biggest looming battle, however, will be over a different territory: The brainâ€”or [operating system](https://benn.substack.com/p/the-data-os)â€”of the data stack.Â 

Nothing like this exists yet. Nothing can tell us about the various activities that are bouncing around in our data tools, much less coordinate and manage that activity. Nobody owns the logic that orchestrates data services, or governs how different products talk to one another. There is no good answer to our directorâ€™s question from 2032: â€œWhat is the management console for our data stack?â€

But it would be nice if there was one, and I suspect a lot of companies have already started eyeing this idea. [Data](https://www.castordoc.com/) [catalogs](https://www.stemma.ai/), [data](https://atlan.com/) [discovery](https://www.secoda.co/) [platforms](https://www.selectstar.com/), [metrics](https://transform.co/) [layers](https://www.hellotrace.io/), [observability](https://www.montecarlodata.com/) [tools](https://www.bigeye.com/), [transformation](https://www.getdbt.com/) [and](https://www.astronomer.io/) [orchestration](https://dagster.io/) [tools](https://www.prefect.io/)[^4]â€”all of these categories offer a sliver of centralization, from a single place to find data, to understand its lineage, to monitor its accuracy, or to define the business logic thatâ€™s applied to it. Itâ€™s easy to imagine a [three-step master plan](https://www.tesla.com/blog/secret-tesla-motors-master-plan-just-between-you-and-me) from each of these companies, starting with where they are today, and ending with being the control tower for the modern data stack.

More importantly, for a lot of companies, this roadmap may not be a loose vision, but a fiduciary promise. Categories like data cataloging are probably too thin to support the [hundreds of millions of dollars in revenue](https://twitter.com/jasonlk/status/1552385334268289024) that companies need to find to justify the investments theyâ€™ve takenâ€”to date, the fourteen startups listed above, to say nothing of the [those that arenâ€™t included](https://www.moderndatastack.xyz/categories), have collected a total of $1.2 billion in funding. That will lead them to chase new markets, and the obvious choice is the very valuable opening at the center of the data stack. If thatâ€™s the case, a half dozen categories and scores of companies are careening toward a massive collision.

# Itâ€™s always a people problem

None of this is exactly news. Collisions like these are inevitable in frenetic markets, and this case is no different.Â 

Still, I think itâ€™s remembering this dynamic when we [criticize](https://benn.substack.com/p/the-modern-data-experience) the [state of the modern data stack](https://hackernoon.com/is-the-modern-data-warehouse-dead), and the apparent chaos within it. More than anything, the problem isnâ€™t our technology or how teams use it; itâ€™s the market. The master of ceremonies is a valuable role to be, and lots of companies have probably recognized that the data stack doesnâ€™t have one yet. That discourages us from rallying around any of the fourteen potential standards linked above, and instead pushes each of us to [create a fifteenth](https://xkcd.com/927/).[^5] Generally speaking, if we canâ€™t get something right in that many tries, the issue probably isnâ€™t some tweak that weâ€™ll fix on the next one. The issue is systemicâ€”and each new effort to make it better actually makes the fragmentation worse.

So where do we go from here? For better or for worse, the market giveth this problem, and only the market can taketh it away.[^6] But we can help it do its work, I think, by more openly acknowledging the reality of the situation: There are a lot of companies that are implicitly competing to unify the disparate elements of the modern data stack.Â 

The companies caught in this traffic jam should evaluate their products and roadmaps against their chances at winning *that* market, not the small box they currently compete in.[^7] And if their odds are lowâ€”which, for a lot of companies, they probably areâ€”Iâ€™d argue that theyâ€™d be better off figuring out how to thrive if someone else owns the center of the data stack. Because by 2032, someone will own it. But there can only be one unifierâ€”there can only be one single source of truth, one single pane of glassâ€”and [not everyone can be king](https://en.wikipedia.org/wiki/Archduke_Franz_Ferdinand_of_Austria).

# Reader mailbag! â€“ Are companies profitable?

Four months ago, I wrote a post that [included a survey](https://benn.substack.com/p/startups-shouldnt-care-about-revenue#footnote-2) asking people how frequently their employers actually turned a profit. [A day late](https://benn.substack.com/p/do-data-driven-companies-win#footnote-3) (and in this surveyâ€™s case, many dollars short), here are the results:

Thirty people responded to the survey. In total, those people have worked for 361 years. In 166 of those years, or 46 percent, peopleâ€™s employers were profitable. There wasnâ€™t much difference between people who identified as a tech person, a startup people, neither, or both. In every group, companies made a profit 45 to 50 percent of the time.Â 

However, people with more experience had spent a lot more time at unprofitable companies. People with less than ten years of experience spent 73 percent of their working years working for a profitable company. For people with ten to nineteen years of experience, that figure fell to 50 percent; for people whoâ€™ve worked twenty or more years, it fell even further, to 26 percent. I have no idea what to make of this, other than to assume that thereâ€™s a lot of bias in the sampleâ€”if youâ€™ve been working for thirty years and made money in most of them, you probably have better things to do with your time than respond to surveys on some some weird blog.

Finally, to those of you who had something else you wanted to yell about:

â€”

> *The rent is too damn high, but if I move where am I supposed to get good pho at any hour of the day.*

This survey was in March, so at least rents havenâ€™t gone up sincâ€”[oh my](https://www.cnbc.com/2022/07/14/average-rent-in-manhattan-was-a-record-5000-last-month.html).

â€”

> *I assure you the impulse to elevate data and analytics teams to be "strategic" and to own decisions and sit in board meetings, etc is negatively correlated with profitability. *

Look, the rent is too damn high, and some of us need to make sure the music doesnâ€™t stop. Let us tell ourselves that all of [this angst](https://benn.substack.com/p/do-data-driven-companies-win) is just imposter syndrome; weâ€™re worth it; Iâ€™m doing great; weâ€™re all doing great.Â 

â€”

> *Hillary Clintonâ€™s emails.* 

Yeah, but what about [Hunter Bidenâ€™s](https://www.youtube.com/watch?v=n3N-YIfVpFI) laptop?

â€”

> *Benn with two n's is a great brand.*

Shoutout to Heather and Jim for that oneâ€”a great teacher, a great lawyer, great parents, and now, great brand marketers.Â Â 


---


[^1]: Like, I dunno, B2B SaaS products for companies that exist solely in the metaverse. Donâ€™t tell me that between now and 2032, someone wonâ€™t pitch their startup as Shopify for [Roblox entrepreneurs](https://thehustle.co/08182020/).

[^2]: What paper cuts do I have to give myself to jump to this timeline?

[^3]: In fairness, this isnâ€™t just about how companies position themselves. The market is unsettled, and a lot of buyers purchase tools that appear, on the surface, to be directly competitive.

[^4]: Iâ€™m a [personal investor](https://benn.substack.com/p/disclose-your-angel-investments) in Bigeye and Elementl.

[^5]: Iâ€™d argue that this dynamic also explains why we still donâ€™t have any good analytical templates. Most template libraries are created by vendorsâ€”be it [Mode](https://mode.com/playbook), [Looker](https://docs.looker.com/data-modeling/looker-blocks), [Tableau](https://www.tableau.com/learn/articles/sales-dashboards-examples-and-templates), [Sisense](https://www.sisense.com/dashboard-examples/), [Deepnote](https://deepnote.com/use-cases), [Hex](https://hex.tech/gallery/), [Streamlit](https://streamlit.io/gallery), or someone else. Sure, there are [well-meaning community intentions](https://mode.com/blog/open-sourcing-our-analysis/) behind these efforts, but theyâ€™re primarily meant to drive a business outcome. This means that companies are motivated to promote *their templates* over *templates as a general asset*. Itâ€™s tough for a standard package of templates to emerge when everyone whoâ€™s creating them would, if push came to shove, probably knife someone elseâ€™s library if it gave theirs a better chance to succeed.

[^6]: Will it work through its invisible hand, a [papal bull](https://www.zdnet.com/article/the-rust-programming-language-just-got-a-big-boost-from-meta/) from on high, or [Stripeâ€™s hitmen](https://techcrunch.com/2022/01/31/ryanbreslow-bolt/)? Time will tell.

[^7]: When companies in these various categories raised money, I suspect most of them didnâ€™t show vendors from other categories on their mandatory [two-by-two competition matrix](https://hunterwalk.com/2020/05/25/if-your-pitch-deck-has-a-competitive-2x2-im-going-to-ask-you-this-question/) slide. But if they drew the same grid for who theyâ€™d be competing with at the end of their [path to $100 million](https://www.bvp.com/atlas/scaling-to-100-million) in revenue, thereâ€™d be a lot more overlap.

================================================================================

# Analysts should have portfolios

*Replace boot camps with slide decks.*

---

![Disney Fan Theory Reveals Who Built the Cave of Wonders in 'Aladdin'](https://substackcdn.com/image/fetch/$s_!qYA2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F67d32ce9-2269-4629-8730-648a15795e30_1920x1080.jpeg)

If I left Mode tomorrow, would you interview me to be an analyst on your data team?

Perhaps you like this blog, and think you might. Or maybe you hate it, and youâ€™d rather put your business in the hands of Google's "I'm Feeling Lucky" button, [Miss Cleo](https://en.wikipedia.org/wiki/Miss_Cleo), or the call-in segment of *[Car Talk](https://en.wikipedia.org/wiki/Car_Talk)* than risk it with me.[^1] 

No matter your feelings, though, you donâ€™t have much to judge my capacity as an analyst on. Despite me blowing more than my fair share of hot air into the public square, it's pretty hard to say whether or not I'm actually any good at "doing the work." I could be a lousy analyst who's just loud.Â 

For other candidatesâ€”the healthy ones who have better things to do with their time than write a weekly blog about dataâ€”the information gap is even worse. Unlike sales people, we canâ€™t share our quota attainment records because our work is often [impossible to quantify](https://benn.substack.com/p/method-for-measuring-analytical-work). Unlike engineers, we canâ€™t show people the technology weâ€™ve built or point them to clever bits of public work on Github, because analytical artifacts like SQL queries and Python scripts typically only have meaning in the context in which they were created. Unlike executives, we canâ€™t use our organizationâ€™s success as evidence of our own, because there are [too many steps](https://benn.substack.com/p/chasing-ghosts) between good analysis and good business results. Instead, the only places we have to showcase our work are blogs and Twitter, which favor those who want a public profile, and Kaggle, which favors those who want to brag about their Mensa membership and Klout score without having to go to more than one website.

In the absence of better signals, most of our fates as job candidates are determined by our resumes and LinkedIn profiles. But resumes tend to only capture two things: The pedigree of your prior experiences, and how good you are at writing a resume. Unfortunately, in the latter case, career counselors and Microsoft Word templates seem to have taught us that resumes should be lengthy lists of skills[^2] and artificially quantified accomplishments,[^3] rather than useful descriptions of what weâ€™ve done and what we like to do. The only talent we can show off on this format of resume is restraint.

And so, as hiring managers, we apply crude and problematic filters when screening candidates to interview. Have I heard of their prior employer? Does their list of technical skills include the arbitrary things that I prefer? Is their resume too long to bother reading? Were they referred by someone I know? Do I [like their name?](https://www.wbur.org/hereandnow/2021/08/18/name-discrimination-jobs)[^4]Â 

Putting aside issues of nepotism[^5] and bias, this approach leaves a lot of talented people on the outside looking in. It creates a variant of the experience catch-22â€”I canâ€™t get a job without experience, but I canâ€™t get experience without a jobâ€”in which analysts can best demonstrate their abilities in an interview, but canâ€™t get an interview without demonstrating their abilities. Unlike the experience trap, however, this applies to mid-level and senior analysts. Consider: If you have a decade of great work behind you, but no brand names on your LinkedIn profile, how do you get noticed? Call me a cynic, but adding [numbers and action verbs](https://resumegenius.com/blog/resume-help/how-to-write-a-resume#numbers) to the three bullets under each job probably wonâ€™t do the trick.

As a result, companies lose out on a massive amount of talent, and talented analysts lose out on a massive amount of opportunity. Analysts from blue chip tech companies and top ten colleges bounce around from prestige job to prestige job, while those on the outside fight tooth and nail to get so much as a phone screen. Which is a tragedyâ€”some of the best analysts I know came from famous companies and fancy colleges, but others came from decades-old retailers, unremarkable colleges, and the bowels of Charles Schwabâ€™s audit department. The only difference between the two groups is how hard they had to work to get the interview.

I think we can solve this thoughâ€”analysts should create portfolios.Â 

# The eight slide resume

When hiring analysts, there is one thing that catches my attention every time I see it on a resume: A personal blog of analysis on public data. Itâ€™s not that the existence of a blog is a signal; itâ€™s that blogs give you an immediate glimpse of the type of analyst their author is. Are they drawn to machine learning or statistical projects? Do they focus on data collection and cleanup? Are they curious problem solvers, and see data as an interrogative means to an inquisitive end? Blogs also expose how clever they are as an analyst, how crisp they are as a communicator, and a bit about their style and personality along the way. If I could choose one way to screen candidatesâ€”a resume review, a take-home coding test, a traditional interview, or a look at their blogâ€”Iâ€™d choose the blog.

Blogs, however, have problems. Like contributing to open-source software, writing a blog isnâ€™t something everyone has the time or interest to do. And blogs introduce favoritism, where I might prefer someone if they write about things that personally interest me. For example, Iâ€™m predisposed to like a candidate if I find their blog about [sports analytics](https://benn.substack.com/p/a-season-without-bats) than if they write about the Westminster dog showâ€”both because of affinity for the subject, and because it gives me material to [lazily wander through](https://twitter.com/harryhurst/status/1415089258218627072) a biased [â€œbeer testâ€ interview](https://www.cnbc.com/2017/04/25/the-beer-and-barbecue-test-a-ceo-uses-to-hire-great-people.html).Â 

Portfolios keep the baby and throw out the bathwater.

Like design portfolios, an analytical portfolio would be a short exhibition of professional work. It would walk through a few projects the analyst completed, and share the context behind different problems, how they solved them, and the impact of their efforts. It could explain their thought process and their [analytical wanderings](https://stkbailey.substack.com/p/wander-well). They could include partially redacted charts, slides, and dashboards that they built.Â 

Though a portfolio could also include code, the point isnâ€™t to share SQL snippets or a few Jupyter notebooks; itâ€™s to talk about the thinking *around* the code. As an interviewer, I want to understand how candidates translate a vague business problem into a quantitative one, and how they navigate the inevitable frustrations of their first efforts not working out. I want to hear about what unexpected things they uncovered, and how they found them. I want to learn how they explain problems and tell stories. And as a candidate, I want to share a bit about myselfâ€”the projects Iâ€™ve enjoyed, the things Iâ€™m proud of, and the work I want to do.

A simple deckâ€”the sort of thing for which a first draft could be made in a few hoursâ€”could accomplish all of this. Inspired by the overly prescriptive spirit of a [five-paragraph essay](https://en.wikipedia.org/wiki/Five-paragraph_essay),[^6] hereâ€™s how Iâ€™d do it in eight slides:

Look, [I made an example!](https://docs.google.com/presentation/d/1eECK5TZIYBRhNMsIjfSPpe2jKh72FmzowRHPKZvTU-g/edit#slide=id.p)

# A common app for analysts

Imagine a world where analysts are taught that their ticket to their dream job isnâ€™t a three-month coding boot camp, but a great portfolio that tells the story of their interests, their work, and their career. Imagine if companies used this material, even if it was just a short slide deck, as the most important piece of a job application.Â 

First, this would reorient analytical jobs away from technical skills and towards the [creative reasoning skills](https://benn.substack.com/p/analytics-is-at-a-crossroads) that they actually require. Whether we like it or not, job candidates are going to [teach themselves to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test): So long as we use take-home SQL assignments and whiteboard coding exercises in our interview processes, thatâ€™s what people will try to learn. Throwing those out in favor of a more creative portfolio proves to candidates that weâ€™re serious about certain technical skills being â€œnice to haves.â€ Just as design portfolios push designersâ€™ choice of tools into the background in favor of highlighting what they actually create, an analytical portfolio puts analysis in front of the languages with which it was performed.

Second, portfolios are an equal opportunity asset. Recent grads can show off school work and personal projects. People looking to make career transitions can include [side projects that show their potential as an analyst](https://benn.substack.com/p/the-next-billion-programmers#:~:text=By%20writing%20a%20script%20that%20would%20automatically%20find%20promising%20candidates%20and%20send%20them%20messages%20on%20LinkedIn%2C%20a%20friend%20of%20mine%20became%20many%20times%20more%20productive%20as%20a%20recruiter%20and%20changed%20the%20course%20of%20his%20career.). Moreover, because portfolios let people tell the story of *how *they did their work, small projects can be just as compelling as big ones.

Third, the structure of the portfolio itselfâ€”the narrative frame around the analytical picturesâ€”is another useful way for candidates to differentiate themselves. Analysts aren't just actuaries who take in and spit out numbers; they're storytellers who need to be engaging and [persuasive](https://benn.substack.com/p/method-for-measuring-analytical-work). Developing and presenting a portfolio allows for exactly that.Â 

Fourth, interviews would be better. Portfolios could replace [controversial take-home tests](https://getdbt.slack.com/archives/C022A67TLFL/p1633052386122700), which turn some candidates away, with material that could be repurposed for every interview (analyst portfolios, the [common app](https://en.wikipedia.org/wiki/Common_Application) for data jobs). Portfolios also give potential employers more context about applicants, allowing interviewers to cover more ground in their conversations with candidates.

Finally, portfolios offer the benefits of a personal blog with less potential for the same bias. By normalizing a focus on work projects over personal hobbies, portfolios would be more likely to be judged on their quality instead of their choice of subject matter. They would take less time to maintain than a blog. And because they don't need to be public, portfolios don't favor those who are comfortable maintaining a public presence over those who'd rather be more private.

# Diamonds in the rough

One of the best product designers I know didnâ€™t start her career as a designer. She was working for a fashion company as a project managerâ€”hardly the type of resume that would catch the eye of most tech design teams. But she dabbled in design, and happened to have a chance to share her work with one of the most respected designers at one of Silicon Valleyâ€™s most design-oriented companies. Immediately impressed with her talent, they offered her not only a job, but an entirely new career.Â 

I firmly believe there are similar analytical [diamonds in the rough](https://www.youtube.com/watch?v=H3bkQB6-j0E)[^7] out there. There are thousands of great hires to be had and careers to be madeâ€”but as long as the best way to get people to pay attention to you is by being [obnoxious on the internet](https://twitter.com/search?q=from%3A%40bennstancil%20friday%20fight&src=typed_query), theyâ€™ll remain undiscovered. Analytical portfolios could finally give them a [chance to shine](https://www.youtube.com/watch?v=lWA2pjMjpBs).


---


[^1]: Or, given that one of the *Car Talk* guys is dead, you might need to ask Miss Cleo to ask them. *Update: I have been informed that Miss Cleo is also dead.*

[^2]: Skills: SQL, Python, R, Julia, Scala, Rust, Javascript, Java, C, C++, Java++, Go, Unix, Linux, Linus, Lucy, Ruby, Diamond, Sapphire, Chase Sapphire Reserve, PHP, PCP, THC, TLC, BTS. Proficient in Microsoft Word.

[^3]: Accomplishments: While I was an analyst at GameSpot, I conducted frequent analyses to help guide marketing operations and strategy. Results I helped drive: CPC improved by an average of 12%; paid pipeline volume increased by 19% y/y; GameStop stock rose by 2,658%.

[^4]: [If the shoe fits.](https://benn.substack.com/p/who-is-the-community)

[^5]: [Shoutout to nepotism tho](https://benn.substack.com/p/analytics-is-at-a-crossroads#:~:text=But%20it%20wasn%E2%80%99t%20my%20r%C3%A9sum%C3%A9%20that%20got%20me%20the%20job%3B%20it%20was%20nepotism.).

[^6]: But make yours in your own way, [because five-paragraph essays are](https://aeon.co/essays/writing-essays-by-formula-teaches-students-how-to-not-think) â€œdysfunctionalâ€¦off-putting, infantilizing and intellectually arid.â€

[^7]: Ok, so I havenâ€™t seen *Aladdin* in a while, and this opening scene is really something. A sinister sorcerer meets a thief in the desert. The sorcerer asks the thief for the thing he paid him to steal; the thief says he has it, but had to â€œslit a few throatsâ€ to get it. The sorcerer steals it from him and uses it to conjure a giant tiger head in the sand. The sorcerer commands the thief to walk into its mouth. The tiger head tells the thief not to; the thief becomes understandably scared; the sorcerer tells him to go in anyway. The tiger head kills the thief. The mildly irritated sorcerer flatly says the thief wasnâ€™t worthy; his parrot sidekick makes a sarcastic crack about it.

================================================================================

# Down with the DAG

*Reverse the ETL timeline.*

---

![](https://substackcdn.com/image/fetch/$s_!8b3a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9d403c7c-9ac1-4eed-90c4-243c8713b7be_1200x600.png)
*[Nothing makes sense and everything is backwards.](https://nypost.com/2020/08/31/tenet-review-christopher-nolans-most-confusing-movie-ever/)*

If a thousand startups have died on the hill of building a better note-taking app, a thousand masochists have died on the hill of building a better calendar.Â 

So naturally, Iâ€™m here to say that the data industry needs a better calendar.Â 

Complaining about schedulers is one of the data communityâ€™s most consistent hobbies, so Iâ€™m not exactly taking some brave stand against a popular hero. To the contrary, Airflow has been a target for yearsâ€”of [competitive](https://dagster.io/blog/dagster-airflow) [products](https://www.prefect.io/guide/blog/why-not-airflow/), of [unbundling](https://blog.fal.ai/the-unbundling-of-airflow-2/), and, just last week, of [breakup notes](https://stkbailey.substack.com/p/airflows-problem).Â 

Most of these objections, however, argue that Airflow is structurally outdated. It isnâ€™t designed for managing todayâ€™s complex data stacks, and itâ€™s not built for modern development workflows. As data teams, we no longer need a tool for running tasks in ordered steps (aka, the infamous DAG); we now need [control planes](https://docs.astronomer.io/astro/data-plane-activation), [coordination planes](https://annageller.medium.com/the-unbundling-vs-rebundling-of-a-data-stack-debate-missed-the-point-beb35ef4ae54), and broader, richer systems of *orchestration*.Â 

Okay, sure. A [single pane of glass](https://benn.substack.com/p/powder-keg) sounds nice. But itâ€™s also a sort of shapeless, all-encompassing leviathan that we can never quite describe. I donâ€™t want to make ([yet another](https://benn.substack.com/p/the-data-os)) case for that, nor do I want to pile on to Airflow.[^1] Instead, I want to make a narrower point specifically about how we schedule things: We do it backwards.Â 

Nearly every data tool schedules stuff like you arrange dominos. You line up a series of ordered tasks, and configure when to knock over the first one. At that scheduled timeâ€”or, when some other external event happens, such as a file getting updated somewhereâ€”the system kicks it down, [triggering the entire chain of subsequent tasks](https://twitter.com/ai_memes/status/1382374419666976771).Â 

In simple cases, this paradigmâ€”A causes B, B causes C, left-to-right across a sequence of eventsâ€”is easy to create and understand. As soon as things get more complicated, however, everything breaks down. Causal chains become impossible to understand and delicate to maintain. And more importantly, theyâ€™re defined by what triggers them, not by what theyâ€™re meant to create.

# What causes a plane to take off?

To extend every orchestratorâ€™s favorite analogyâ€”the [air traffic](https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638) [controller](https://www.prefect.io/guide/blog/the-global-coordination-plane/#ATCfortheMDS)â€”imagine being an airport administrator. To run an airport, you could model the entire operation around the events that matterâ€”when do flights take off and land?â€”and build forwards and backwards from there. For the flights you want, how many gate agents need to be available to check people in? Can the security line process people fast enough? What baggage claim carousels need to be ready when a new plane arrives? First schedule all the flights, and then figure out what has to happen to support them. To add more flights, stick them in the model, and sequence the rest of the necessary processes around them.Â 

An alternative approach would be to model the airport as a huge DAG, starting with people arriving at the terminal. When a car shows up, tell an airline employee to go to the baggage counter to check their bag. Once a hundred people arrive, begin boarding the plane. When all boarding tasks have been completed, taxi the plane out to the runway. When a plane lands, queue a team of [aircraft marshallers](https://www.tiktok.com/@complexaviation/video/7047254045613903109) to guide it to a gate. Fifteen minutes after a plane arrives, call twenty cabs.Â 

This approach would be a disaster. Planes wouldnâ€™t actually have scheduled departure times; theyâ€™d just take off whenever the preceding tasks were done. It would be impossible to add a new flight to the daily schedule without causing huge disruptions, because you couldnâ€™t add it directly; youâ€™d instead have to figure out which sequence of dominos to push over, and hope that the chain reactions they cause eventually gets a plane in the air. The problem gets even harder as you add more variability, like wanting to schedule some commuter flights to take off six times a day in a [lawn mower with wings](https://www.youtube.com/watch?v=FH-LmkLFJg0), and others to be weekly long-haul trips in a [flying beluga whale](https://en.wikipedia.org/wiki/Airbus_Beluga).[^2]Â 

Unfortunately, weâ€™ve built most of our data pipelines using the latter modelâ€”we want the updated datasets or fresh dashboards at the end of our lineage graph, but define our schedules around whatâ€™s at the beginning of it. Like an airport that coordinates everything based on when Ubers drop people off, itâ€™s hard to create, hard to understand, and hard to maintain.Â 

Iâ€™m convinced that thereâ€™s a better way.Â 

# Reverse ETL orchestration

Mode was inspired by an internal query tool that I usedâ€”and [Josh](https://twitter.com/besquared), one of Modeâ€™s other cofounders, builtâ€”at Yammer. In addition to that tool, Josh also created an internal data transformation tool called Integritie,[^3] which was very similar to dbt. As analysts, we committed parameterized SQL statements and YAML configuration files to Integritie, which would use the queries, and the dependencies implied in them, to regularly create and update tables in our warehouse. In May of 2016, a couple years after starting Mode, I built a command-line version of the same thing, which I named Easybake.[^4]

Both Integritie and Easybake were simpler, cruder versions of dbt in nearly every way, except one: Their schedulers. In both tools, there was no explicit dependency graph, and we never scheduled a chain of models to run at specific times (i.e., there was nothing analogous to `dbt run`.). Instead, for each model, analysts set a latency requirementâ€”this table should never have data thatâ€™s more than four hours out of date, a day out of date, a week out of date.Â 

The system figured out the rest. It would construct the DAG behind the scenes, and work out when to run all of the models, including upstream ones, to maintain the guarantees. It would then continually orchestrate all the runs, updating models as needed. When jobs failed, it would alert you of the failure and, as downstream delays cascaded through the system, when other tables started exceeding their latency requirements.Â 

It wasnâ€™t perfect. I used some hacky heuristics to estimate how long a model would take to run; it would sometimes update tables unnecessarily; it didnâ€™t make any effort to distribute runs in an intelligent way, which could create bottlenecks if a bunch of tables were about to expire at the same time.Â 

But it worked really well in three ways:Â 

First, because each model was configured independently, we could easily set different requirements for different tables. Important dimension tables often had tight guarantees of an hour or less; computationally expensive tables, like rollup tables that we used for reporting, were rebuilt once a day, or even once a week. This significantly lowered the burden we put on our databaseâ€”and, had metered cloud databases like Snowflake and BigQuery existed at the time, wouldâ€™ve lowered our costs.Â 

Second, even differentiated requirements like these were simple to maintain. When adding a new table that we needed to update once a day, we didnâ€™t have to define a procedure for making it so, like choosing which schedule to attach it to or where it should sit in the DAG. We just told the tool [what we wanted](https://www.youtube.com/watch?v=E1I0hAxGFXw).

Third, latency requirements were a direct way to tell the application what was important to usâ€”fresh dataâ€”*and* to identify when something was broken in a meaningful way. In scheduler-based dependency graphs, we often conflate failed jobs with out-of-date data. But thatâ€™s not actually true. A frequently-updating data ingestion task, for example, might periodically fail and self-correct, all within the bounds of a latency [SLA](https://en.wikipedia.org/wiki/Service-level_agreement). Alerting people about the failed task teaches us to ignore these warnings, and divorces system problems and internal failures from actual problems that affect whether or not data can be trusted.Â 

# On the roadmap

Still, these things being internal toolsâ€”and one of them being an internal tool [that I built](https://docs.google.com/presentation/d/1WG6-CuTSOudHsUyDcwsIGCN5iAhMk1P3-RTXnkIzhKM/edit#slide=id.g458d530c43_0_5)â€”they barely scratched the surface of what was possible. Had Josh had more time, or if I had modicum of understanding about how to actually create software,[^5] we couldâ€™ve extended this paradigm in all sorts of interesting ways.Â 

We couldâ€™ve made it capable of detecting when latency guarantees were inconsistent with upstream dependencies. For example, if I assigned a six-hour requirement to a customer activity table that was derived from an accounts table with a twelve-hour guarantee, the system would update the accounts table more frequently. Instead, it couldâ€™ve told me about this mismatch, and let me decide how to handle it.Â 

With a way to identify these conflicting requirements, we couldâ€™ve added reverse guarantees that would prevent tables from updating too frequently, protecting ourselves from wasteful jobs and runaway database costs.Â 

We couldâ€™ve built smarter ways to distribute load. The system knew the state it wanted to create; it could, like a SQL planner optimizing a query plan, figure out the best way to get there. We couldâ€™ve gone even further and told it what to optimize for, like spreading jobs out evenly over time, running them all at once, or minimizing how long they take.

We couldâ€™ve supported other temporal guarantees, such as latency requirements fixed to a certain time. These would function like standard schedules, but with bandsâ€”make sure this is always updated sometime after midnight and before 8 am. Range requirements would both maintain the guarantee-oriented paradigm over the task-based one, and give the scheduler more flexibility over when to update tables.

We couldâ€™ve let people specify layers of guarantees, where the system tries to maintain the strict one, but only alerts us if a looser one gets violated. This could help temper the alarms and notificationsâ€”this Fivetran job failed! Stitch ran into Marketoâ€™s API limit! Github is down!â€”that are now all too easy to disregard.

We couldâ€™ve used these guarantees to create tidy indicators of pipeline health, where we track how much time individual jobs and the system as a whole are meeting their latency requirements. As it stands today, concepts like [data downtime](https://www.montecarlodata.com/blog-the-rise-of-data-downtime/) are mostly marketing catchphrases that represent the vague and overinclusive notion of there being errors and anomalies in your data stack. But downtime should mean something more precise: When tables[^6] violate their SLAs. Just as engineers wouldnâ€™t call an application down because it throws a few errors, we shouldnâ€™t call data down because things fail. Instead, we should specify the latency bounds weâ€™re willing to tolerate, and only worry if we step beyond them.Â 

# Isnâ€™t it justâ€¦?

People have spent their [entire lives on problems like these](https://en.wikipedia.org/wiki/Directed_acyclic_graph#Mathematical_properties), and Iâ€™m sure there are enormously difficult challenges with even the simplest parts of this proposal.[^7] Iâ€”a technical buffoon with a bachelorâ€™s degree and a blogâ€”wonâ€™t pretend to know how to write a program or do the math to build something that does all of this. But as a *user*, this is the experience I want. As critical as they are to make scheduling systems work, I donâ€™t actually want to think about when to run jobs, how to define DAGs, or to manually orchestrate anything. I just want my data to be freshâ€”where I can declare what fresh meansâ€”and to know when itâ€™s not. Like a passenger wanting to know if their flight is [delayed or departing on time](https://twitter.com/bennstancil/status/1507440263001743365), I donâ€™t care about the intricate and fragile complexities that make it possible for me to safely rocket myself across the planet from major city to tiny remote island in a matter of hours; I just want to know [when I can complain](https://twitter.com/bennstancil/status/1537871433845514240).Â 


---


[^1]: You gotta give credit to anything that tries to make computers understand time. Time zones donâ€™t make any sense (what time zone [is Arizona in](https://en.wikipedia.org/wiki/Time_in_Arizona)?); doing math with dates is hard (whatâ€™s February 29, 2020 plus a year?); time is impossible to format (when is 10/11/12?); we canâ€™t even agree on what words mean (when does a week start?). I can barely set an alarm clock; if you can figure out how to tell a computer how to automatically adjust clocks in Phoenix for daylight saving time during a leap year, you have my eternal respect.

[^2]: Imagine building [this plane](https://en.wikipedia.org/wiki/Airbus_Beluga) and thinking, â€œLooks great, if only it were [bigger](https://en.wikipedia.org/wiki/Airbus_BelugaXL).â€

[^3]: As in â€œintegrity,â€ but ending in -ie. For reasons unknown, that was the naming standard for all internal tools at Yammer (and for [most baseball nicknames](https://www.si.com/mlb/2017/06/16/nickname-weekend-best-player-nicknames#:~:text=My%20fear%20is%20that%2C%20on%20nickname%20weekend%2C%20we%27re%20going%20to%20get%20dozens%20of%20dudes%20who%20will%20follow%20Joe%20Girardi%27s%20naming%20convention%20and%20sport%20jerseys%20with%20a%20shortening%20of%20their%20last%20name%20and%20a%20%22Y%22%20attached%20to%20the%20end)).

[^4]: About a year after creating Easybake, we scrapped it in favor of dbt. Prior to that, we briefly considered adding Easybake into Mode, but deemed it a bad business. A dbt-led revolution of data transformation laterâ€¦[oops](https://www.youtube.com/watch?v=noekVG8XLQI).

[^5]: Seriously, what is `__init__`?

[^6]: This could be extended to other data assets as well, like dashboards, operational data pipelines, ML models, and so on.

[^7]: I bet, for instance, that thereâ€™s some [Arrowâ€™s impossibility theorem](https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem) for schedulers, where a scheduler can only ever do four of the five things you want it to.

================================================================================

# What do we do when we get it wrong?

*This is not a rhetorical question.*

---

![](https://substackcdn.com/image/fetch/$s_!PKh7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5707141-0353-4da5-8ac1-0aa2ccb7564f_3100x1560.png)
*[we miscalculate](https://www.youtube.com/watch?v=NnPBSy5FsOc)*

Every once in a while, to the great displeasure of my coworkers and colleagues, I [have the opportunity](https://benn.substack.com/p/why-do-people-want-to-be-analytics#:~:text=getting%20their%20hands%20dirty%20on%20some%20real%20analysis) to â€œget my hands dirty on some real analysisâ€ at Mode. In these brief, perilous moments, I become an [anachronistic analyst](https://www.youtube.com/watch?v=98k05_bM2e4), an out-to-pasture exec pretending not to be out of practice, a clown in the codebaseâ€”a codebase where my legacy is a shoddy foundation that has long since been gutted, corrected or replaced. [I try to show off my arm](https://www.youtube.com/watch?v=xL-VX3WbA9U); everyone else waits for me to leave.Â 

To nobodyâ€™s surprise but my own, I sometimes get things wrong. I accidentally fan out a join in a product usage model;[^1] I fail to consider some nuance in a query that computes our average revenue per user;[^2] I botch the logic behind our user retention rate.[^3] In some cases, these errors get caught immediately and never make it beyond the safe confines of our data teamâ€™s review process. In other cases, my mistakes, shrouded in a six hundred line Rube Goldberg machine of [unconventionally formatted SQL](https://benn.substack.com/p/the-case-against-sql-formatting), slip through.[^4] They make their way into productionâ€”in dashboards, in board decks, in [peopleâ€™s heads](https://counting.substack.com/p/the-many-faces-of-production#:~:text=But%2C%20in%20a%20very%20peculiar%20sense%2C%20the%20production%20environment%20has%20shifted%20to%20become%E2%80%A6%20the%20minds%20of%20the%20people%20in%20the%20organization.)â€”and silently compound until, months later, some unwitting data scientist, hiking through an adjacent query, finds a dead body.

As an industry, we spend a lot of time talking about how technology can protect analysts from these mistakes. We [create](https://www.montecarlodata.com/) [products](https://www.bigeye.com/)[^5] to detect them; we write [contracts in our APIs](https://www.linkedin.com/feed/update/urn:li:activity:6963880392672043008/) to prevent them; we [build processes](https://davidsj.substack.com/p/five-nines-three-nines-no-nines) that correct them; we [debate architectures](https://roundup.getdbt.com/p/interfaces-and-breaking-stuff) that make them harder to introduce. This all seems generally good; if data is as messy and error-prone as every cynical analyst and data observability pitch deck says it is, we need to take a lot of precautions.Â 

But thereâ€™s a weird paradox to this dialogue. For as much as we talk about how to avoid getting things wrongâ€”which is an implicit acknowledgement that we get things wrong a lotâ€”we spend hardly any time talking about what happens when we do.Â 

# Some shift in the numbers

In early February of this year, sometime in between my unwelcome joy rides through our dbt project and Mode report library, I logged into Substack to look at the metrics for this blog. Substack rewarded my vanity with an alarming message: They'd messed up. When calculating the number they use to headline every postâ€™s performanceâ€”the number of viewsâ€”they double counted email opens. The metrics (which I definitely donâ€™t care about and assign no personal meaning to, no, none whatsoever, nothing at all was tied up in those at all, I didnâ€™t even know Substack tracked views tbh) fell off a cliff. Everything was down between twenty and fifty percent.Â 

I have no inside knowledge of what caused the issue, and Substack didnâ€™t say much about it [beyond a few tweets](https://twitter.com/search?q=until%3A2022-02-11%20since%3A2022-02-10%20from%3A%40substackinc&src=typed_query&f=live) about â€œa bugâ€ that caused [â€œsome shift in the numbers.â€](https://twitter.com/SubstackInc/status/1491779791711870976) But having published my fair share of eventually-correct dashboards, I have my guesses: Some events got logged twice, and the query that computed view counts didnâ€™t clean this up. Or an analyst misunderstood the distinction between `email_opened` and `email_viewed` events, and mistakenly thought that opening an email implied it was viewed. Or a query just had a typo or a bit of faulty logic or a bad join onto a table that seemed like itâ€™d have one row per email but actually has two, and view counts got doubled. Such are the occupational hazards of being an analyst. Do the job log enough, and we all [break something](https://www.amazon.com/dp/B00CBNIBAU/ref=sspa_dk_detail_0)[^6]â€”and somehow, the numbers are never revised up.[^7]

For all of the various guides, training materials, and SEO-bait that our industry produces, Iâ€™ve never seen a single conversation[^8] about what weâ€™re supposed to do whenâ€”when, not ifâ€”something like this happens.Â 

Perhaps thatâ€™s because thereâ€™s a seemingly common sense, ethical answer: Tell people about the issue, disclose what happened, and update charts and figures with the right values. And we can, as we are often inclined to do, borrow from engineering teams: Hold a [blameless retro](https://newrelic.com/blog/best-practices/blameless-retrospectives), write an [after-action review](https://en.wikipedia.org/wiki/After-action_review), and move on. No deeper discussion is required.Â 

Maybe. But if nothing else, I think itâ€™s worth asking if the ostensibly â€œrightâ€ answer is actually so right after all. Plus, there are enough differences between engineering incidents and data errors that we should question if thatâ€™s the right discipline to draw our lessons from.Â Â 

# Analytics != Ethics

Suppose that you shipped some worryingâ€”but not egregious!â€”error that miscalculates a metric of moderate importance. Say, for instance, that the list of leads you send out to your sales team every week ranks leads incorrectly, and the best prospects arenâ€™t always at the top of the list. Nobody notices, and you find the mistake before anyone else catches it. Putting aside what you *do*â€”and your conscience, and any inherent value in being transparent[^9]â€”if you could [manifest](https://www.vice.com/en/article/qjbn43/manifesting-is-gen-zs-answer-to-new-age-spirituality) a reality, what would it be?

Ideally, I *think* youâ€™d want the issue to be fixed, and nobody to know it was ever wrong. Despite its sheen of scientific rigor, data is a [confidence game](https://twitter.com/seanjtaylor/status/1433636587699539996); the only currency we have is trust. Issuing a correction, then, undercuts that trust, with little benefit. People start questioning if any data is reliable, and sales reps start giving your lead scores the side-eye. Itâ€™s possible the correction shows people that youâ€™re making efforts to find and fix issuesâ€”but more likely, I suspect, it just makes people aware that there are potential issues for them to be worried about.Â 

The counterpoint to this is that mistakes donâ€™t happen in a vacuum. While the lead scoring bug might escape notice, some other problem wonâ€™t. A data team that silently fixes the errors they find, but publicly fixes the errors that other people find, might make everyone believe that they only test in production. In this context, a few showy corrections could bolster peopleâ€™s trust in the data they see.Â 

That, I think, could point to an odd solution: Issue corrections, but less frequently than you make them. Analytics is an imprecise art, often built on top of a shaky foundation of fickle and unreliable data. Data teams can only be effective if people trust that art *more than they probably should*. Cynically, strategically timed corrections could be a confidence trick that makes analytics just gritty enough to be real, but not appear so flimsy as to be worthless.Â 

# Analytics != Engineering

The other deeply complicating factor about analytical bugs is that theyâ€™re not at all the same as software bugs. Itâ€™s tempting to draw the comparisonâ€”we look to software engineering for so many other things, and development teams have spent decades perfecting how to fix stuff. The parallels, however, are mostly superficial.

Engineering bugs and outages tend to be point-in-time issues. They cause visible, obvious problems when they happen. In some cases, those mistakes can have lasting ramificationsâ€”people [miss flights](https://www.usatoday.com/story/travel/flights/2019/04/01/southwest-american-delta-flights-grounded-monday-outage-aerodata/3329537002/), you have to buy a [new server cage](https://twitter.com/cullend/status/1445156376934862848)â€”but most run-of-the-mill bugs have very brief echoes. When things are broken, weâ€™re upset; when things are working, we're happy.Â 

Analytical bugs, from bad data to miscalculated metrics to unsound analysis, are different. They stick. Strategic decisions get made on them. History is defined by them. Investments have been made; [jobs have been quit](https://on.substack.com/p/grow-series-11-michael-fritzell); [newsrooms have been restructured](https://www.wsj.com/articles/facebook-overestimated-key-video-metric-for-two-years-1474586951); [far-reaching policies have been enacted](https://www.bbc.com/news/magazine-22223190). And more gently, minds have been made up, and beliefs have become entrenched.[^10]Â 

Moreover, the â€œusersâ€ affected by incorrect metricsâ€”execs building their annual plans, writers taking a full-time flier on Substack, editors firing reporters for videographers, politicians passing austerity bills, and sales reps calling the wrong prospectsâ€”may never know that something was wrong. A busted feature is easy to spot, whereas good data looks the same as bad data.Â 

This is what makes analytical corrections so hard. Our data is [full of errors](https://twitter.com/sarahcat21/status/1560465264369512448), nobody knows it, and any one of those errors couldâ€”likely wonâ€™t, but couldâ€”lead to an irreversible mistake. How do you fight *that* enemy?

# â€œWe have 100 customers, give or takeâ€

Honestly, I donâ€™t knowâ€”the title of this post isnâ€™t a rhetorical question. At best, I have two half-baked and off-the-wall guesses.

First, rather than leaning into engineering philosophiesâ€”iterate, move fast and break things, adopt a blameless mindsetâ€”we should take more cues [from journalists](https://stkbailey.substack.com/p/product-sketch-the-new-corp-times). Both analysts and journalists put forward projections of the world, and both professions are only as good as the trust people have in those projections. Moreover, while bad reporting can be corrected, it canâ€™t easily be undone. Once a story is out there, it can take on a life of its own.Â 

In that context, a blameless, iterative culture actually seems dangerous. With journalistsâ€™ power comes responsibility, and personal accountability to mistakes. If data teams want the same kind of influence, to be in the room where decisions get made, and to â€œbe heard and have an impact,â€ we probably have to bear the burden of our mistakes.

Second, we should think about ways to protect ourselves from having to make corrections. Someday, weâ€™ll have an AI-powered, DALL-E-backed, real time anomaly detection and data extrapolation system that corrects bad data, replaces missing data, and draws fun pictures of charts in Gartner decks that recommend you restructure your management hierarchy in the style of Picassoâ€™s blue period. Until then, though, we could try something easier: Rounding.Â 

A long time ago, [Sean Taylor](https://twitter.com/seanjtaylor) told me about a brilliantly simple solution he came up with to communicate uncertainty in numbers: Round them, so that people know they arenâ€™t exact. For example, rather than saying that the Braves had an [8.4 percent chance](https://www.fangraphs.com/standings/playoff-odds?date=2021-10-01&dateDelta=) of winning the World Series last year, which implies a figure that is both precise and accurate,[^11] just say 10 percent. The difference is meaningless, and shows people that they should take the figure as an educated guess.Â 

Weird as it seems, we could do the same for other metrics, including those we can precisely count, but are uncertain that we can count exactly right.Â 

For example, in another recent correction, Substack made a minor adjustment to how they measure historical traffic. It was the sort of [small, fiddling change](https://twitter.com/bennstancil/status/1557002160663134209) that doesnâ€™t materially affect anything, other than my confidence in the numbers. If Substack presented all of their data as rounded numbersâ€”daily counts to the nearest ten or hundredâ€”the adjustment wouldâ€™ve had less of an impact, both because fewer things would change, and because I wouldâ€™ve already assumed the counts were estimates.Â 

And that, I think, gets at the root of the challenge: No matter how vetted our queries and precise our math, most things that we produce as data teams *are* estimates. They may look irrefutable and exactâ€”between 1 am EST on August 15, 2022 and 1 am EST on August 16, 2022, we had exactly 6,373,118 active usersâ€”but theyâ€™re not. Some logs couldâ€™ve been dropped; the definition of an active user could change; the calculation could exclude some type of activity, like opening an email, that other people argue makes a user active; some cavalier exec couldâ€™ve butchered the entire thing. Everything is a bit of a guess, and sometimes, weâ€™re going to get the guesses wrong. The more pertinent question is what we do about *that*.


---


[^1]: â€œBenn, this dashboard says that you wrote nine hundred thousand queries yesterday. Does that sound right to you?â€

[^2]: â€œBenn, do you think if we lost four million dollars per user per month, would we still be in business?â€

[^3]: â€œBenn, I asked you for our new user retention rate by segment, and you gave me a [pie chart](https://www.evernote.com/l/ADrkopXmik5H26st_x0InGhGRi-pSKt2MUI) where one of the values just says six thousand dollars.â€

[^4]: If you canâ€™t beat a code review, wear it down.

[^5]: Iâ€™m a [personal investor](https://benn.substack.com/p/disclose-your-angel-investments) in Bigeye.

[^6]: Ah yes, an *Osha recordable *injury, the kind we actually want to avoid. â€œWell, last week, Steve got gored by a forklift, but it happened when he was on break, so technically speaking, he wasnâ€™t working and we arenâ€™t legally required to report anything.â€

[^7]: This is a bit weird, actually. Anecdotally speaking, it seems like most corrections make things worse, not better. I have no idea why that is. Maybe overcounting is easier than undercounting, because our default is to count everything, not filter out everything? Because we can spot-check against real customers, but canâ€™t prove the negative that not real customers arenâ€™t included? Because when we do undercount something, we donâ€™t notice it because missing data is, well, missing? I donâ€™t know, but the universe seems aligned against us.

[^8]: Admittedly, [I donâ€™t read](https://twitter.com/bennstancil/status/1558223477487935488).

[^9]: To be clear, Iâ€™m not saying we shouldnâ€™t value these things, or that we shouldnâ€™t be ethical. Iâ€™m simply asking what weâ€™d want if put those values aside, for the point of seeing if we can get to that thing while still acting in an ethical way.

[^10]: Is fat bad for you? If you, like me, grew up in the nineties, you probably can never fully rid yourself of some embedded sense that, [no matter how much you got conned](https://www.nytimes.com/2016/09/13/well/eat/how-the-sugar-industry-shifted-blame-to-fat.html), fat has to be *kinda* unhealthy.

[^11]: And [off by 91.6 percent](https://www.youtube.com/watch?v=4qxfWMqEITY)

================================================================================

# The past is not precious

*On habit and hallowed ground. *

---

![](https://substackcdn.com/image/fetch/$s_!gsLc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7c0c845-a53e-424f-985b-cb6590ef630b_880x470.png)

For the last 1,189 straight days, I've filled out a short survey that I made for myself about what's going on in my life. Some fleeting curiosity in 2019 led me to wonder if the events and moments that I thought were important as they were happening were things that Iâ€™d actually remember or care about later. To figure that out, I needed a log of contemporary predictions. So I created a calendar event, attached a Google form to it that asked me to describe and score the most notable thing that happened that day, and set it up to recur daily, forever.Â 

For the first few months, I diligently responded to it. The ninety seconds it took to answer a couple questions felt like a worthwhile price to pay for what would surely be an interesting archeological record to one day dig through.

Today, I still complete it every nightâ€”but now, I do it because I did it yesterday. Despite this exercise dragging on for much longer than I ever expected it to, I've never looked at the data.[^1] I suppose I might, someday, but it's not on my to-do list. The survey itself, however, remains a daily task to be done, mostly because I havenâ€™t missed one yet.

For better or for worse, maintaining momentum like thisâ€”or, less generously, refusing to resist inertiaâ€”is a defining feature of my life. If I do something once, it's an experiment. Ten times, it's a pattern. Fifty, a streak.[^2] But a hundred times, and it becomes my identity. Itâ€™s no longer a thing I do; itâ€™s who I am. I become someone who gets good grades; who has perfect attendance; who doesnâ€™t smoke or swear; who faithfully goes to Barryâ€™s and SoulCycle; who only wears black; who doesnâ€™t eat refined sugar; who doesn't use social media; who publishes a Substack about data every Friday.[^3] Changing these behaviors is no longer about breaking from a habit; it's about breaking from who I am. And so, out of respect for what Iâ€™ve doneâ€”framed, often, as a demonstration of grit and perseverance, traits that can themselves become self-reinforcing identities of their ownâ€”I forgo the things you might want to do: Blow off a test; skip a class; get high and drop f-bombs; sleep in; buy a flamboyant flower-patterned jacket; eat a [Birthday Cake Remix](https://www.coldstonecreamery.com/icecream/signaturecreations/birthdaycakeremix/index.html)â„¢ at Cold Stone; share photos from a trip on Instagram and make jokes on Twitter; write a weird blog post thatâ€™s decidedly *not* about data.

Though this persistence may make sense in some chosen circumstances, like when we make an intentional investment in who we want to become, the things I hold on to are rarely carefully considered or rigorously researched. Like my survey, they were instead built on haphazard bits of experiential driftwood that wedged themselves in my headâ€”and eventually, into my identity. An offhand comment in 2012 about a former coworker turned into a standard of performance that I felt I needed to live up to, which metastasized into the defining feature of my relationship with work. A lunchtime presentation at a 2004 conference in Tallahassee changed how I present myself, which changed how I want to be seen. A casual 2010 New Year's resolution never got forgotten. A circumstantial friendship from 2011 [became an interview for a job](https://benn.substack.com/p/analytics-is-at-a-crossroads#:~:text=A%20coworkers%E2%80%99%20sister%20worked%20at%20Yammer%2C%20and%20she%E2%80%99d%20made%20sure%20my%20application%20made%20got%20a%20careful%20look.) in an industry that I had no particular passion for, which became a career, and eventually, a professional identity.[^4]Â Â 

Though I've never been a religious person (in high school, I was an insufferable teenager whoâ€™d read a [couple hackneyed paragraphs](https://www.goodreads.com/quotes/38828-if-i-were-to-suggest-that-between-the-earth-and) of Bertrand Russell), [we all worship something](https://fs.blog/david-foster-wallace-this-is-water/#:~:text=Because%20here%E2%80%99s%20something%20else%20that%E2%80%99s%20weird%20but%20true%3A%20in%20the%20day%2Dto%2Dday%20trenches%20of%20adult%20life%2C%20there%20is%20actually%20no%20such%20thing%20as%20atheism.%20There%20is%20no%20such%20thing%20as%20not%20worshipping.%20Everybody%20worships.%20The%20only%20choice%20we%20get%20is%20what%20to%20worship.).[^5] My religion is habit. Cobbled together, my routines make up who I amâ€”not because I chose them, but because [I kept making those faces, and froze that way](https://www.gocomics.com/calvinandhobbes/1986/08/12). And once entrenched, their humble and happenstance origins no longer mattered. They were the way it always was; thus, the way it is; and then, fatally, the way it should always be.Â 

A couple years ago, a good friend of mine told me something that stuck with me ever since. â€œIf someone really wanted to,â€ she said, â€œthey could completely change their life in 48 hours.â€ Though we often feel bound to various weightsâ€”our jobs, our relationships, our possessions, our homes, our habits, even our identitiesâ€”those anchors arenâ€™t nearly as heavy as we think they are. Itâ€™s our belief that we arenâ€™t ourselves without them that keeps us attached.

In other words, to paraphrase DJ Khaled, [we Stockholm Syndrome ourselves](https://www.youtube.com/watch?v=jE-6n4dO95A). Our sense of identity imprisons our inconsistent inclinations, and we convince ourselves that those divergent desires won't make us as happy as staying in character. I want to change careersâ€¦but I canâ€™t, because my family and friends expect me to be a doctor. I want to get a divorceâ€¦but I wonâ€™t, because proper people like me only get married once. I want to stop reading this bookâ€¦but I shouldnâ€™t, because Iâ€™m the type of person who doesn't quit.[^6]Â 

In behaving this way, weâ€”as individuals, and teams, and companiesâ€”become captives of our history. We mortgage the promise of our future for the familiarity of our past. We sacrifice the living at the altar of the dead.Â 

But the future is precious, not the past. It's the opportunities in front of us that we should protect dearly, not the routines and traditions that are behind us. What we did and who we were* *only matters insofar as they inform what we want to do and who we want to be.

This isn't an endorsement of some *carpe diem* philosophy, where we should live life like there's no tomorrow. Quite the opposite: We should live life like there's no yesterday. If our habits, our preferences, and even our identities help us do, like, or be the things we want to do, like, or be next, hold on to them. If not, [let it go](https://www.youtube.com/watch?v=L0MK7qz13bU).

Easier said than done, though. The greatest trick the devil ever pulled was to convince us that endurance is a virtue. Once a patternâ€™s been establishedâ€”a streak created, a process standardized, an identity forgedâ€”the righteous thing to do is to keep it going.Â 

In some cases, absolutely. The best views are at the end of long hikes;[^7] often, we have to endure tough valleys to get to higher peaks.

In other cases, however, weâ€™re on the road to nowhere. Weâ€™ve lost the trail, and are walking in circles. *That* grind pays no dividends. Weâ€™re just Stanley Yelnats, [digging holes and refilling them](https://en.wikipedia.org/wiki/Holes_(novel)), and neither us nor the world are any better for it. But if weâ€™ve always been digging holes, or we come from a family of diggers, or we used to like digging, or started digging because we were looking for something, putting down the shovel can feel like giving up. In the face of wandering interests, waning desires, or wavering circumstances, it would be weak to succumb, and noble to persist. Keep going, we tell ourselves; dig on.Â 

Inertia, however, is not strength. We can convince ourselves that it is, because the more something has enduredâ€”the longer the streak, the more developed the habit, the more embedded the identityâ€”the more admirable we treat its continuation. Often, the opposite is true. It takes more energy to stop a train than to keep it rolling down the tracks. And the longer it's been building speed, the more courage it takes to stand in front of it.[^8]

Personally, that courage still eludes me. Tonight, Iâ€™ll fill out another survey. Next week, this blog will return to its usual fare of hapless commentary on data contracts, or universal semantic languages, or the need for SaaS tools for data teams.[^9] [Simply deciding to stop](https://youtu.be/Ry3PTrsza1w?t=328), after weâ€™ve been going for so long, is an awfully hard thing to do.

There are some moments, though, when the courage finds us. Some external force stops the train, and we have to decide if we want to start it again.[^10] 

In those times, rather than clinging to the past and grieving who we no longer are, we should look to the future, and think about who might become. In some cases, [absence makes the heart grow fonder](https://www.youtube.com/watch?v=xuGaLIleROI), and a pause in momentum only confirms our commitment to where the train was headed.Â 

In other casesâ€”more often than we probably realize, if we let ourselves see itâ€”our guiding stars turn out to be little more than random teapots we put in orbit. Theyâ€™re not heavenly objects, but [atmospheric flotsam](https://www.youtube.com/watch?v=kn6-c223DUU);[^11] we arrived here by dead reckoning off of yesterday's habits. Or, even if they were once noble ambitions, honorable traditions, or good ideas, theyâ€™ve since lost their original luster. And then, regardless of how long our shared history is, we should [thank them for their service, and let them go](https://konmari.com/about-the-konmari-method/#:~:text=Thank%20them%20for%20their%20service%20%E2%80%93%20then%20let%20them%20go.). As my friend said, weâ€™re much less bound to the past we knowâ€”and much closer to the future we wantâ€”than we think we are. All we have to do is realize that the most hallowed ground is not the trail behind us, but the open field in front of us.


---


[^1]: [Iâ€™m noticing a pattern.](https://benn.substack.com/p/do-data-driven-companies-win#footnote-3)

[^2]: Hello, Wordle.

[^3]: Of these eight things, three apply to me.

[^4]: Of these four things, four apply to me.

[^5]: I am now an insufferable blogger whoâ€™s read a couple hackneyed paragraphs of David Foster Wallace.

[^6]: Of these three things, one applies to me.

[^7]: Actually, the best views are out of airplane windows, but why get unnecessarily [worked up](https://benn.substack.com/p/open-the-window) about that?

[^8]: Identity and tradition, [very Lindy](https://en.wikipedia.org/wiki/Lindy_effect).

[^9]: Of these two things, I still enjoy one of them.

[^10]: Fun fact: Thereâ€™s a [very specific way to start a train](https://www.wired.com/2014/06/how-do-you-get-a-train-moving/).

[^11]: This is, Wikipedia tells me, an entirely improper use of the [surprisingly specific definition of flotsam](https://en.wikipedia.org/wiki/Flotsam,_jetsam,_lagan_and_derelict). Whatever. Sue me, agents of the [United Nations Convention of the Law of the Sea](https://en.wikipedia.org/wiki/United_Nations_Convention_on_the_Law_of_the_Sea).

================================================================================

# How Snowflake fails

*Climate change comes for us all.*

---

![](https://substackcdn.com/image/fetch/$s_!D0aD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F52c10657-c9c7-42bd-a841-d9e0befdf5dc_540x360.png)

If we had to mark the moment when our hubris peaked, sometime around 1 p.m. on March 24, 2022 is as good a point as any.Â 

Like much of the tech industry, data companies came roaring out of the pandemic. Snowflakeâ€™s [record IPO](https://www.cnn.com/2020/09/16/investing/snowflake-ipo/index.html) in September of 2020 kicked off a gold rush for data startups, which accelerated through 2021 and peaked in early 2022. The default Series A term sheet was a $20 million raise on a working prototype and a $100 million valuation. Series B and C rounds were often priced at more than half a billion dollars, with founders taking more money out in secondary than their companies had ever booked in revenue.

The fever pitch came to a head in late March at Data Council, a long-standing community conference on data engineering. The event, [full of vendors and VCs pitching to one another](https://roundup.getdbt.com/p/keep-data-council-weird), felt less like a technology conference and more like a celebration of an industry that suddenly couldn't miss. At an afternoon panel on the 24th, an investor was asked if he was worried about the amount of traffic in the market. When a lot of companies chase the same problem, he was asked, how many can actually be successful?Â 

"A fuckton," he said. â€œGo build a $100 million business. Itâ€™s all possible for everyone in this room.â€ The audience applauded.

We went full Ponzi scheme after that. In one reaction to the conference, a commentator speculated that as more data practitioners leave their jobs to found startups, other companies will have a harder time staffing their data engineering teams. Theyâ€™ll need to buy data products because nobody will be able to build them, the argument went, creating an "almost a self-fulfilling prophecy that many of these data companies will be successful."Â 

Icarus, may I introduce you to the sun?

[Our fall was swift](https://benn.substack.com/p/free-fall). The climate changed almost overnight. In early April, the stock market started a [two-month plunge](https://www.google.com/finance/quote/.IXIC:INDEXNASDAQ?sa=X&ved=2ahUKEwiCgvaPoPL5AhVEEmIAHYvdBJ4Q3ecFegQIBRAg&window=YTD), with the Nasdaq bottoming 25 percent below its value in March. Fundraising markets dried up. Venture capitalists sent out [apocalyptic warnings](https://techcrunch.com/2022/05/19/yc-advises-founders-to-plan-for-the-worst/). Companies started hoarding cash. The paved downhill road to success turned into an overgrown trail up a mountain.Â Â 

Still, a few companies have proven well-suited for the climb. As we're often reminded by VCs in their corporate pep talks, [the hottest fires can forge the most enduring companies](https://www.youtube.com/watch?v=vBkzm4a7iY4&t=1548s). This downturn appears no differentâ€”and three companies, each representing the emerging default for their [respective pillars](https://benn.substack.com/p/the-intergalatic-data-stack) of the modern data stack, now seem poised to become the inevitable winners.

Snowflake, every analytics engineerâ€™s favorite database, keeps piling up [remarkable financial results](https://twitter.com/saxena_puru/status/1562562020355870726). dbt is still the unchallenged hub of data transformation.[^1] And Fivetran has consolidated its lead in ELT nÃ©e ETL, building and acquiring its way into a position that its (biased, yes) [board members have called â€œunassailable.â€](https://www.forbes.com/sites/kenrickcai/2022/08/08/the-56-billion-internet-plumbers/?sh=263b97a73522)

I, and nearly everyone I talk to, subscribes to this consensusâ€”so much so that I know more than one person whoâ€™s contemplating putting as much as half of their savings in Snowflake stock. But, just two years ago, [we all marveled](https://twitter.com/chetanp/status/1303338353954230272) at Zoomâ€™s financial performance. Since then, Zoom shares have fallen 85 percent. Slack, once hyped with [breathless coverage](https://www.forbes.com/sites/johnkoetsier/2018/11/30/how-slack-became-the-fastest-growing-enterprise-software-ever/?sh=1d0bc19c6e7a) about how itâ€™s transforming the way we work, now makes [$900 million a year](https://slack.com/blog/news/slack-announces-strong-fourth-quarter-and-fiscal-year-2021-results)â€”an impressive number, to be sure, but barely twice the [estimated revenue of Edible Arrangements](https://www.zoominfo.com/c/edible-arrangements-llc/27692360).[^2] Do we all work in WeWork offices? WeDonâ€™t. And I still donâ€™t know [what happened to Magic Leap](https://www.forbes.com/sites/briansolomon/2015/12/09/secretive-augmented-reality-startup-magic-leap-raising-827-million/?sh=16fb64082b2e]).

Sure things, in other words, arenâ€™t always dominant.Â 

In the spirit of being [the tenth man](https://www.youtube.com/watch?v=W_A5j3RuWHM), what if weâ€™re also wrong about Snowflake, dbt, or Fivetran? What if we had to assume, five years from now, that each company fell from grace? What if itâ€™s no longer the Silicon Valley darling, but the disappointment, the cautionary tale, the missed opportunity, the tragic has-been and once-was? What if theyâ€™ve been replaced by the new new thing, disrupted by a more exciting newcomer, and left behind by the community? If we have to take this outcome as a givenâ€”a future that is clearly possible, if unlikelyâ€”how would it happen?[^3] How do these companies lose?Â 

Let's start with Snowflake.[^4]

*Editorâ€™s note: I was going to do this for all three companies in this post. But, I spent 700 words on frivolous preamble, and ran out of space. Fivetran and dbt, your dark timelines are coming later.*

# Itâ€™s expensive, part I

We were always going to spend a lot on Snowflake. Data is often a one-way ratchet: We are [constantly collecting more of it](https://www.statista.com/statistics/871513/worldwide-data-created), and trying to do more with it. Any company that meters that usage inevitably creates a pricing elevator that [people will complain about](https://twitter.com/search?q=mixpanel%20expensive&src=typed_query).Â 

The data zeitgeist is slowly souring on Snowflake for thisâ€”though I donâ€™t think it represents a real risk for Snowflake. Yes, if buyers become more cost-conscious, their bills probably wonâ€™t go up by the current average of [71 percent a year](https://twitter.com/jaminball/status/1562536274149249024). But itâ€™s a fallacy to assume that high prices that are *created by rising demand* will drive people away from Snowflake.

If you work [around economic policy](https://benn.substack.com/p/delirium#:~:text=was%20at%20a%20think%20tank%20in%20Washington%2C%20D.C), you sometimes hear arguments like this: If prices for a product rise, fewer people will be able to buy it. That will lead to less demand for it, so producers will sell less stuff and make less money than they would if prices were at their original levels. But this confuses natural prices increase set by the market with artificial prices increases set by sellers. In the former case, demand affects price. In the latter case, price affects demand.Â 

The rising Snowflake costs that people complain about, which are driven by product adoption, are analogous to the first case. Price increases of this sort donâ€™t â€œgo too high;â€ they are simply what the market dictates them to be.

More succinctly, demand for Snowflake wonâ€™t go down because demand for Snowflake went up.Â And thereâ€™s a different between something being â€œexpensiveâ€ and people buying a lot of it. 

That said, itâ€™s conceivable that the Snowflakeâ€™s ability to close large deals (and the natural need to chase bigger enterprise contracts) causes Snowflake to ignore the lower end of the market. This could create space for new players to undercut them, just as [Amplitude did to Mixpanel](https://techcrunch.com/2014/07/11/amplitude-the-analytics-startup-undercutting-mixpanel-raises-2-million-seed-round/). Though this dynamic is pretty common, it typically happens to well-established, dominant incumbents. Given that Snowflake is still a fraction of the size of Microsoft, Google, and [Oracle](https://twitter.com/bennstancil/status/1535368675938947072), Snowflake is still more disruptor than disrupted, and seems unlikely to cede control of the bottom of the market any time soon.Â 

# Itâ€™s expensive, part II

Even if cost isnâ€™t a weakness for Snowflake, however, it might be an opportunity for someone else. For better or for worse, Snowflake (and other warehouses like BigQuery and Databricks) have become the engine of the modern data stack. Most other data products lazily use Snowflake (et al) for their compute services, hiding the total cost of using them inside Snowflake bills.Â 

Nobody has that much incentive to change this. Snowflake takes the pricing heat, so vendors arenâ€™t as motivated to make their services as efficient as they would be if they had to justify the compute bill as part of their offering. And Snowflake probably doesnâ€™t mind these services being inefficient; no gas station will complain too much if you buy an [Excursion](https://en.wikipedia.org/wiki/Ford_Excursion).Â 

On one hand, the database provider can only do so much here. If you hammer Snowflake with queries, they can try to make those queries cheaper to process. But at some point, if you use it a ton, theyâ€™ll bill you a ton.Â 

On the other hand, the warehouse could be smarter about which queries it processes and how. A database could come with a [built-in battery saver](https://benn.substack.com/p/data-and-the-almighty-dollar) thatâ€™s less focused on pure technical performance, and more focused on optimizing how other data tools interact with it. Given how much [extraneous or redundant compute](https://benn.substack.com/p/down-with-the-dag) todayâ€™s tools probably push through Snowflake,[^5] I suspect you could squeeze considerably more savings out of streamlining how databases are used than you could by trying to outrun Mooreâ€™s Law. A database that figures this out could compete with Snowflake on priceâ€”not on the performance of each unit of compute, but on how useful each unit is.Â 

# A modern data warehouse

This principleâ€”be the warehouse for the modern data stackâ€”could be extended to more fundamental characteristics of the database. To a user, the first versions of Snowflake were just a database, but big, fast, and stable. Yes, there was magic under the hood, but youâ€™d never know that using it. If youâ€™d given me an unlabeled connection to Snowflake in 2016, I wouldâ€™ve thought it was Postgres on steroids.

As Iâ€™ve said before, I think [this was the right pitch](https://benn.substack.com/p/the-end-of-big-data)â€”but it might not be the right foundation for the future. Weâ€™re starting to ask databases to do lots of things that Postgres canâ€™t do. We want them to be transactional and analytical; we want them to power machine learning infrastructure; we want them to be semantically aware of their contents; we want to build operational systems on top of them; we want them to power advanced analysis in Python and R; we want them to support interactive data exploration; we want them to host their own applications; and we want all of it to happen in real time.Â 

Though Snowflake clearly wants to do all of these things too, they could be awkward fits. Snowflakeâ€™s bones werenâ€™t (I donâ€™t think?) built for polyglot querying like Databricks, to support native APIs like BigQuery, to embed semantic models like [RelationalAI](https://relational.ai/), for streaming data like [Materialize](https://materialize.com/), or for interactive analytical queries like [Firebolt](https://www.firebolt.io/); they were built in a time when databases were, well, databases. In ten years, that could prove to be outdated, and Snowflakeâ€™s new features could beâ€”like an on-premise provider stumbling their way into a clumsy cloud offeringâ€”a facade on an aging foundation.

# The retro data warehouse

Or, the opposite could happen. The effort to redefine the database as something between a [cloud provider and an operating system](https://snowflakecloud.wpengine.com/en/data-cloud/platform/) might break down under its own weight. This sort of offering could become too confusing, too complicated, and too hard to manage. Longing for simpler times, the pendulum swings back the other wayâ€”towards big, dumb, basic databases that just store data and run queries. Like [touch screens in cars](https://www.nytimes.com/2022/05/23/opinion/touch-screens-cars.html), just because we can build a database with lots of technical bells and flashy whistles doesnâ€™t mean we *should*.

As an aside, if I were in charge of Redshift, this is the bet Iâ€™d be making. The war for the â€œdata cloudâ€ is lost. But does every business need such a heavy solution? Noâ€”some might want a simple warehouse thatâ€™s fast, reliable, and predictably priced. Make it easy to spin up; make it easy to buy; make it easy to load a CSV into. Donâ€™t chase the glittering lure of technology. Take us to the time we ache to go again; be the analog, stick shift database. [Sell us nostalgia](https://www.youtube.com/watch?v=suRDUFpsHus).[^6]Â 

# Picking the wrong fight

If we do prefer our newfangled all-in-one digital databasesâ€”if we want a data platform and not a data warehouseâ€”Snowflake would likely expand aggressively into new markets and product lines. This could open a lot of fronts of competition, and Snowflake might not have the resources to win them all.Â 

You can already see this happening. During the keynote at Snowflake Summit, Snowflake said they were committed to their technology partnersâ€”while simultaneously announcing new features that directly compete with a number of those partners. If this expands further, and Snowflake fully embraces its ambitions as a â€œdata cloud,â€ they could also start competing more directly with cloud providers too. 

In that war, all bets are off. They might alienate the ecosystem. They might find themselves at war with [Google's tech](https://benn.substack.com/p/the-original-purple-people), [Microsoft's money](https://benn.substack.com/p/case-for-consolidation), and [Amazonâ€™s market share](https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/). They might isolate themselves as an oversized database vendor and an undersized cloud provider. Under these conditions, in a war of attrition against a bigger opponent, [even the invincible become vulnerable](https://en.wikipedia.org/wiki/French_invasion_of_Russia).Â 

# A margin squeeze

Ok, this oneâ€™s a bit weird for a company with [sixty percent margins](https://www.wsj.com/market-data/quotes/SNOW/financials/annual/income-statement), but hear me out.Â 

Last fall, [Erik Bernhardsson made the case](https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html) that AWS and other cloud providers might be happy to sell core compute services like EC2, and let other vendorsâ€”Snowflake, for exampleâ€”do the hard work of building, marketing, and distributing applications on top of it.[^7] Some businesses will build rich applications on top of these services, and sell them at a huge premium over cost. Others, however, will end up mostly repackaging compute and storage at a huge scale.Â 

Snowflake has clearly built a lot of technology on top of AWS, GCP, and Azure. But, if this ends up as mostly extraneousâ€”i.e., if we mostly just want a database to store and query dataâ€”Snowflake could slowly drift towards being a lower-margin reseller of cloud compute. This could encourage Snowflake to build its own cloud, opening, again, a dangerous and expensive front against the current cloud providers. Wall Street would start scrutinizing Snowflakeâ€™s expenses, just as their margins start to tighten. And that combination can turn around a businessâ€™s momentum in a hurry.

# Karma

Snowflakeâ€™s original sinâ€”the hideous decision to [use capital letters everywhere](https://benn.substack.com/p/the-case-against-sql-formatting#footnote-4)â€”finally comes home to roost. Elon Musk loses his lawsuit against Twitter, buys the company, and immediately reinstates Donald Trumpâ€™s account. Trump blasts out his first tweet in all capsâ€”â€œEND THE LIBERAL SNOWFLAKE!!!â€â€”and Elon Musk retweets it. Republicans, assuming that Trump is talking about the data company, boycott Snowflake in favor of the database they believe was made for them: Redshift (#turnSnowflakeRed). Democrats, assuming that Elon Muskâ€™s retweet means that Elon Musk will try to buy Snowflake, boycott it in favor the preferred database of Elon Muskâ€™s mortal enemy Jeff Bezos: Redshift (#theEnemyOfMyEnemyIsMyFriend). Within months, Redshift is everywhere, Snowflake is a pariah, and [EVERY ANALYST IS A LITTLE BIT CALMER](https://benn.substack.com/p/the-end-of-big-data#footnote-5).

Will any of these things happen? I have no idea. Though I have no money in Snowflake (either long or short), Iâ€™m [still bullish](https://benn.substack.com/p/i-snowflake). The most likely path is probably gradually slowing growth, followed by twenty years of dominance, followed by twenty years of slow disruption. But, as inevitable as Snowflakeâ€™s success seems, how many paths could it end up down that *aren't* that consensus forecast?

A [fuckton](https://benn.substack.com/p/the-past-is-not-precious#:~:text=get%20high%20and-,drop%20f%2Dbombs,-%3B%20sleep%20in%3B%20buy).


---


[^1]: Are there any other examples of companies that have created a big market while also remaining largely unchallenged in that market? Typically, when someone finds a vacuum, new competitors and copycats rush to fill it. dbt found an opening andâ€¦nobody else even tried to compete.

[^2]: Is this source reputable? Probably not. But donâ€™t you *want *to live in a world where Edible Arrangements is a bigger business than most blue chip Silicon Valley startups?

[^3]: People whoâ€™ve had the misfortune to work with me know that I ask this question a lot. The [â€œpre-mortemâ€](https://en.wikipedia.org/wiki/Pre-mortem) is one of the analytical shortcuts Iâ€™ve stolen from other people, along with things like â€œreverse the timeline,â€ and â€œinvert the success metric,â€ and â€œwhat else would happen?â€ A blog post for another day, maybe.

[^4]: There's a Wall Street version of this post that requires reading financial statements and doing actual math. I'll leave that effort to [more talented financial analysts](https://cloudedjudgement.substack.com/). All of my money is in the S&P 500. I applied to many jobs at banks, and never got an offer. I sold all my [Microsoft shares](https://benn.substack.com/p/case-for-consolidation#footnote-7) at $32, a mere 91 percent off its eventual $343 high. I'm an [aNgEl InVeStOr](https://benn.substack.com/p/disclose-your-angel-investments).

[^5]: If a dashboard or pipeline updates in a forest and nobody is around to look at it, does it still make a sound? If that sound is Snowflakeâ€™s cash register, then you better believe it does.

[^6]: Millennials are executives of analytics now. They donâ€™t want futuristic corporate databases; they want retro 80s [kidcore](https://www.nylon.com/fashion/kidcore-is-the-internet-aesthetic-that-takes-us-back-to-childhood) arcade databases.

[^7]: People in the tech world like to use "commodity" as a pejorative, butâ€¦commodities are a pretty good business? Data isnâ€™t the new oil; cloud compute is. Both are commodified, sold by an oligopoly, and incomprehensibly rich.

================================================================================

# If data is a product, what is production?

*We canâ€™t build something unless we know what it is.*

---

![](https://substackcdn.com/image/fetch/$s_!-HSf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3164af5-f7ee-4cc6-bcd2-8728fae98a12_899x598.png)
*[in San Francisco](https://www.spur.org/news/2022-04-19/how-we-got-parkway-people)*

We did it, yâ€™allâ€”the â€œdata as a productâ€ hype cycle is officially complete. Over the last few years, the idea rocketed through the [maturity curve](https://en.wikipedia.org/wiki/Gartner_hype_cycle), starting somewhere unknown, made its way [onto Medium](https://medium.com/@itunpredictable/data-as-a-product-vs-data-as-a-service-d9f7e622dc55), and graduated to [popular talk](https://www.getdbt.com/coalesce-2020/run-your-data-team-as-a-product-team/) at a popular conference before eventually coalescing into a [seminal blog post](https://locallyoptimistic.com/post/run-your-data-team-like-a-product-team/). People wrote [explainers](https://towardsdatascience.com/data-as-a-product-vs-data-products-what-are-the-differences-b43ddbb0f123) about it; other people [wrote explainers to summarize the explainers](https://www.getcensus.com/blog/what-does-data-as-a-product-really-mean). Vendors [created guides](https://www.montecarlodata.com/blog-how-to-treat-your-data-as-a-product/) with [unique, accurate page titles](https://developers.google.com/search/docs/beginner/seo-starter-guide) and many frequently-searched keywords. The [critics](https://astralcodexten.substack.com/p/a-cyclic-theory-of-subcultures) published [contrarian takes](https://www.linkedin.com/feed/update/urn:li:activity:6973293543939325953/). And now, in the inevitable closing phase, the once-novel proposalâ€”that data teams should think like product teamsâ€”jumped the final corporate shark: [McKinsey wrote a report about it.](https://www.mckinsey.com/business-functions/quantumblack/our-insights/how-to-unlock-the-full-value-of-data-manage-it-like-a-product)[^1]Â 

Still, Iâ€™m glad the idea had its moment. As useful as it is for data teams to learn from engineering departments, theyâ€™re an [imperfect guide](https://benn.substack.com/p/analytics-is-at-a-crossroads). Sourcing inspiration from product teams (and [design teams](https://benn.substack.com/p/analysts-should-have-portfolios), and [support teams](https://benn.substack.com/p/service-pressure)) surely makes for a better collection of best practices than getting all of our ideas from one place.Â 

But before the concept of data as a product retires to its final sterile formâ€”co-opted for a Teradata billboard on the 101 or for the vision statements of a dozen YC applications, presumablyâ€”Iâ€™d like to slip one last thought in under the deadline: If data is a product, we should have a better definition of production.

# Everything, everywhere, all at once

When companies create a product like a website, or a car, or a movie, itâ€™s usually pretty straightforward to define whatâ€™s in production and what isnâ€™t. Itâ€™s the website that people on the internet use; itâ€™s the mass-produced car that people drive off the lot; itâ€™s the sequence of pictures and sounds that people see when theyâ€™re in the theater. Though various things might get worked on that donâ€™t make the final cutâ€”features that bomb in internal testing, clay concept cars that never make it past the dramatic shot in a commercial of Cadillic reinventing itself from the road up, movie scenes that get edited outâ€”thereâ€™s a clear line between development and production.Â 

This makes sense. Tech companies have to support and update the products they ship, and need to be thoughtful about which features are worth that investment. A movie would never hold together if the plot had to include every idea from the writerâ€™s room. Production has to be a protected space, to make it feasible to build, functional to use, and affordable to maintain.Â 

Data teams, unfortunately, havenâ€™t developed the same habit. Production is a fuzzy concept, with a wide and blurry boundary between it and development. Some things are obviously in production, such as core dashboards that are used by an executive team and customer-facing features, like Spotifyâ€™s algorithmic playlists.[^2] And some things obviously never make it out of development, like the hundreds of one-off queries that I write in the course of trying to remember how various tables are supposed to be joined together. But a huge percentage of a data teamâ€™s work sits somewhere in a muddy middle. We share one-off reports to answer one-off questions. We create new dashboards for a product launch, or to fill an urgent demand from an executive. We copy existing dashboards to make new versions that tweak some calculation for a specific customer or marketing campaign. We build data apps that solve narrow problems, like reconciling revenue figures for a financial audit. We ship report after report around the business, each of them addressing something specific, none of them meant to last forever.Â 

When I send a report like this to someone, thereâ€™s an implicit contract attached to it: It works now, for the thing I said it would work for. It is, at that moment, *in production.* But that contract has an undefined scope and an uncertain expiration date. Will it work in the future? For how long? Can the report be extended to answer related questions? People donâ€™t usually ask these questions, and data teams donâ€™t usually volunteer answers.[^3] 

As a result, production becomes an expansive, nebulous Frankenstein.[^4] An analyst sends a report to an account manager whoâ€™s putting together a monthly business review for an important prospect; the account manager finds it useful, bookmarks it, and keeps returning to it well beyond its intended (but unstated) â€œbest beforeâ€ date. A designer finds an old analysis on customer segments and uses it to make decisions about their upcoming user study. An operations analyst builds their own reporting on a few tables they find in Tableau, and never tells the data team. A product manager creates a series of new user engagement metrics to assess a recent release, and starts regularly reporting on them to the executive team.[^5] 

And everyone eventually ends up frustrated. Data teams lose control over what theyâ€™re expected to maintain, and canâ€™t make changes without upsetting at least one side of the [seven-dimensional Rubikâ€™s cube](https://superliminal.com/andrey/mc7d/) of canonical dashboards and lingering ad hoc reports.[^6] Everyone else loses track of what they trust. And the problem compounds, because data teams canâ€™t fix production if theyâ€™re never sure whatâ€™s in it; all we can do is add to it.Â 

So whatâ€™s a data team to do? Think like a product team, and start defining production.

# Commit more, less often

Iâ€™ll take a hard line here: Data teams should be explicit and comprehensive in defining whatâ€™s in production. Whenever we create something with an external edgeâ€”a dashboard, an explorable dataset, an operational pipeline with a user or system on the other endâ€”we should make a choice: Is this a production asset? If it is, it should be marked, recorded, and supported until decided otherwise.Â 

Admittedly, this would be somewhat onerous, and could make people reluctant to declare something as a production thingâ€”but thatâ€™s the point. A company with a few dashboards and a handful of key metrics can focus on whatâ€™s important; a company with hundreds canâ€™t focus on anything. A data team that supports a small collection of production reports can keep them fresh *and *work on other projects; a data team with reports everywhere canâ€™t do either.Â 

This doesnâ€™t mean we shouldnâ€™t answer questions quickly, or turn around one-off dashboards to explore tangential curiosities. We shouldâ€”speed [is a big part](https://hbr.org/2018/12/what-great-data-analysts-do-and-why-every-organization-needs-them) of what makes analysts valuable. But we should be realistic about what happens to things after we make them: They expire. We stop supporting them. They should probably [self-destruct](https://counting.substack.com/p/what-if-every-dashboard-self-destructed). We might as well be honest about this, and neither confuse people who stumble on them six months in the future, nor burden ourselves with wondering if we need to update them when we make some change to the tables they sit on.Â 

In exchange for this peace of mind, we get a new responsibility: We actually have to support the smaller set of things that we officially designate as being in production. We have to stand behind them, just as a product team stands behind features they ship. When some data source or dbt model changes, we have to make sure these things still work. And when we want to stop supporting something, we have to actually deprecate it, rather than letting it drift into slow decay.[^7] 

Of course, data teams already do this; itâ€™s just not usually that explicit. Though that may seem like a minor distinction, the guarantee makes all the difference. Imagine a ski resort with vaguely defined runs: Resorts would never quite know which slopes people expect them to keep clear, and skiers could never be sure if they were somewhere safe or dangerous.[^8] This would be chaos, for everyone. To paraphrase a [favorite data clichÃ©](https://www.azquotes.com/quote/727622), that which is defined is maintained.[^9] 

# De facto production

Without a clear definition of production, data teams risk getting caught in another trap: They can lose control of their own roadmaps.

For most data teams, production is a de facto state. Something gets created; it gets used; it gets maintained out of necessity; it is, for all intents and purposes, now in production. Though data teams can push the reports and dashboards they think are most valuable, they can still get caught chasing the behaviors of their users. Our runs become [the paths people ski](https://99percentinvisible.org/app/uploads/2015/12/desire-path-usability.png), not the runs we want to groom.Â 

For example, suppose a data team has created a set of user engagement metrics that provide, in their view, a good summary of how people are using a product. A financial analyst whoâ€™s building an account health model asks for an adjusted set of metrics that they think will be useful predictors of customer churn. As a data team, weâ€™ve got a choice: We can say no, and shoot down a potentially valuable project. Or we can say yes, and create the new metrics. However, without a firm line around what we consider production, these metrics could easily drift from a one-time exploration into something that weâ€™re expected to regularly support. The new report could also undercut our existing engagement metrics, and confuse people about which ones they should be focused on.Â 

Product teams can handle these sorts of â€œfeature requestsâ€ pretty easily. They tell their customers that they always love to hear from them, that their feedback will be taken into consideration, and in the meantimeâ€”and maybe indefinitelyâ€”to do their best with the product as it is. And then the product team makes a choice: Can they not only build the feature, but also maintain it and support future updates? Does the feature fit into the vision of what they want the product to be? If it does, they build it; if it doesnâ€™t, they donâ€™t.Â 

If production for data teams was better delineated, we could take the same approach. We could decide up front if the financial analystâ€™s request fit into our roadmap or not. If it didnâ€™t, we could still help them without accidentally signing up for a long-term maintenance commitment. We could still create new metrics without threatening the primacy of our existing ones. We could stay true to our visionâ€”these should be our key metrics, these are our core dashboardsâ€”without having to say no to anything that might overlap with it. And if new reports become so valuable that we want to elevate them to production assets, we couldâ€”but the choice would be ours.Â 

# Always be shipping

Data teams and product teams do differ in one very fundamental way though. Product teams are built to ship stuff. Yes, they sometimes have research arms, and yes, people build prototypes that are meant to be thrown away, but all of this is in service of eventually delivering something to production.Â 

Data teams shouldnâ€™t have this mindset. Lots of the work we doâ€”answering one-off questions, sharing self-destructing dashboards, creating new metrics for a financial analystâ€™s churn modelâ€”should never make it to production. We should keep production narrow and exclusive, and be content going days and weeks at a time without ever shipping anything to it.Â 

How do we develop that approach, and build processes that support this way of working? The answer to that question, I suspect, is something we canâ€™t borrow from another department. Itâ€™s one weâ€™re going to figure out for ourselves.


---


[^1]: [The boomers are trying to stay hip.](https://kith.com/blogs/kith/jerry-seinfeld-for-kith-fall-2022)

[^2]: TIL that you can see your â€œtop tracks this monthâ€ and my ego did not need that.

[^3]: A report is like Velveeta. It wonâ€™t go bad right away, and it definitely wonâ€™t be good *forever*. But how long does it last? It depends on [a lot of confusing factors](https://www.luckybelly.com/does-velveeta-cheese-go-bad/), itâ€™s hard to tell when it does go bad, and most of the ingredients are artificial.

[^4]: I know what you pedants want this footnote to say about Frankenstein, and I wonâ€™t do it.

[^5]: To be clear, these arenâ€™t complaints about the people using dashboards or creating things. How are they supposed to do anything else? If we tell someone they can trust it today, why should they assume they canâ€™t trust it tomorrow?Â Itâ€™s a problem with the world they work in, not with what theyâ€™re trying to do in it.

[^6]: Unless that data team employs my former colleague [Nan Ma](https://www.linkedin.com/in/nan-ma-b4421b49/), who actually solved one of these things.

[^7]: [My my, hey hey, itâ€™s better to deprecate than fade away.](https://www.youtube.com/watch?v=W5pNzBqryOE)

[^8]: Either way, youâ€™re [getting eaten by the Abominable Snowman](https://classicreload.com/win3x-skifree.html).

[^9]: Given what he was [measuring and trying to improve](https://en.wikipedia.org/wiki/Karl_Pearson#Politics_and_eugenics), we should probably come up with another favorite clichÃ©.

================================================================================

# How Fivetran fails

*SaaS giveth, and SaaS can taketh away. *

---

![7 Big-Bargain Outlet Malls | Visit California](https://substackcdn.com/image/fetch/$s_!2sE9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F77a02ec5-9371-4263-8dfa-76f797f9c5fb_1280x640.jpeg)
*The biggest threat to Fivetran: The outlet mall.*

Sometime in 2014 or 2015, we were trying to figure out [our next move](https://www.tiktok.com/@thepointerbrothers_/video/7080907346796449070) at Mode. At that point, weâ€™d built a query and visualization tool that was a [solid frontend](https://benn.substack.com/p/business-in-the-back-party-in-the-front) for the modern data stack. The only problem was that the modern data stack didnâ€™t exist yet. Most companies weren't using a cloud database, an ELT provider, and dbt; instead, they had Salesforce, Mixpanel, Stripe, Facebook Ads, and a bunch of building frustrations about the limitations of each productâ€™s embedded reporting tools. They wanted something more flexible that could work directly with raw data, which was exactly what Mode was forâ€”so long as you had a warehouse with data in it. Back then, people had neither.

We had a choice: Do we tell these companiesâ€”potential customers, at a time when we only had a few dozenâ€”that we can't work with them until they buy a database and figure out how to load stuff into it? Or do we connect directly to their SaaS services, extract data out of tools like Salesforce, and try solve the entire problem ourselves?

On the roof of a bike shop in SoMa, we decided on the former. Redshift was becoming popular; everyone, we predicted, would have a warehouse soon. And it was getting easier to fill it with data. A small explosion of cloud ETL startupsâ€”[Fivetran](https://www.producthunt.com/products/fivetran#fivetran), [Alooma](https://www.producthunt.com/products/alooma#alooma), [Xplenty](https://www.producthunt.com/products/xplenty#xplenty), [ETLeap](https://www.producthunt.com/products/selfie-ticket#etleap), and [Blendo](https://www.producthunt.com/products/blendo#blendo)[^1]â€”had launched recently,[^2] and data collection tools like [Segment](https://segment.com/blog/introducing-segment-warehouses-redshift-postgres/)[^3] and [Snowplow](https://snowplow.io/blog/snowplow-0-7-6-released-with-redshift-data-warehouse-support/) were starting to write directly to Redshift. Our view was two-fold: Data warehousing and ingestion will get solved by other companies, and the ELT market, which we thought provided an undifferentiated syncing service between a handful of SaaS apps and databases, wasnâ€™t a business worth getting into.Â 

Eight years later, we were half right. Other companies certainly solved both of these problems. But we got the size of the ELT market wrong. As [SaaS adoption blew up](https://www.statista.com/statistics/1233538/average-number-saas-apps-yearly/), so too did demand to pull data from these services into a database. ELT products were no longer extracting data from a few apps; they were extracting data from hundreds.Â 

Now, the success of Fivetran, the modern data stackâ€™s leading ELT provider, seems obvious *and* inevitable. But, to [tenth man Fivetran](https://benn.substack.com/p/how-snowflake-fails#:~:text=In%20the%20spirit%20of%20being%20the%20tenth%20man), is it? Surely, there are twists that would take us all down a road on which Fivetran slips from its perch, and its decline seems more guaranteed than its ascent. If we knew thatâ€™s where weâ€™re headedâ€”if we assume the result, and try to figure out the causeâ€”what might lead us there?Â 

There are three plausible paths, I think: We no longer need to move data from SaaS apps to warehouses; we still do, but we start moving it in a fundamentally different way; or we need to solve the same problem in the same way, and Fivetranâ€™s no longer the best tool for doing it.

# The problem goes away

Fivetran is, if nothing else, a product of its time. It perfectly married two of the biggest trends in data over the last decade: The explosion of the SaaS industry, and growing desire for a centralized data warehouse.Â 

These waves could dissipate. We might stop buying SaaS productsâ€”because our [SaaS debt](https://sarahsnewsletter.substack.com/p/what-is-saas-debt) catches up to us, or because Microsoft and Salesforce ([and Adobe](https://www.wsj.com/articles/adobe-to-buy-figma-for-about-20-billion-sources-say-11663241962)) go [full Borg](https://www.youtube.com/watch?v=rtEaR1JU-ps) and assimilate rebundle the entire SaaS ecosystem into a single hulking mothership. We could conclude that the centralized warehouse was a mistake, and move back towards [data marts](https://www.talend.com/resources/what-is-data-mart/).[^4] We could keep our SaaS tools and our databases, and simply decide we donâ€™t need to move things from one to the other.Â 

The first two don't seem very plausible to me. Millennials [love apps](https://www.youtube.com/watch?v=szrsfeyLzyg) and [hate acquisitions](https://twitter.com/search?q=figma%20ruin&src=typed_query&f=top). And databases have been around long enough that I wouldnâ€™t bet against their durability.[^5] However, the third possibilityâ€”we stop wanting to move dataâ€”is a bit more intriguing.Â 

Major SaaS vendors like Salesforce, Hubspot, and Intuit have a strong incentive to build their own reporting tools. Companies at this scale make a lot of money from selling new product lines and to new business units; analytics and BI tools offer an opportunity for both. And itâ€™s already happening: Salesforce bought Tableau (to sell as an independent BI product and, I suspect, to improve the native dashboards in Salesforce); Atlassian bought Chartio [to upgrade their appsâ€™ internal reporting](https://techcrunch.com/2021/02/26/atlassian-is-acquiring-chartio-to-bring-data-visualization-to-the-platform/); Stripe built an [embedded query tool](https://stripe.com/sigma) to give their customers direct access to data.Â 

These efforts make sense to vendors, who probably see the money analytics companies are making on top of their data and want to take a cut for themselves. And they make sense for some customers, who want reportingâ€”and in this case, precise and purpose-built reportingâ€”without having to buy a database, Fivetran, and a BI tool to get it.Â 

But does this kill Fivetran? Probably not. Most SaaS companies wonâ€™t be able to [drop $16 billion](https://www.wsj.com/articles/salesforce-to-buy-analytics-platform-tableau-11560167718) on a better reporting suite. Moreover, embedded tools only have access to the data created by the product theyâ€™re embedded in. No matter how good Stripeâ€™s query tool is, it canâ€™t query whatâ€™s in Salesforce, or Zendesk, or that random new ad tech app that someone bought from a Product Hunt promotion three weeks ago. To do that, we still need to centralize everything, and we still need Fivetran.Â 

Unless, of course, we centralize that data in a different way.Â 

# The problem is different

This is where things get interesting.Â 

Though weâ€™ve grown accustomed to seeing Fivetran as the natural line between SaaS apps and the data warehouse, ELT tools are actually a kind of awkward kink in this flow. They often operate as an unofficial middleman, manually mapping themselves to the APIs of the services they source from. Itâ€™s brittleâ€”APIs can change without warningâ€”incompleteâ€”some data may not be available via an APIâ€”and inefficientâ€”rather than getting told when things change, ELT tools usually have to scrape for updates. Fivetran and others do the hard work of dealing with all of this for us, but it canâ€™t be cheap to support.[^6] As customers, we pay that price somewhere.

Weâ€™d all be better offâ€”well, all of us except ELT vendorsâ€”if more apps did what Segment does, and wrote data straight to the warehouse themselves.

SaaS vendors could charge for this service, opening a new revenue stream thatâ€™s considerably easier to build than an embedded BI tool. If customers are already paying usage fees to Fivetran to sync data to their warehouse, the sale is easy: Pay us instead of them. They could be aggressive about it too, by packaging bulk access to their APIs with the syncing service, which would choke off Fivetranâ€™s ability to run the syncs on their own.Â 

Customers would probably prefer to buy these pipelines directly from the SaaS provider. Itâ€™s one less vendor to deal with, and would probably cost less. Most importantly, though, vendors could provide a better service than Fivetran. An native syncing product would be more reliable, because the SaaS app and the data pipeline would be managed by the same people; itâ€™d be faster, because vendors could push data to the warehouse in real time when the app updates; itâ€™d be more efficientâ€”and therefore cheaperâ€”for the warehouse, because it wouldnâ€™t rely on bulk updates.Â 

Warehouses might make efforts to support this as well. For example, Snowflake, which clearly [has ambitions to be the hub of the data ecosystem](https://benn.substack.com/p/i-snowflake), could build simple ingestion APIs for SaaS services to write to. The more apps that create native integrations into Snowflake, the more Snowflakeâ€™s gravity buildsâ€”if you know you want to centralize your SaaS data, and all your SaaS tools can write to Snowflake, you probably buy Snowflake. And the easier it is to build these syncs, the more SaaS vendors could and would offer them.Â 

Granted, if this happens, Fivetran has at least two defenses. First, Iâ€™m probably underappreciating how hard it is to build these sorts of syncing services, and the amount of effort that goes into handling errors, retries, not dropping data, alerting people when something breaks, and so on. Admittedly, I know nothing about this point, or how durable of a defense it is.[^7]Â 

Second, there will always be SaaS apps that donâ€™t want to build a feature like this, or warehouses that vendors donâ€™t write to.[^8] But I doubt this protects Fivetran as much as it might seem. If the biggest SaaS providers start creating their own pipelines, Fivetran has to sell on the tail of popularity distribution. That tail is longâ€”but the moneyâ€™s in the body.Â 

This, to me, is the biggest structural risk to Fivetran. Apps build outlet stores, and go [DTC](https://en.wikipedia.org/wiki/Direct-to-consumer). SaaS vendors giveth Fivetran its market; SaaS vendors could taketh it away.Â 

# The problem is better solved by someone else

Of course, Fivetran could also lose in the boring way: Another tool takes them head on, and [beats them at their own game](https://twitter.com/levie/status/1570415132815269889).Â 

One version of a better Fivetran is Fivetran plus...much better observability. Or Fivetran plus real-time pipelines. Or Fivetran plus way more integrations. Or Fivetran plus built-in (ugh) Data Contracts**â„¢**. I can see the value in all of these additions, but if anyone's going to build Fivetran Plus, itâ€™ll probably be Fivetran.

A second version of a better Fivetran is Fivetran minus...the cost. A product that runs on open source connectors could offload the effort of building and maintaining those connectors to the community, while providing a hosted service that runs them. This would presumably lower the cost to customers. Airbyte, for example, claims to be [ten times cheaper](https://airbyte.com/blog/the-deck-we-used-to-raise-our-150m-series-b) than other ELT alternatives.

Still, despite Airbyteâ€™s assurances to the contrary, Iâ€™m skeptical that this [â€œopen coreâ€ model](https://en.wikipedia.org/wiki/Open-core_model) can compete with Fivetran on quality. Connector maintenance is tedious work, especially across the long tail of infrequently used integrations. Even if people get compensated to maintain them, the four-hundredth most popular SaaS connector is, by definition, not going to make much money. I suspect the only way to fund the development of these integrationsâ€”which, unlike open source software packages, have to be vigilantly updated alongside the corresponding SaaS appâ€”is by using the money thatâ€™s made from the lucrative ones. That funding modelâ€”pay engineers in one part of the business to make features that are subsidized with profits from another part of the businessâ€”starts to look a lot like a more traditional software company, and erodes most of the potential cost savings associated with the open source approach.[^9]Â 

Open source tools could also displace Fivetran if people decide itâ€™s cheaper to host them themselves. Like all compute-driven SaaS services, Fivetran surely adds a healthy overhead on top of their own AWS (or GCP, or Azure) bills. As buyers, we could run a service like Airbyte ourselves, and pay those compute fees at cost.Â 

Iâ€™m skeptical of this too. The data market is moving aggressively towards hosted services. While there are a lot more conversations about the [financial implications](https://twitter.com/jthandy/status/1564359981180264454) of this migration, our preference is clearly tilting away from self-hosted software. Rising costs might slow that lean, but [it wonâ€™t reverse it](https://benn.substack.com/i/71510341/its-expensive-part-i).Â 

A final version of a better Fivetran is Fivetranâ€¦a Snowflake Company (or Amazon, or Microsoft, or Databricks Company). For the same reasons I mentioned earlier, database vendors have an obvious incentive to make it easier for customers to load data into their products. It gives them more opportunities to spin their usage meters; it means they donâ€™t have to recommend other vendors to their customers to help them source data.Â 

But, it seems unlikely that any of the major databases will build a true competitor to Fivetran on their own. Itâ€™s more plausible for someone to buy Fivetran outrightâ€”just as Google did (and [squandered](https://benn.substack.com/i/58708482/google-is-wasting-all-the-good-cards)) [with Alooma](https://techcrunch.com/2019/02/19/google-acquires-cloud-migration-platform-alooma/). Given the [current prices acquirers are paying](https://techcrunch.com/2022/09/15/how-about-that-20b-figma-adobe-deal/) for premier SaaS companies, itâ€™d be hard to characterize this result as anything other than Fivetran succeeding spectacularly.

# The problem is the crystal ball

Putting all of this together, my best guess is that, if Fivetran gets dethroned, itâ€™s a combination of events[^10] that does it. Warehouse providers start building ingestion APIs to encourage more people to write data into them; major SaaS vendors begin selling integrations with those warehouses.[^11] Fivetran loses their most profitable integrations, and has to focus on the long tail of nonessential connections that make less money per dollar spent to develop and maintain. This leads to erosion of Fivetranâ€™s business economics, a long decline in growth, and Fivetranâ€™s eventual reclassification as a fringe tool in the modern data stack.Â 

Is this likely? Probably not. Fivetran is the leading product in space thatâ€™s growing on both ends, and my bet is almost always on inertia.[^12] But if my past predictions about the ELT market are any guide, Iâ€™m not very good at making bets. And sometimes, the future that looks unreasonable and impractical today ends up looking right and natural tomorrow.

# An addendum on Snowflake

I went through this same obnoxious contrarian exercise with Snowflake [a couple weeks ago](https://benn.substack.com/p/how-snowflake-fails). After reluctantly digesting other peopleâ€™s criticism,[^13] my views have evolved a bit: The most important determinant of Snowflakeâ€™s success will be its marketing.Â 

Right now, Snowflake is a database with a bunch of new features hanging off the side. If they can convince people that itâ€™s actually a single cohesive platform that canâ€™t be separated into its component parts, Snowflake will be extremely hard to unseat. Weâ€™ll want to pay for the cohesive experience, not for each feature. Weâ€™ll want the full product, and not a bare bones open source database running on our own AWS metal. Weâ€™ll want [the iPhone](https://benn.substack.com/p/i-snowflake), and not a disconnected [iPod, phone, and internet communication device](https://www.youtube.com/watch?v=MnrJzXM7a6o).

But if they donâ€™t convince us of thatâ€”if Snowflake becomes a database, with frilly bells and whistlesâ€”people will start to shop for something cheaper. Theyâ€™ll look for ways to get their core needs met without paying for the unnecessary add-ons and upsells. And theyâ€™ll potentially migrate from a database with [a hundred small features](https://www.theatlantic.com/politics/archive/2013/01/president-obama-would-choose-to-fight-the-horse-sized-duck/267071/) to a database thatâ€™s one giant [duck](https://twitter.com/josh_wills/status/1565738898436661248).


---


[^1]: Stitch was [still part of RJMetrics](https://blog.rjmetrics.com/2016/08/01/rjmetrics-acquired-by-magento-commerce-pipeline-is-now-stitch/) at this time.

[^2]: I stand by the best definition of the modern data stack is data tools that launched on Product Hunt.

[^3]: In a forgotten bit of esoteric history, Segment originally wrote data to a Redshift database [that they hosted](https://segment.com/blog/segment-sql-amazon-redshift/). White-labeling Redshift like this was briefly a trendâ€”[Periscope Data did the same thing](https://www.producthunt.com/posts/periscope-data-cache)â€”and, though it faded reasonably quickly, itâ€™s interesting to imagine a world where this pattern had stuck.

[^4]: Or, I guess, move towards data meshes, though Iâ€™m gonna be honestâ€”these seem like the same thing.

[^5]: Relational database, very lindy.

[^6]: I will say, I respect the simplicity of Fivetranâ€™s business model: Find the tedious work that everyone does but hates doing, and just do it for us. I think more startups could be successful this way. Donâ€™t out-innovate the competition in the sky; outlast them in the salt mines. (Counterpoint: [Donâ€™t do that.](https://benn.substack.com/p/the-past-is-not-precious))

[^7]: Well, not nothing. Iâ€™ve built two syncing services at Mode: Goatherd, and Goatherd Lambda (itâ€™s important to have two goatherds; otherwise they get [lonely](https://www.youtube.com/watch?v=UmmOJx_Hxto)). Do they work? I canâ€™t be sure, because they donâ€™t have anything for handling errors, retries, not dropping data, alerting people when something breaks, and so on.

[^8]: IBMâ€™s cloud data platformsâ€”which includes DB2â€”[made eleven ](https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2020.pdf)*[billion](https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2020.pdf)*[ dollars in 2020](https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2020.pdf). Thereâ€™s a decent chance that this is more than every modern data stack vendor combined.

[^9]: Thereâ€™s an indirect way that Airbyte (and tools like [Portable](https://portable.io/)) could compete on price though. In the process of developing open source frameworks on which the community can create connectors, these companies might uncover ways to make connector creation easy. Easier to build connectors are cheaper connectors, even if all of them are built in-house.

[^10]: And a [bloody pen](https://twitter.com/ABC/status/1569769405189443586).

[^11]: Two hours before this got published, thisâ€¦[started happening](https://techcrunch.com/2022/09/15/salesforce-snowflake-partnership-moves-customer-data-in-real-time-across-systems/)?

[^12]: [Itâ€™s kinda my thing.](https://benn.substack.com/p/the-past-is-not-precious)

[^13]: itâ€™s friday, why are people fighting with me?

================================================================================

# Fine, let's talk about data contracts

*I agree that disagreement is a problem, but disagree that we need an agreement to solve it. *

---

![](https://substackcdn.com/image/fetch/$s_!zeTz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F63ea09fc-6970-414a-b86c-405dcd74cbec_1600x814.png)
*[Weâ€™â€™re supposed to haggle.](https://www.youtube.com/watch?v=nwWz0VM94m8)*

The walls have been closing in for a while now.Â 

Data contracts first entered the conversation at a safe distance: On [LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:6967515733471735808/).[^1] But recently, theyâ€™ve started inching closer to home. The idea jumped the usual firewall between LinkedIn and [Twitter](https://twitter.com/JohnKutay/status/1572855431063797760), made its way [onto](https://roundup.getdbt.com/p/coalesce-data-contracts-the-semantic) [Substack](https://stkbailey.substack.com/p/data-person-attorney-at-law), into the [replies](https://twitter.com/josh_wills/status/1568276619307139072), and eventually, in the ultimate temptation, [into](https://twitter.com/sarahcat21/status/1562094057534369792) [fights](https://www.linkedin.com/video/event/urn:li:ugcPost:6970843266061664256/).Â 

Gah. Fine. Letâ€™s get this over with.Â 

My initial reaction to data contracts was the same as my reaction [to the data mesh](https://benn.substack.com/i/40271466/the-data-mesh-postscript). Both struck me as a kind of Rorschach proposition: Something defined well enough that we can all sense its shape, but abstract enough that we can also project our own opinions on top of it. Shapeshifting ideas like these are magnets for debateâ€”itâ€™s easy to say what you think a cloud looks likeâ€”but impossible to pin down. The moment we agree on what one corner of it should be, the rest of it melts into something new.Â 

To my profound disappointment, I think I was wrong. There is something useful here thatâ€™s worth talking about, and some concrete architectural points to discuss.[^2] I donâ€™t think it looks like the data contracts that have been proposed in other placesâ€”and it may not be a proper contract at allâ€”but, before we all move on to the next new thing, I have some ideas about [where to put the bike shed](https://effectiviology.com/bikeshedding-law-of-triviality/).

# No liquids, aerosols, or gels

Over the last few years, the data ecosystem has blown up. This hasnâ€™t just attracted new tools and vendors, though itâ€™s certainly done that; itâ€™s also created a lot of new data producers and consumers within businesses. Companies source data from production applications, from event streams, and from third-party SaaS applications like Salesforce and Hubspot. That data is stuffed into a warehouse, passes through some transformation pipelines, pays the Snowflake toll a few times, and eventually finds its way to a dashboard, into a customer-facing product or an internal application, or back into third-party SaaS services.[^3]Â Â 

Useful as this may beâ€”and critical as it is for many businessesâ€™ operationsâ€”the entire system is pretty flimsy. Itâ€™s all interconnected, and the connections are unspecified. Transformation pipelines usually assume data will continue to arrive in the warehouse as it did when the pipe was built; dashboards usually assume data will continue to be transformed in the way it was when they were created. On every edge between every node, thereâ€™s an implicit, unrecorded expectation on both sides. This means we often have no idea whatâ€™s going to happen when we change something. It might be fine, or it might break everythingâ€”and if it does, [weâ€™ll catch it in production](https://roundup.getdbt.com/p/interfaces-and-breaking-stuff#:~:text=Today%2C%20we%E2%80%99re%20catching%20far%20too%20many%20error%20states%20in%20production).[^4]Â 

Data contracts are a proposal for solving this problem. As best I can tell, they have [two core components](https://dataproducts.substack.com/p/the-rise-of-data-contracts?r=7hcrb&s=w&utm_campaign=post&utm_medium=web). First, the people on the two sides of the connectionâ€”the engineer building the application that feeds data into the warehouse, and the analytics engineer building a pipeline on top of it, sayâ€”get together to figure out what that connection should look like. The engineer negotiates on what they can reasonably provide; the analytics engineer negotiates on what they need. At some pointâ€”after either a short conversation or a long series of calls, decks, proposals, counter proposals, and amendments that eventually coalesce into a dealâ€”they agree on something.

Once they do, those expectations are codified. The exact mechanism for this seems to vary, though [most arrangements](https://medium.com/gocardless-tech/data-contracts-at-gocardless-6-months-on-bbf24a37206e) involve sticking some service in between the data source and the database that checks if incoming data meets the agreed-upon standard. Data contracts are the databaseâ€™s TSA: They screen new arrivals to make sure they don't have any bombs, bazookas, or 3.5 ounce tubes of toothpaste.[^5]Â 

No doubt, the problem data contracts aim to solveâ€”keep the lead out of the water, before it gets to peopleâ€™s faucetsâ€”is a real one, and well worth solving. Just last week, we updated how we record customer contracts in Salesforce, our downstream reporting broke, and less than a day after making the initial change, a customer success manager presented incorrect information to a customer. This is what finding errors in production looks likeâ€”and [it can get much worse](https://www.wsj.com/articles/facebook-overestimated-key-video-metric-for-two-years-1474586951).

Data contracts also have ancillary benefits. [They offer](https://twitter.com/josh_wills/status/1568276619307139072) a clear way [to define whatâ€™s in production](https://benn.substack.com/p/what-is-production). They also provide a means for describing the output of what data tools are supposed to produce. Today, most pipelines are procedural tasks: Write from this source to this destination; execute this code; hope what comes out the other end looks like what we thought it would. A data contract adds an expectation to these jobs by specifying what the result should look like. This not only makes the system more durable, but it also makes [declarative DAGs possible](https://benn.substack.com/p/down-with-the-dag).[^6]Â 

However, the way data contracts try to achieve this thoughâ€”through a negotiated agreementâ€”seems wrong on all fronts: Itâ€™s impractical to achieve, impossible to maintain, andâ€”most damning of allâ€”an undesirable outcome to chase.

# You canâ€™t stop Salesforce, you can only hope to contain it

A smart person once told me that the most foolish thing you could do is turn a technology problem into a people problem. For all their faults, they said, computers arenâ€™t fickle or unpredictable. No matter how painful it is to reconcile mismatched code in a computer or messy data in a database, neither are nearly as hard as getting ten people to agree on anything.[^7]Â 

Data contracts make exactly that trade. They replace a brittle technical system with a negotiating table. And the more that contracts depend on one another, the more people will want to be involved. I donâ€™t know if that [kills innovation](https://www.linkedin.com/posts/ethanaaron_data-analytics-activity-6970381178965000193-e_A3/), but itâ€™s at least an annoying set of conversations that most people donâ€™t want to have.

Moreover, even if we do create these contracts, a lot of data â€œprovidersâ€ (e.g., software engineers who are maintaining an application database, and sales ops managers who configure Salesforce) canâ€™t guarantee them anyway. If you change some bit of logic in Salesforceâ€”if, for example, a team stops recording pricing information on the [product object](https://developer.salesforce.com/docs/atlas.en-us.238.0.object_reference.meta/object_reference/sforce_api_objects_product2.htm) and starts recording it on the [product attribute object](https://developer.salesforce.com/docs/atlas.en-us.234.0.object_reference.meta/object_reference/sforce_api_objects_productattribute.htm)â€”itâ€™s hard to know how that change will be reflected in the underlying data model. Salesforceâ€™s UI exists for exactly that reasonâ€”so that we donâ€™t have to think about our entire CRM as a bunch of tables and an [entity-relationship diagram](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model). If administrators sign a contract to maintain a particular data structure, [how can we expect them to hit what they canâ€™t see?](https://twitter.com/muhammadali/status/818502617043177476)

Finally, Iâ€™d also argue that we donâ€™t want data providers to be worried about these contracts in the first place. Engineers, marketers, sales operations managersâ€”all of these people have more important jobs to do than providing consistent data to a data team. They need to build great products, find new customers, and help sales teams sell that product to those customers. The data structures they create are in service of these goals. If those structures need to change to make a product better or to smooth over a kink in the sales cycle, they shouldnâ€™t have to consult an analytics engineer first. In other words, data teams canâ€™t expect to stop changes to products or Salesforce; [we can only hope to contain them](https://youtu.be/lXBXsJ3BR1A?t=50). Though there may be exceptions, most notably when data circles its way back to customer-facing production systems, data teams [are the tail](https://twitter.com/teej_m/status/1572964516841017345).[^8] We should be told when something changes, but itâ€™s a notification, not a negotiation.

But, some people might say, we canâ€™t do our job without quality data. We canâ€™t serve good food if we get bad food from the kitchen. Trueâ€”but this subtly shifts the goal posts. If data contracts are meant to prevent us from serving bad food, we can do *that* on our own. Before we start demanding higher quality data from our providersâ€”and passing the responsibility of what we deliver off to themâ€”we should prove that we know how to identify low quality data first.

To put it another way, data contracts shouldnâ€™t introduce unnecessary and impractical â€‹â€‹negotiations to extract promises from data providers that they canâ€™t and shouldnâ€™t keep. They should instead be a simple defenseâ€”built by data teams, for data teamsâ€”against [communication failures](https://twitter.com/teej_m/status/1572964516841017345). They should be a technical solution that protect against human mistakes, not organizational red tape layered on top of a technological mess.Â 

# The data contract I want

The good news is these safeguards are something that we can build today, in our existing infrastructure.Â 

Today, most architectures look like the diagram below. Data gets written into a warehouse in its raw form; itâ€™s transformed by dbt; and then goes on its merry way to whatever output is nextâ€”a dashboard, an operational tool, an application, whatever. Data quality checks, like dbt tests and observability tools, run after the fact, in production.

![](https://substackcdn.com/image/fetch/$s_!AcRc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F44843a7f-9984-4504-8dce-c02bc0c81259_1594x976.jpeg)

Most proposals for data contracts stick some testing mechanismâ€”the TSA screeningâ€”on the databaseâ€™s front door. This is better than nothing, but thereâ€™s a simpler way to solve the same problem with a tools we already have: database schemas.[^9]Â 

Anything that writes to the database writes to a staging schema.[^10] We define data contractsâ€”or, dbt tests, as is often already doneâ€”against those tables. When the table updates, the test runs. If it passes, the table [moves](https://docs.snowflake.com/en/sql-reference/sql/alter-table.html) to the destination schema that we write to today. If the test fails, someone gets an alert, and the data stays putâ€”and therefore, never reaches production. And if we donâ€™t care about testing against the table, we donâ€™t specify a contract and the table passes through the staging schema instantly.Â 

![](https://substackcdn.com/image/fetch/$s_!UGDW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc810af59-1e6d-4507-b463-e38c6b67ce11_1594x976.jpeg)

This structure satisfies our primary problem: It tests data before it reaches production, not after. By writing to a staging table, debugging a violated contract is also easy, since you can directly inspect the table that caused the failure.

Second, unlike in the TSA approach, tests like these can be applied across all stages of transformation, not just when data is written into the warehouse. Every dbt job could write to staging as well, and every model could be checked against their respective contracts (this detail is how this differs from standard dbt tests, I believe). This provides governance over intermediate steps, and visibility into exactly which step caused a violation.

And finally, this framework fits neatly into todayâ€™s most common architectures. Iâ€™d imagine you could rejigger most dbt projects to operate exactly this way, and a few clever dbt macros could probably handle the renaming gymnastics. This would also be an ideal candidate for Snowflakeâ€™s app store: Build a simple interface for defining contracts (i.e., tests) against tables, and have the app run those tests whenever tables get updated.

Both versions could be extended further, and offer a dedicated web interface for viewing all of the tests. This has a benefit beyond simple convenience: It helps other people, notably data providers, see what expectations are in place. Though they have no obligation to meet those guarantees, I suspect most people would try to respect them, if they know what they are.

Obviously, this arrangement isnâ€™t not perfect. Running a bunch of tests in the warehouse incurs more costs. It might introduce additional latency. Iâ€™m sure it creates all sorts of problems for tables that are loading incrementally.[^11]Â 

But if I handwave past these thingsâ€”Iâ€™m a pundit now, not a practitionerâ€”it checks all the major boxes. It protects data teams from pushing bad data into production; it provides a means for defining what is production; it offers a self-contained way to encode expectations into data pipelines; it helps teams inspect the data that violate those expectations. And somebodyâ€™s probably already built it, and shared it in some Discourse channel years ago.[^12]Â Â 

My suspicion is that most existing data contracts, to the extent that these things exist in the wild, look more like this rather than agreements that were hammered out between grizzled negotiators. Iâ€™d bet that theyâ€™re tests that got introduced after the factâ€”the data looks this way today, and, with this new test, weâ€™re formally declaring that we expect it to keep looking that way tomorrow.Â 

Admittedly, that doesnâ€™t sound as novel as a data contract, and â€œstaging schemasâ€ donâ€™t make for much of a LinkedIn conversation. But thatâ€™s why I stay on Twitterâ€”nobody expects you to come up with revolutionary ideas in 280 characters.Â 


---


[^1]: Itâ€™s funny the degree to which the online data world is split in half between Twitter and LinkedIn. Outside of a few brave ambassadors, there seems to be very little overlap. Both networks have their influencers and reply guys, but the regulars on one are rarely the regulars on the other. (The true pioneers could move to TikTok, but then theyâ€™d have to compete with [Miss Excel](https://www.theverge.com/22807858/tiktok-influencer-microsoft-excel-instagram-decoder-podcast).)

[^2]: Sorry, [TJ](https://twitter.com/teej_m/status/1572972532680237057). In my defense, I tried to ship stuff at Mode, but [canâ€™t hold down a job doing it](https://benn.substack.com/p/the-missing-analytics-executive#:~:text=During%20my%20time%20at%20Mode%2C%20I%E2%80%99ve%20had%20about%20a%20dozen%20jobs). And [like](https://en.wikipedia.org/wiki/Rick_Santorum) [most](https://en.wikipedia.org/wiki/Mike_Huckabee) [failed](https://en.wikipedia.org/wiki/Claire_McCaskill) [practitioners](https://en.wikipedia.org/wiki/Chris_Christie), I have nowhere left to go but to become an out-of-touch talking head that debates [what other people are going to do](https://www.youtube.com/watch?v=2A9l7Mm6AUQ).

[^3]: Is saying SaaS service like saying ATM machine? Or is SaaS an adjective, and software-as-a-service service is allowed?

[^4]: [This song](https://open.spotify.com/track/16NnrgkWfx4i9Avnwqy51q) (and, amazingly, the artist) sums up most companiesâ€™ strategy for maintaining high quality data.

[^5]: All these [four ounce tubes](https://www.amazon.com/s?k=toothpaste+sensodyne&sprefix=toothpaste+sensod%2Caps%2C127&ref=nb_sb_ss_ts-doa-p_1_17) are a conspiracy between the feds and Sensodyne to make us buy more toothpaste and to get TSA agents free confiscated toothpaste, and you canâ€™t convince me otherwise.

[^6]: In that original post, I said that the only difference between our internal transformation tools and dbt [was the scheduler](https://benn.substack.com/p/down-with-the-dag#footnote-anchor-3:~:text=Both%20Integritie%20and%20Easybake%20were%20simpler%2C%20cruder%20versions%20of%20dbt%20in%20nearly%20every%20way%2C%20except%20one%3A%20Their%20schedulers.). There was actually a second difference: Our internal tools required people to define the schema of every model, as in [this example](https://gist.github.com/bstancil/ee9ee57743e7423741fef3c0a3cc669d). This was, in effect, a very dumb yet very effective data contract that Iâ€™d love for dbt to adopt. If schemas or data types changed, the job would fail.

[^7]: I'm no techno-apologist, but if weâ€™re going to save ourselves from climate change, Iâ€™m more optimistic about some Thiel Fellow inventing cold fusion in a MAGA-fueled effort to own the libs than I am about Joe Manchin and Kyrsten Sinema negotiating their way to meaningful political solution.

[^8]: To be more blunt about it, why should we expect other teams to agree to these contracts at all? Would we do it ourselves? Suppose that the finance team comes to us and says we need to organize our data infrastructure in a very particular way because it helps with invoicing. They can still do their job if we change it, but itâ€™ll cause an inconvenience. I suspect most of us would do our best to be helpfulâ€”we donâ€™t need to [go full Microsoft](https://www.businessinsider.com/big-tech-org-charts-2011-6) on everyoneâ€”but we wouldnâ€™t promise to consult with them every time we wanted to add a new schema to our warehouse.

[^9]: Schemas, [itâ€™s always schemas](https://twitter.com/bennstancil/status/1425652920805203976).

[^10]: Iâ€™m using staging to mean something slightly more expansive than how [dbt typically uses it](https://docs.getdbt.com/guides/best-practices/how-we-structure/2-staging).

[^11]: Something something [Materialize](https://materialize.com/).

[^12]: In a way, this is describing a dumbed-down version of [Dagster](https://dagster.io/) as well. Dagster does more than this, but you could probably repurpose it for exactly this problem. (Iâ€™m a very [small investor](https://benn.substack.com/p/disclose-your-angel-investments) in Dagster.)

================================================================================

# A rich man and his iPhone

*That which can't be measured...probably needs to be better managed. *

---

![Elon Musk's 5 must-do everyday habits](https://substackcdn.com/image/fetch/$s_!h2YU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F40444d95-033f-45d9-a4e1-e0f6b97501f7_999x666.jpeg)

Of all of the iPhoneâ€™s accomplishments, one of the most remarkable is that it put the same exact piece of technology into the pockets of people in every strata of society.

In nearly every aspect of their lives, the extraordinarily wealthy exist in a different universe than the rest of us. They live in neighborhoods we canâ€™t enter and on [islands we canâ€™t reach](https://en.wikipedia.org/wiki/Lanai). They fly on separate airplanes, land at [separate terminals](https://www.theguardian.com/world/2017/may/12/lax-private-terminal-rich-people-celebrities), and always get [the window seat](https://benn.substack.com/p/open-the-window?s=w). They wear clothes we canâ€™t shop for, eat food we canâ€™t order, and go to parties we canâ€™t get into. To step out of a normal lifeâ€”or even a normally wealthy oneâ€”into that of a billionaire is, I suspect, to teleport to a different planetâ€”[almost](https://www.nytimes.com/live/2021/07/11/science/virgin-galactic-launch-richard-branson) [literally](https://apnews.com/article/jeff-bezos-space-e0afeaa813ff0bdf23c37fe16fd34265).

Except, oddly enough, when it comes to the place where we spend most of our time: In our phones. Wall Street bankers use iPhones; retired mechanics in Modesto use iPhones; teenagers in Tallahassee use iPhones. Gadget-obsessed techies, grunge guitarists, college baristas, park rangers living on the edge of civilization, professional athletes, and members of Congress use iPhones.[^1] And a man personally worth more than all but [twenty five companies](https://www.google.com/search?q=elon+musk+net+worth&oq=elon+m&aqs=chrome.0.69i59l2j69i57j69i59j69i60l3.1013j0j7&sourceid=chrome&ie=UTF-8) on the S&P 500  casually texts about about [very different things](https://twitter.com/exec_sum/status/1575612060674555905) than the rest of usâ€”but does it [from an iPhone](https://twitter.com/elonmusk/status/1519179787163652099).

This is an astonishing feat. For better or for worse, our phones have become one of our most important possessions. And no matter how rich you are, you canâ€™t get a better one than the one I can get at the Cricket Wireless on the corner. Elon Musk, rocketeer, technological messiah, Iron Man incarnate, canâ€™t buy a better phone than a suburban middle schooler.[^2]Â 

Though there's a long list of reasons why the iPhone was so successful, its egalitarian experience is somewhere on it. Steve Jobs used the same phone that we did, likely got frustrated by the same limitations, and delighted in the same bits of magic. His legendary â€œproduct senseâ€ may not have come from a clairvoyant perception of what his customers wanted, but a keen sense of what *he* wanted. Because on an iPhone, chances are what he wanted is what we wanted, too.Â 

Twitter is the exact opposite.Â 

On any social media platformâ€”and on Twitter in particularâ€”none of us use the same product. Our experiences are unique to who we follow, how prominent we are, and, sadly, who we are and what we look like. Twitter with a few hundred followers is very different than with a few thousand followersâ€”and both are very different than the Twitter that people with tens and hundreds of thousands of followers experience. As a white man, no matter how many followers I do or donâ€™t have, Twitter also ships me a [different version](https://www.amnesty.org/en/latest/news/2018/03/online-violence-against-women-chapter-3/) than the one women, people of color, and people who identify as L.G.B.T.Q. are allowed to download.Â 

This presents an enormous challenge for even the most well-intentioned leaders at Twitter. A Jobsian eye for product isnâ€™t enough. To build Twitter is to build, in effect, thousands of different products, many of which one person can never use. I canâ€™t use the version that BTS uses, in which Iâ€™m inundated with thousands of adoring notifications. A well-paid product manager canâ€™t use the version that politically marginalized groups use [to organize and hold powerful people to account](https://www.washingtonpost.com/opinions/2022/04/26/twitter-elon-musk-new-owner-change-platform). And Jack Dorsey canâ€™t use the version that tries to grind you into oblivion, with no recourse but to [scream into the void](https://benn.substack.com/p/a-slur-on-clubhouse?s=w).[^3] Worse still, unless we look and listen carefully, we may not even see that these versions of Twitter *exist*. In contrast to the iPhone, Twitter is remarkable in its experiential inequality.

Building and supporting a product like this doesnâ€™t require intuitionâ€”it requires humility. It requires asking todayâ€™s tech industry, which has a history of building [products for their creators](https://hbr.org/2016/07/the-internet-of-stuff-your-mom-wont-do-for-you-anymore) and cultures for their founders, to reach well beyond their own experiences. It requires understanding how other people *feel* when they use something, even if that feeling is inaccessible and unquantifiable.

This is particularly difficult in â€œdata-drivenâ€ corporate OKR cultures, which are often full of [punchy platitudes](https://blogs.worldbank.org/education/you-can-t-manage-what-you-don-t-measure) that tell us measurable things are realâ€”and smugly imply that [unmeasurable things arenâ€™t](https://www.ibm.com/blogs/nordic-msp/in-god-we-trust-all-others-must-bring-data/). But emotions and lived experiences are as equally factual as dashboards and product logs. We donâ€™t have to use numbers to be rational.

For those of us who work in data, thereâ€™s a brief lesson here. If and when people go looking to understand the experiences of others, they often ask to â€œsee the data.â€ They want to know, empirically, if the stories are real, or if theyâ€™re â€œstatistically significant.â€ Uncover ideas with anecdotes, the pattern goes, and comfortably verifyâ€”or [try to reject](https://about.fb.com/news/2021/09/research-teen-well-being-and-instagram/)â€”them with data.Â 

We shouldnâ€™t offer ourselves this out. No matter how clever our analysis, and how precise our figures, the multitudes of experiences people have when using a product like Twitter canâ€™t be captured in spreadsheets any more than reading *Oliver Twist *can teach us [how it feels to be an orphan](https://www.youtube.com/watch?v=oRG2jlQWCsY). It must be felt, up close. With some productsâ€”a dying phone battery, a crashing app, a Tesla that drives itself [through red lights](https://www.npr.org/2022/01/18/1073857310/tesla-autopilot-crash-charges)â€”we can feel it for ourselves. But for others, like Twitter, we have to seek it out. We have to accept the humbling fact that our feelings are not enough, and the discomforting fact that our data wonâ€™t save us from having to endure those of others.Â 

The good news is that, like the iPhone, weâ€™re all working with the same emotions, [no matter how rich and petty we are](https://www.ft.com/content/9fd3a6f1-7e01-4fc6-9d7b-a4a956656089).


---


[^1]: Some clowns, however, [do not use iPhones](https://benn.substack.com/p/the-data-os#footnote-7).

[^2]: Thereâ€™s something deeply satisfying about this.

[^3]: Yes, people like Jack Dorsey and Elon Musk get harassed too. But doxxed women have a history of being attacked by harassers. Iâ€™m betting Elon Musk isnâ€™t actually worried about his [private jet](https://www.nytimes.com/2022/02/03/technology/elon-musk-jet-tracking.html) getting cannoned down by some internet goon.